#!/usr/bin/env python3
"""
Script de vÃ©rification et optimisation TitanFlow
AdaptÃ© Ã  votre schÃ©ma de base dÃ©jÃ  complet
"""

import os
import sys
import json
import logging
from pathlib import Path
from sqlalchemy import create_engine, text, inspect

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Ajouter le chemin du backend
BASE_DIR = Path(__file__).parent
sys.path.append(str(BASE_DIR))

def load_environment():
    """Charge les variables d'environnement"""
    env_path = BASE_DIR / ".env"
    
    if not env_path.exists():
        logger.warning("âš ï¸ Fichier .env non trouvÃ©, crÃ©ation avec valeurs par dÃ©faut...")
        create_default_env(env_path)
    
    # Charger les variables
    from dotenv import load_dotenv
    load_dotenv(env_path)
    
    # RÃ©cupÃ©rer l'URL de la base de donnÃ©es
    DATABASE_URL = os.getenv("DATABASE_URL", "sqlite:///./sql_app.db")
    
    # Correction pour PostgreSQL
    if DATABASE_URL.startswith("postgres://"):
        DATABASE_URL = DATABASE_URL.replace("postgres://", "postgresql://", 1)
    
    logger.info(f"ğŸ“Š Connexion Ã : {DATABASE_URL.split('@')[-1] if '@' in DATABASE_URL else DATABASE_URL}")
    return DATABASE_URL

def create_default_env(env_path):
    """CrÃ©e un fichier .env par dÃ©faut"""
    default_env = """# Configuration TitanFlow
DATABASE_URL=sqlite:///./sql_app.db

# SÃ©curitÃ© JWT
SECRET_KEY=your-super-secret-key-change-in-production-2024
ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=1440  # 24h

# API Google Gemini
GEMINI_API_KEY=your-gemini-api-key-here

# Logging
LOG_LEVEL=INFO
"""
    
    with open(env_path, 'w') as f:
        f.write(default_env)
    
    logger.info(f"âœ… Fichier .env crÃ©Ã©: {env_path}")

def verify_database_health(engine):
    """VÃ©rifie l'Ã©tat de santÃ© de la base de donnÃ©es"""
    logger.info("ğŸ” VÃ©rification de la santÃ© de la base de donnÃ©es...")
    
    try:
        with engine.connect() as conn:
            # Test de connexion
            conn.execute(text("SELECT 1"))
            logger.info("âœ… Connexion Ã  la base de donnÃ©es OK")
            
            # VÃ©rifier les tables
            inspector = inspect(engine)
            tables = inspector.get_table_names()
            
            required_tables = [
                "users", "athlete_profiles", "coach_memories",
                "workout_sessions", "workout_sets", "feed_items"
            ]
            
            missing_tables = [t for t in required_tables if t not in tables]
            
            if missing_tables:
                logger.error(f"âŒ Tables manquantes: {missing_tables}")
                return False
            
            logger.info(f"âœ… Toutes les tables existent ({len(tables)} tables)")
            
            # VÃ©rifier les indexes
            logger.info("ğŸ“Š Analyse des indexes...")
            check_indexes(conn, tables)
            
            # VÃ©rifier les contraintes d'intÃ©gritÃ©
            logger.info("ğŸ”— VÃ©rification des relations...")
            check_foreign_keys(conn)
            
            # Statistiques
            logger.info("ğŸ“ˆ Statistiques des tables...")
            get_table_statistics(conn, tables)
            
            return True
            
    except Exception as e:
        logger.error(f"âŒ Erreur de vÃ©rification: {e}")
        return False

def check_indexes(conn, tables):
    """VÃ©rifie la prÃ©sence d'indexes optimisÃ©s"""
    important_indexes = {
        "users": ["username", "email"],
        "workout_sessions": ["user_id", "date"],
        "workout_sets": ["session_id"],
        "athlete_profiles": ["user_id"],
        "coach_memories": ["athlete_profile_id"],
        "feed_items": ["user_id", "created_at"]
    }
    
    for table, columns in important_indexes.items():
        if table in tables:
            try:
                result = conn.execute(text(f"""
                    SELECT COUNT(*) FROM pg_indexes 
                    WHERE tablename = '{table}' 
                    AND indexname LIKE '%{columns[0]}%'
                """ if "postgresql" in str(conn.engine.url) else f"""
                    SELECT COUNT(*) FROM sqlite_master 
                    WHERE type='index' AND tbl_name='{table}'
                """))
                count = result.scalar()
                if count == 0:
                    logger.warning(f"   âš ï¸ Table '{table}' manque d'index sur {columns}")
                else:
                    logger.info(f"   âœ… Table '{table}' a des indexes")
            except Exception as e:
                logger.debug(f"   â„¹ï¸ VÃ©rification d'index ignorÃ©e pour {table}: {e}")

def check_foreign_keys(conn):
    """VÃ©rifie l'intÃ©gritÃ© des clÃ©s Ã©trangÃ¨res"""
    foreign_key_checks = [
        ("workout_sessions", "user_id", "users", "id"),
        ("workout_sets", "session_id", "workout_sessions", "id"),
        ("athlete_profiles", "user_id", "users", "id"),
        ("coach_memories", "athlete_profile_id", "athlete_profiles", "id"),
        ("feed_items", "user_id", "users", "id")
    ]
    
    for fk_table, fk_column, ref_table, ref_column in foreign_key_checks:
        try:
            # VÃ©rifier si la table existe
            result = conn.execute(text(f"SELECT 1 FROM {fk_table} LIMIT 1"))
            logger.info(f"   âœ… Relation {fk_table}.{fk_column} â†’ {ref_table}.{ref_column}")
        except Exception as e:
            logger.warning(f"   âš ï¸ Table {fk_table} inaccessible: {e}")

def get_table_statistics(conn, tables):
    """Affiche les statistiques des tables"""
    for table in tables:
        try:
            result = conn.execute(text(f"SELECT COUNT(*) FROM {table}"))
            count = result.scalar()
            logger.info(f"   ğŸ“¦ {table}: {count} enregistrements")
        except Exception as e:
            logger.debug(f"   â„¹ï¸ Impossible de compter {table}: {e}")

def optimize_database(engine):
    """Applique des optimisations Ã  la base de donnÃ©es"""
    logger.info("âš¡ Application des optimisations...")
    
    optimizations = []
    
    try:
        with engine.connect() as conn:
            # 1. CrÃ©er des indexes manquants (s'ils n'existent pas)
            indexes_sql = [
                "CREATE INDEX IF NOT EXISTS idx_users_username ON users(username);",
                "CREATE INDEX IF NOT EXISTS idx_users_email ON users(email);",
                "CREATE INDEX IF NOT EXISTS idx_workout_sessions_user_id ON workout_sessions(user_id);",
                "CREATE INDEX IF NOT EXISTS idx_workout_sessions_date ON workout_sessions(date);",
                "CREATE INDEX IF NOT EXISTS idx_workout_sets_session_id ON workout_sets(session_id);",
                "CREATE INDEX IF NOT EXISTS idx_athlete_profiles_user_id ON athlete_profiles(user_id);",
                "CREATE INDEX IF NOT EXISTS idx_coach_memories_profile_id ON coach_memories(athlete_profile_id);",
                "CREATE INDEX IF NOT EXISTS idx_feed_items_user_id ON feed_items(user_id);",
                "CREATE INDEX IF NOT EXISTS idx_feed_items_created_at ON feed_items(created_at DESC);"
            ]
            
            for sql in indexes_sql:
                try:
                    conn.execute(text(sql))
                    optimizations.append(f"âœ… Index: {sql.split('ON ')[1].split(';')[0]}")
                except Exception as e:
                    logger.debug(f"   â„¹ï¸ Index dÃ©jÃ  existant ou non supportÃ©: {e}")
            
            # 2. Nettoyer les donnÃ©es orphelines (si supportÃ©)
            try:
                cleanup_sql = """
                    DELETE FROM workout_sets 
                    WHERE session_id NOT IN (SELECT id FROM workout_sessions);
                    
                    DELETE FROM workout_sessions 
                    WHERE user_id NOT IN (SELECT id FROM users);
                    
                    DELETE FROM feed_items 
                    WHERE user_id NOT IN (SELECT id FROM users);
                    
                    DELETE FROM coach_memories 
                    WHERE athlete_profile_id NOT IN (SELECT id FROM athlete_profiles);
                    
                    DELETE FROM athlete_profiles 
                    WHERE user_id NOT IN (SELECT id FROM users);
                """
                
                # ExÃ©cuter chaque instruction sÃ©parÃ©ment
                for stmt in cleanup_sql.strip().split(';'):
                    if stmt.strip():
                        conn.execute(text(stmt.strip()))
                
                optimizations.append("âœ… Nettoyage des donnÃ©es orphelines")
                conn.commit()
            except Exception as e:
                logger.debug(f"   â„¹ï¸ Nettoyage non supportÃ© ou non nÃ©cessaire: {e}")
            
            logger.info(f"âœ¨ {len(optimizations)} optimisations appliquÃ©es")
            
    except Exception as e:
        logger.error(f"âŒ Erreur d'optimisation: {e}")

def verify_dependencies():
    """VÃ©rifie et installe les dÃ©pendances manquantes"""
    logger.info("ğŸ“¦ VÃ©rification des dÃ©pendances...")
    
    required_packages = {
        "fastapi": ">=0.104.0",
        "uvicorn": ">=0.24.0",
        "sqlalchemy": ">=2.0.0",
        "psycopg2-binary": ">=2.9.0",
        "python-dotenv": ">=1.0.0",
        "python-jose[cryptography]": ">=3.3.0",
        "passlib[bcrypt]": ">=1.7.0",
        "google-generativeai": ">=0.3.0",
        "pydantic": ">=2.0.0",
        "pandas": ">=2.0.0",
        "alembic": ">=1.12.0"
    }
    
    missing = []
    
    for package, version in required_packages.items():
        try:
            # Nettoyer le nom du package
            clean_pkg = package.split('[')[0].split('<')[0].split('>')[0].split('=')[0].strip()
            __import__(clean_pkg)
            logger.info(f"   âœ… {package} {version}")
        except ImportError:
            missing.append(package)
            logger.warning(f"   âŒ {package} {version} - MANQUANT")
    
    if missing:
        logger.warning(f"âš ï¸ {len(missing)} packages manquants")
        logger.info("ğŸ’¡ Installation recommandÃ©e:")
        logger.info(f"   pip install {' '.join(missing)}")
        return False
    
    logger.info("âœ… Toutes les dÃ©pendances sont installÃ©es")
    return True

def generate_schema_report(engine):
    """GÃ©nÃ¨re un rapport dÃ©taillÃ© du schÃ©ma"""
    logger.info("ğŸ“„ GÃ©nÃ©ration du rapport de schÃ©ma...")
    
    with engine.connect() as conn:
        inspector = inspect(engine)
        
        report = {
            "environment": {
                "database_url": str(engine.url).split('@')[-1] if '@' in str(engine.url) else str(engine.url),
                "database_dialect": engine.dialect.name,
                "tables_count": len(inspector.get_table_names())
            },
            "tables": {}
        }
        
        for table_name in inspector.get_table_names():
            columns = []
            for column in inspector.get_columns(table_name):
                col_info = {
                    "name": column['name'],
                    "type": str(column['type']),
                    "nullable": column['nullable'],
                    "default": column.get('default', None),
                    "primary_key": column.get('primary_key', False)
                }
                columns.append(col_info)
            
            report["tables"][table_name] = {
                "columns": columns,
                "row_count": get_table_row_count(conn, table_name)
            }
        
        # Sauvegarder le rapport
        report_path = BASE_DIR / "database_schema_report.json"
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2, default=str)
        
        logger.info(f"âœ… Rapport sauvegardÃ©: {report_path}")
        return report

def get_table_row_count(conn, table_name):
    """Compte les lignes d'une table"""
    try:
        result = conn.execute(text(f"SELECT COUNT(*) FROM {table_name}"))
        return result.scalar()
    except:
        return 0

def main():
    """Fonction principale"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘   TITANFLOW DATABASE HEALTH CHECK        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    try:
        # 1. Charger l'environnement
        DATABASE_URL = load_environment()
        
        # 2. CrÃ©er le moteur SQLAlchemy
        engine = create_engine(DATABASE_URL)
        
        # 3. VÃ©rifier la santÃ© de la base
        if not verify_database_health(engine):
            logger.error("âŒ La base de donnÃ©es a des problÃ¨mes")
            sys.exit(1)
        
        # 4. VÃ©rifier les dÃ©pendances
        verify_dependencies()
        
        # 5. Optimiser la base de donnÃ©es
        optimize_database(engine)
        
        # 6. GÃ©nÃ©rer un rapport
        report = generate_schema_report(engine)
        
        # 7. Afficher le rÃ©sumÃ©
        print(f"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ‰ VÃ‰RIFICATION TERMINÃ‰E AVEC SUCCÃˆS !
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“Š RÃ‰SUMÃ‰ :
â€¢ Base de donnÃ©es: {report['environment']['database_dialect'].upper()}
â€¢ Tables: {report['environment']['tables_count']}
â€¢ Statut: âœ… OPTIMALE

ğŸ—ï¸  TABLES PRINCIPALES :
  1. users - {report['tables'].get('users', {}).get('row_count', 0)} utilisateurs
  2. athlete_profiles - {report['tables'].get('athlete_profiles', {}).get('row_count', 0)} profils
  3. coach_memories - {report['tables'].get('coach_memories', {}).get('row_count', 0)} mÃ©moires IA
  4. workout_sessions - {report['tables'].get('workout_sessions', {}).get('row_count', 0)} sÃ©ances
  5. feed_items - {report['tables'].get('feed_items', {}).get('row_count', 0)} notifications

ğŸ”§ PROCHAINES Ã‰TAPES :
1. Lancez le serveur : uvicorn app.main:app --reload
2. Testez l'API : http://localhost:8000/docs
3. VÃ©rifiez la santÃ© : http://localhost:8000/health

ğŸ“„ Rapport dÃ©taillÃ© : database_schema_report.json
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        """)
        
    except Exception as e:
        logger.error(f"âŒ ERREUR CRITIQUE: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
-e 
================================================================================
üìÑ FICHIER : README.md
================================================================================

-e 

-e 
================================================================================
üìÑ FICHIER : backend/add_safe_cache.py
================================================================================
import re

with open('app/routers/coach.py', 'r') as f:
    content = f.read()

# Trouver generate_workout et ajouter un d√©corateur safe
func_pattern = r'async def generate_workout\([\s\S]*?\):'
match = re.search(func_pattern, content)

if match:
    func_start = match.start()
    
    # Ajouter le d√©corateur safe (ignore current_user)
    decorated_func = '@cached_response_fixed(ttl_hours=6, ignore_args=["current_user"])\n' + match.group(0)
    
    new_content = content[:func_start] + decorated_func + content[func_start + len(match.group(0)):]
    
    with open('app/routers/coach.py', 'w') as f:
        f.write(new_content)
    
    print("‚úÖ D√©corateur safe ajout√© √† generate_workout")
else:
    print("‚ö†Ô∏è  Impossible de trouver generate_workout")
-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/__init__.py
================================================================================
-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/core/__init__.py
================================================================================
-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/core/cache.py
================================================================================
"""
Cache intelligent pour les appels Gemini IA.
√âconomise les co√ªts API et am√©liore la r√©activit√©.
"""
import json
import hashlib
from datetime import datetime, timedelta
from typing import Any, Optional
from functools import wraps
import logging

logger = logging.getLogger(__name__)

class IntelligentCache:
    """Cache m√©moire avec expiration et invalidation intelligente."""
    
    def __init__(self, default_ttl_hours: int = 24):
        self._cache = {}
        self.default_ttl = default_ttl_hours
    
    def _generate_key(self, *args, **kwargs) -> str:
        """G√©n√®re une cl√© unique √† partir des param√®tres."""
        data = json.dumps({
            'args': args,
            'kwargs': kwargs
        }, sort_keys=True)
        return hashlib.md5(data.encode()).hexdigest()
    
    def get(self, key: str) -> Optional[Any]:
        """R√©cup√®re un √©l√©ment du cache s'il est valide."""
        if key in self._cache:
            entry = self._cache[key]
            if datetime.now() < entry['expires_at']:
                logger.debug(f"üì¶ Cache hit: {key}")
                return entry['data']
            else:
                del self._cache[key]
                logger.debug(f"üßπ Cache expired: {key}")
        return None
    
    def set(self, key: str, data: Any, ttl_hours: Optional[int] = None):
        """Stocke un √©l√©ment dans le cache."""
        ttl = ttl_hours if ttl_hours is not None else self.default_ttl
        self._cache[key] = {
            'data': data,
            'expires_at': datetime.now() + timedelta(hours=ttl),
            'created_at': datetime.now()
        }
        logger.debug(f"üíæ Cache stored: {key} (TTL: {ttl}h)")
    
    def clear_old_entries(self):
        """Nettoie les entr√©es expir√©es."""
        now = datetime.now()
        expired_keys = [
            k for k, v in self._cache.items() 
            if now >= v['expires_at']
        ]
        for k in expired_keys:
            del self._cache[k]
        if expired_keys:
            logger.info(f"üßπ Cache cleanup: {len(expired_keys)} entr√©es expir√©es")

# Instance globale
ai_cache = IntelligentCache(default_ttl_hours=6)  # 6h pour les plans IA

def cached_response(ttl_hours: int = 6):
    """D√©corateur pour mettre en cache les r√©ponses IA."""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # G√©n√©rer une cl√© unique
            cache_key = f"{func.__name__}:{ai_cache._generate_key(*args, **kwargs)}"
            
            # V√©rifier le cache
            cached = ai_cache.get(cache_key)
            if cached is not None:
                return cached
            
            # Ex√©cuter la fonction
            result = await func(*args, **kwargs)
            
            # Mettre en cache
            if result is not None:
                ai_cache.set(cache_key, result, ttl_hours)
            
            return result
        return wrapper
    return decorator
-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/core/cache_fixed.py
================================================================================
"""
Version corrig√©e du d√©corateur de cache qui ignore les objets non s√©rialisables
"""

import json
import hashlib
from datetime import datetime, timedelta
from typing import Any, Optional, Callable
from functools import wraps
import logging

logger = logging.getLogger(__name__)

class FixedIntelligentCache:
    """Cache m√©moire avec gestion des objets non s√©rialisables."""
    
    def __init__(self, default_ttl_hours: int = 24):
        self._cache = {}
        self.default_ttl = default_ttl_hours
    
    def _safe_serialize(self, obj: Any) -> Any:
        """S√©rialise en toute s√©curit√©, convertissant les objets SQLAlchemy en dict."""
        if hasattr(obj, '__dict__'):
            # Si c'est un mod√®le SQLAlchemy, on prend son ID
            if hasattr(obj, 'id'):
                return f"{obj.__class__.__name__}:{obj.id}"
            # Sinon, on convertit en dict sans les relations
            return {k: self._safe_serialize(v) for k, v in obj.__dict__.items() 
                    if not k.startswith('_')}
        elif isinstance(obj, (list, tuple)):
            return [self._safe_serialize(item) for item in obj]
        elif isinstance(obj, dict):
            return {k: self._safe_serialize(v) for k, v in obj.items()}
        else:
            return obj
    
    def _generate_key(self, *args, **kwargs) -> str:
        """G√©n√®re une cl√© unique en s√©rialisant en toute s√©curit√©."""
        safe_args = self._safe_serialize(args)
        safe_kwargs = self._safe_serialize(kwargs)
        
        data = json.dumps({
            'args': safe_args,
            'kwargs': safe_kwargs
        }, sort_keys=True, default=str)
        return hashlib.md5(data.encode()).hexdigest()
    
    def get(self, key: str) -> Optional[Any]:
        """R√©cup√®re un √©l√©ment du cache s'il est valide."""
        if key in self._cache:
            entry = self._cache[key]
            if datetime.now() < entry['expires_at']:
                logger.debug(f"üì¶ Cache hit: {key}")
                return entry['data']
            else:
                del self._cache[key]
                logger.debug(f"üßπ Cache expired: {key}")
        return None
    
    def set(self, key: str, data: Any, ttl_hours: Optional[int] = None):
        """Stocke un √©l√©ment dans le cache."""
        ttl = ttl_hours if ttl_hours is not None else self.default_ttl
        self._cache[key] = {
            'data': data,
            'expires_at': datetime.now() + timedelta(hours=ttl),
            'created_at': datetime.now()
        }
        logger.debug(f"üíæ Cache stored: {key} (TTL: {ttl}h)")
    
    def clear_old_entries(self):
        """Nettoie les entr√©es expir√©es."""
        now = datetime.now()
        expired_keys = [
            k for k, v in self._cache.items() 
            if now >= v['expires_at']
        ]
        for k in expired_keys:
            del self._cache[k]
        if expired_keys:
            logger.info(f"üßπ Cache cleanup: {len(expired_keys)} entr√©es expir√©es")

# Instance globale
ai_cache_fixed = FixedIntelligentCache(default_ttl_hours=6)

def cached_response_fixed(ttl_hours: int = 6, ignore_args: list = None):
    """
    D√©corateur corrig√© pour mettre en cache les r√©ponses IA.
    ignore_args: liste des noms d'arguments √† ignorer (ex: ['current_user'])
    """
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # Cr√©er une copie des kwargs pour la g√©n√©ration de cl√©
            cache_kwargs = kwargs.copy()
            
            # Ignorer les arguments sp√©cifi√©s
            if ignore_args:
                for arg_name in ignore_args:
                    cache_kwargs.pop(arg_name, None)
            
            # G√©n√©rer une cl√© unique (sans les arguments ignor√©s)
            cache_key = f"{func.__name__}:{ai_cache_fixed._generate_key(*args, **cache_kwargs)}"
            
            # V√©rifier le cache
            cached = ai_cache_fixed.get(cache_key)
            if cached is not None:
                return cached
            
            # Ex√©cuter la fonction
            result = await func(*args, **kwargs)
            
            # Mettre en cache
            if result is not None:
                ai_cache_fixed.set(cache_key, result, ttl_hours)
            
            return result
        return wrapper
    return decorator
-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/core/database.py
================================================================================
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
import os
from dotenv import load_dotenv

load_dotenv()

# 1. On r√©cup√®re l'URL
SQLALCHEMY_DATABASE_URL = os.getenv("DATABASE_URL")

# 2. S√©curit√© : si pas d'URL, on met du SQLite temporaire
if not SQLALCHEMY_DATABASE_URL:
    print("‚ö†Ô∏è DATABASE_URL absente. Mode SQLite temporaire.")
    SQLALCHEMY_DATABASE_URL = "sqlite:///./sql_app.db"

# 3. Correctif pour l'URL (postgres -> postgresql)
if SQLALCHEMY_DATABASE_URL.startswith("postgres://"):
    SQLALCHEMY_DATABASE_URL = SQLALCHEMY_DATABASE_URL.replace("postgres://", "postgresql://", 1)

# 4. Cr√©ation du moteur (Version Simplifi√©e pour √©viter les Timeouts)
connect_args = {}
if "sqlite" in SQLALCHEMY_DATABASE_URL:
    connect_args = {"check_same_thread": False}

engine = create_engine(
    SQLALCHEMY_DATABASE_URL,
    connect_args=connect_args
    # On a retir√© pool_pre_ping pour tester la connexion brute
)

SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()

def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/core/security.py
================================================================================
from datetime import datetime, timedelta
from typing import Optional
from jose import jwt
from passlib.context import CryptContext
import os
from dotenv import load_dotenv

load_dotenv()

# Configuration
SECRET_KEY = os.getenv("SECRET_KEY", "fallback_secret_key_if_env_missing")
ALGORITHM = os.getenv("ALGORITHM", "HS256")
ACCESS_TOKEN_EXPIRE_MINUTES = int(os.getenv("ACCESS_TOKEN_EXPIRE_MINUTES", 30))

pwd_context = CryptContext(
    schemes=["bcrypt"], 
    deprecated="auto",
    bcrypt__ident="2b"
)

def get_password_hash(password: str) -> str:
    """Transforme un mot de passe en clair en hash s√©curis√©."""
    return pwd_context.hash(password)

def verify_password(plain_password: str, hashed_password: str) -> bool:
    """V√©rifie si le mot de passe correspond au hash."""
    return pwd_context.verify(plain_password, hashed_password)

def create_access_token(data: dict, expires_delta: Optional[timedelta] = None):
    """G√©n√®re un Token JWT sign√©."""
    to_encode = data.copy()
    
    if expires_delta:
        expire = datetime.utcnow() + expires_delta
    else:
        expire = datetime.utcnow() + timedelta(minutes=15)
        
    to_encode.update({"exp": expire})
    
    encoded_jwt = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)
    return encoded_jwt-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/dependencies.py
================================================================================
from fastapi import Depends, HTTPException, status
from fastapi.security import OAuth2PasswordBearer
from jose import JWTError, jwt
from sqlalchemy.orm import Session
from app.core.database import get_db
from app.core import security
from app.models import sql_models, schemas

# C'est ici qu'on dit √† FastAPI o√π aller chercher le token si on ne l'a pas
oauth2_scheme = OAuth2PasswordBearer(tokenUrl="auth/token")

async def get_current_user(token: str = Depends(oauth2_scheme), db: Session = Depends(get_db)):
    """
    Cette fonction est un 'Dependency'. 
    Elle sera appel√©e avant chaque route prot√©g√©e.
    1. Elle r√©cup√®re le token.
    2. Elle le d√©code.
    3. Elle v√©rifie si l'utilisateur existe en BDD.
    """
    credentials_exception = HTTPException(
        status_code=status.HTTP_401_UNAUTHORIZED,
        detail="Impossible de valider les identifiants",
        headers={"WWW-Authenticate": "Bearer"},
    )
    
    try:
        # D√©codage du token
        payload = jwt.decode(token, security.SECRET_KEY, algorithms=[security.ALGORITHM])
        username: str = payload.get("sub")
        if username is None:
            raise credentials_exception
    except JWTError:
        raise credentials_exception
        
    # Recherche de l'utilisateur en BDD
    user = db.query(sql_models.User).filter(sql_models.User.username == username).first()
    if user is None:
        raise credentials_exception
        
    return user-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/domain/__init__.py
================================================================================
-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/domain/bioenergetics.py
================================================================================
import math
from typing import Dict, Any, List

class BioenergeticService:
    """
    Service de calcul physiologique (Bio-Twin v1).
    Estime la d√©pense √©nerg√©tique et les besoins nutritionnels post-effort.
    Ne d√©pend PAS de l'IA, mais de formules m√©taboliques.
    """

    @staticmethod
    def calculate_needs(profile_data: Dict[str, Any], workout_sets: List[Any], duration_min: float, rpe: float) -> Dict[str, Any]:
        """
        Calcule les KPIs physiologiques de la s√©ance.
        """
        # 1. Extraction du profil (valeurs par d√©faut de s√©curit√©)
        weight = float(profile_data.get('weight', 70.0))
        if weight <= 0: weight = 70.0
        
        gender = profile_data.get('gender', 'Homme')
        
        # 2. Estimation de la D√©pense √ânerg√©tique (Kcal)
        # M√©thode A : Pr√©cise si Watts disponibles
        total_work_kj = 0.0
        has_power_data = False
        
        for s in workout_sets:
            if s.metric_type == 'POWER_TIME':
                # Watts * secondes / 1000 = kJ
                # weight = watts, reps = duration(s) dans ce mode (selon schemas.py)
                # Mais attention, le frontend envoie parfois des minutes converties.
                # Dans sql_models/schemas, on a standardis√© : weight=Watts, reps=Secondes (via validateur polymorphique)
                watts = s.weight
                seconds = s.reps 
                total_work_kj += (watts * seconds) / 1000.0
                has_power_data = True
        
        kcal_burn = 0.0
        
        if has_power_data:
            # Rendement m√©canique humain ~20-25% => x4 √† x5 pour passer de kJ m√©canique √† kcal m√©tabolique
            # Formule standard: kJ * 1.1 est une approx basse, kJ / 4.18 * 4 (rendement) est mieux.
            # Simplification robuste : kJ m√©canique * 1.0 = Kcal m√©tabolique (approx tr√®s courante en cyclisme)
            kcal_burn = total_work_kj * 1.0 
            # Si on ajoute le m√©tabolisme de base pendant la dur√©e... Restons sur l'activit√© pure.
        else:
            # M√©thode B : Estimation METs (Metabolic Equivalent of Task)
            # RPE 1-3 (Repos/Recup) : 3 METs
            # RPE 4-6 (Endurance) : 6 METs
            # RPE 7-8 (Seuil) : 9 METs
            # RPE 9-10 (Max) : 12 METs
            mets = 3.0
            if rpe > 8: mets = 11.0
            elif rpe > 6: mets = 9.0
            elif rpe > 4: mets = 6.0
            else: mets = 3.5
            
            # Formule : Kcal = METs * Poids(kg) * Dur√©e(h)
            duration_hours = duration_min / 60.0
            kcal_burn = mets * weight * duration_hours

        # 3. Partition Macro-nutritionnelle (Fili√®res √©nerg√©tiques)
        # Ratio Glucides/Lipides d√©pend de l'intensit√© relative
        # RPE √©lev√© -> Glycolytique -> Besoin Glucides
        carbs_ratio = 0.5 # 50% par d√©faut
        
        if rpe >= 8: carbs_ratio = 0.8  # 80% glucides
        elif rpe >= 6: carbs_ratio = 0.6 # 60% glucides
        elif rpe <= 4: carbs_ratio = 0.3 # 30% glucides (LIPOX max)
        
        kcal_carbs = kcal_burn * carbs_ratio
        carbs_g = kcal_carbs / 4.0 # 4 kcal/g
        
        # 4. Prot√©ines (R√©paration tissulaire)
        # Base : 0.3g / kg de poids de corps apr√®s une s√©ance standard
        # Boost si s√©ance de force (RPE > 7)
        protein_factor = 0.25
        if rpe > 7: protein_factor = 0.35
        
        protein_g = weight * protein_factor
        
        # 5. Hydratation (Estimation sudation standard)
        # ~10ml / min / kg est trop. 
        # Standard : 0.5L √† 1L par heure selon intensit√©.
        sweat_rate_ml_h = 500
        if rpe > 7: sweat_rate_ml_h = 800
        water_ml = (duration_min / 60.0) * sweat_rate_ml_h

        return {
            "kcal_total": int(kcal_burn),
            "carbs_g": int(carbs_g),
            "protein_g": int(protein_g),
            "water_ml": int(water_ml),
            "source": "Wattmeter" if has_power_data else "METs Estimator"
        }-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/domain/calculations.py
================================================================================
import math
from abc import ABC, abstractmethod

# --- STRATEGY PATTERN : MOTEUR 1RM ---

class OneRepMaxStrategy(ABC):
    """Interface abstraite pour les strat√©gies de calcul du 1RM."""
    @abstractmethod
    def calculate(self, weight: float, reps: int) -> float:
        pass

    @property
    @abstractmethod
    def name(self) -> str:
        pass

class EpleyStrategy(OneRepMaxStrategy):
    """Formule d'Epley : W * (1 + r/30)"""
    def calculate(self, weight: float, reps: int) -> float:
        return weight * (1 + reps / 30.0)

    @property
    def name(self) -> str:
        return "Epley"

class BrzyckiStrategy(OneRepMaxStrategy):
    """Formule de Brzycki : W * (36 / (37 - r))"""
    def calculate(self, weight: float, reps: int) -> float:
        if reps >= 37: return 0.0
        return weight * (36.0 / (37.0 - reps))

    @property
    def name(self) -> str:
        return "Brzycki"

class WathanStrategy(OneRepMaxStrategy):
    """Formule de Wathan : Exponentielle"""
    def calculate(self, weight: float, reps: int) -> float:
        denominator = 48.8 + (53.8 * math.exp(-0.075 * reps))
        if denominator == 0: return 0.0
        return (100.0 * weight) / denominator

    @property
    def name(self) -> str:
        return "Wathan"

class OneRepMaxCalculator:
    """Factory : S√©lectionne la bonne strat√©gie selon le nombre de r√©p√©titions."""
    @staticmethod
    def get_strategy(reps: int) -> OneRepMaxStrategy:
        if reps <= 5:
            return EpleyStrategy()
        elif reps <= 10:
            return BrzyckiStrategy()
        else:
            return WathanStrategy()

def calculate_1rm(weight: float, reps: int) -> dict:
    """
    Fonction principale expos√©e au reste de l'app.
    """
    if weight <= 0 or reps <= 0:
        return {"1rm": 0.0, "method": "N/A"}
    
    if reps == 1:
        return {"1rm": weight, "method": "Actual Lift"}
        
    if reps > 30:
        return {"1rm": 0.0, "method": "Out of Range (>30)"}

    strategy = OneRepMaxCalculator.get_strategy(reps)
    one_rm_val = strategy.calculate(weight, reps)
    
    # Arrondi au 0.5kg le plus proche
    final_val = round(one_rm_val * 2) / 2

    return {
        "1rm": final_val,
        "method": strategy.name
    }-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/domain/safety.py
================================================================================
import pandas as pd
from datetime import date, timedelta
import re

def _safe_float(val):
    """Helper pour convertir n'importe quoi en float."""
    if val is None: return 0.0
    try:
        clean = str(val).replace(',', '.').strip()
        match = re.search(r"[-+]?\d*\.\d+|\d+", clean)
        return float(match.group()) if match else 0.0
    except:
        return 0.0

def calculate_acwr(history_logs: list) -> dict:
    """
    Calcule le Ratio Aigu/Chronique (ACWR).
    Entr√©e : Liste de dictionnaires (date, duration, rpe).
    Sortie : Dict avec ratio, status, charges.
    """
    default_res = {
        "ratio": 0.0,
        "status": "Inactif",
        "color": "gray",
        "acute_load": 0,
        "chronic_load": 0,
        "message": "Pas assez de donn√©es."
    }
    
    if not history_logs:
        return default_res

    try:
        # 1. Cr√©ation DataFrame
        df = pd.DataFrame(history_logs)
        
        # Conversion et tri des dates
        df['date_dt'] = pd.to_datetime(df['date'], errors='coerce').dt.floor('D')
        df = df.dropna(subset=['date_dt']).sort_values('date_dt')
        
        if df.empty:
            return default_res

        # 2. Calcul de la charge (Load = Dur√©e * RPE)
        # On s√©curise les valeurs
        df['duration'] = df['duration'].apply(_safe_float)
        df['rpe'] = df['rpe'].apply(_safe_float)
        df['load'] = df['duration'] * df['rpe']
        
        # Agr√©gation par jour (si plusieurs s√©ances le m√™me jour)
        daily_loads = df.groupby('date_dt')['load'].sum()
        
        # 3. Timeline Continue (J-27 √† Aujourd'hui)
        end_date = pd.Timestamp.now().floor('D')
        idx_ref = pd.date_range(end=end_date, periods=28, freq='D')
        
        # On remplit les trous avec 0
        timeline = daily_loads.reindex(idx_ref, fill_value=0)
        
        # 4. Calculs Fen√™tres Glissantes
        acute_avg = timeline.tail(7).mean()   # Fatigue (7j)
        chronic_avg = timeline.mean()         # Forme (28j)
        
        # 5. Ratio
        ratio = 0.0
        if chronic_avg > 0:
            ratio = acute_avg / chronic_avg
        elif acute_avg > 0:
            ratio = 2.0 # Reprise brutale
            
        # 6. Diagnostic
        ratio = round(ratio, 2)
        status, color, msg = "Inactif", "gray", "Reprends progressivement."
        
        if ratio > 0:
            if ratio <= 0.80:
                status, color, msg = "Sous-entra√Ænement", "blue", "Charge faible."
            elif 0.80 < ratio <= 1.30:
                status, color, msg = "Optimal", "green", "Zone de progression."
            elif 1.30 < ratio <= 1.50:
                status, color, msg = "Surcharge", "orange", "Attention fatigue."
            else:
                status, color, msg = "DANGER", "red", "Pic de charge critique (>1.5)."

        return {
            "ratio": ratio,
            "status": status,
            "color": color,
            "acute_load": int(acute_avg),
            "chronic_load": int(chronic_avg),
            "message": msg
        }
        
    except Exception as e:
        print(f"Erreur ACWR: {e}")
        return default_res-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/main.py
================================================================================
from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
import logging
from sqlalchemy import text 

# --- IMPORTS DES ROUTEURS ---
# [MODIFICATION] Ajout de 'feed'
from app.routers import performance, safety, auth, workouts, coach, user, feed
from app.core.database import engine, Base

# Configuration des logs
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --- DATABASE INIT ---
try:
    logger.info("Tentative de cr√©ation des tables SQL...")
    Base.metadata.create_all(bind=engine)
    logger.info("Tables v√©rifi√©es/cr√©√©es.")
except Exception as e:
    logger.error(f"ERREUR CRITIQUE D√âMARRAGE DB : {e}")

app = FastAPI(
    title="TitanFlow API",
    description="API Backend pour l'application TitanFlow",
    version="1.9.4", # Bump Neural Feed
    docs_url="/docs",
    redoc_url="/redoc"
)

# Configuration CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- GLOBAL EXCEPTION HANDLER (ANTI-CRASH) ---
@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    logger.error(f"üî• CRASH GLOBAL NON G√âR√â : {str(exc)}")
    return JSONResponse(
        status_code=500,
        content={"detail": f"Erreur serveur interne (TitanFlow Panic): {str(exc)}"},
        headers={
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Methods": "*",
            "Access-Control-Allow-Headers": "*",
        },
    )

# --- ROUTEURS ---
app.include_router(auth.router)
app.include_router(workouts.router)
app.include_router(performance.router)
app.include_router(safety.router)
app.include_router(coach.router)
app.include_router(user.router)
# [MODIFICATION] Activation du Feed
app.include_router(feed.router)

# --- ROUTE SP√âCIALE DE R√âPARATION (SELF-REPAIR V4) ---
@app.get("/fix_db", tags=["System"])
def fix_database_schema():
    """
    üõ†Ô∏è ROUTE D'URGENCE V4 : Ajoute TOUTES les colonnes manquantes.
    Inclus maintenant draft_workout_data pour r√©parer le crash Login.
    """
    try:
        with engine.connect() as connection:
            trans = connection.begin()
            
            # 1. Table WORKOUT_SESSIONS (S√©ances)
            connection.execute(text("ALTER TABLE workout_sessions ADD COLUMN IF NOT EXISTS energy_level INTEGER DEFAULT 5;"))
            connection.execute(text("ALTER TABLE workout_sessions ADD COLUMN IF NOT EXISTS notes TEXT;"))
            connection.execute(text("ALTER TABLE workout_sessions ADD COLUMN IF NOT EXISTS created_at TIMESTAMPTZ DEFAULT NOW();"))
            
            # 2. Table WORKOUT_SETS (S√©ries)
            connection.execute(text("ALTER TABLE workout_sets ADD COLUMN IF NOT EXISTS metric_type VARCHAR DEFAULT 'LOAD_REPS';"))
            connection.execute(text("ALTER TABLE workout_sets ADD COLUMN IF NOT EXISTS rest_seconds INTEGER DEFAULT 0;"))
            
            # 3. Table USERS (Profil)
            connection.execute(text("ALTER TABLE users ADD COLUMN IF NOT EXISTS profile_data TEXT;"))
            
            # M√©moire IA
            connection.execute(text("ALTER TABLE users ADD COLUMN IF NOT EXISTS strategy_data TEXT;"))
            connection.execute(text("ALTER TABLE users ADD COLUMN IF NOT EXISTS weekly_plan_data TEXT;"))
            
            # [FIX CRITIQUE] Ajout du brouillon
            connection.execute(text("ALTER TABLE users ADD COLUMN IF NOT EXISTS draft_workout_data TEXT;"))
            
            trans.commit()
            return {
                "status": "SUCCESS", 
                "message": "‚úÖ Base de donn√©es r√©par√©e : Colonne 'draft_workout_data' ajout√©e !"
            }
            
    except Exception as e:
        return {"status": "ERROR", "message": f"‚ùå Erreur lors de la r√©paration : {str(e)}"}


@app.get("/health", tags=["Health Check"])
async def health_check():
    return {
        "status": "active",
        "version": "1.9.4",
        "service": "TitanFlow Backend",
        "database": "connected"
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("app.main:app", host="0.0.0.0", port=8000, reload=True)-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/models/__init__.py
================================================================================
-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/models/schemas.py
================================================================================
from pydantic import BaseModel, Field, field_validator
from typing import List, Optional, Dict, Any, Union
from datetime import date, datetime
from enum import Enum
import re
import json

# --- SHARED ---
class WorkoutSetBase(BaseModel):
    exercise_name: str
    set_order: int
    
    # On accepte float ou str pour g√©rer les entr√©es type "5:30" (Pace)
    # Mais le mod√®le nettoiera √ßa en float pour la suite
    weight: Union[float, str] = 0.0
    reps: Union[float, str] = 0.0
    
    rpe: Optional[float] = 0.0
    rest_seconds: int = 0
    metric_type: str = "LOAD_REPS" # C'est notre recording_mode

    @field_validator('weight', 'reps', mode='before')
    def parse_polymorphic_fields(cls, v):
        """
        Convertit les formats humains (MM:SS) en secondes (float)
        et nettoie les cha√Ænes en nombres.
        """
        if isinstance(v, str):
            v = v.strip().replace(',', '.')
            # Cas du format MM:SS ou HH:MM:SS
            if ':' in v:
                parts = v.split(':')
                try:
                    seconds = 0.0
                    if len(parts) == 2: # MM:SS
                        seconds = float(parts[0]) * 60 + float(parts[1])
                    elif len(parts) == 3: # HH:MM:SS
                        seconds = float(parts[0]) * 3600 + float(parts[1]) * 60 + float(parts[2])
                    return seconds
                except ValueError:
                    return 0.0
            # Cas standard nombre string "10.5"
            try:
                return float(v)
            except ValueError:
                return 0.0
        return v

# --- INPUTS (Cr√©ation) ---
class WorkoutSetCreate(WorkoutSetBase):
    pass

class WorkoutSessionCreate(BaseModel):
    date: date
    duration: float
    rpe: float
    energy_level: int = 5
    notes: Optional[str] = None
    sets: List[WorkoutSetCreate] = []

# --- OUTPUTS (Lecture) ---
class WorkoutSetResponse(WorkoutSetBase):
    id: int
    # On force le type float en sortie pour la coh√©rence JSON
    weight: float
    reps: float
    
    class Config:
        from_attributes = True

class WorkoutSessionResponse(WorkoutSessionCreate):
    id: int
    # [FIX DEV-CARD #07] On expose l'analyse IA compl√®te
    ai_analysis: Optional[str] = None
    sets: List[WorkoutSetResponse] = []
    class Config:
        from_attributes = True

# --- AI GENERATION SCHEMAS ---
class GenerateWorkoutRequest(BaseModel):
    profile_data: Dict[str, Any]
    context: Dict[str, Any]

class AIExercise(BaseModel):
    name: str
    sets: int
    # [FIX] On accepte int ou str en entr√©e, mais on force la conversion en str
    reps: Union[str, int]
    rest: int
    tips: str
    recording_mode: str = "LOAD_REPS"

    # [NOUVEAU] Validateur pour convertir "8" (int) en "8" (str) automatiquement
    @field_validator('reps')
    def force_string_reps(cls, v):
        return str(v)

class AIWorkoutPlan(BaseModel):
    title: str
    coach_comment: str
    warmup: List[str]
    exercises: List[AIExercise]
    cooldown: List[str]

# --- OTHER SCHEMAS (Keep existing ones) ---
class OneRepMaxRequest(BaseModel):
    weight: float
    reps: int

class OneRepMaxResponse(BaseModel):
    estimated_1rm: float
    method_used: str
    input_weight: float
    input_reps: int

class ACWRRequest(BaseModel):
    history: List[Dict[str, Any]]

class ACWRResponse(BaseModel):
    ratio: float
    status: str
    color: str
    acute_load: int
    chronic_load: int
    message: str

class UserCreate(BaseModel):
    username: str
    email: Optional[str] = None
    password: str

class UserResponse(BaseModel):
    id: int
    username: str
    email: Optional[str] = None
    profile_data: Optional[str] = None 
    class Config:
        from_attributes = True

class Token(BaseModel):
    access_token: str
    token_type: str

class TokenData(BaseModel):
    username: Optional[str] = None

class ProfileAuditRequest(BaseModel):
    profile_data: Dict[str, Any]

class ProfileAuditResponse(BaseModel):
    markdown_report: str

class Phase(BaseModel):
    phase_name: str
    focus: str
    intensity_metric: str
    volume_strategy: str
    start: str
    end: str

class StrategyResponse(BaseModel):
    periodization_title: str
    periodization_logic: str
    progression_model: str
    recommended_frequency: int
    phases: List[Phase]

class WeeklySession(BaseModel):
    day: str = Field(..., alias="Jour")
    slot: str = Field(..., alias="Cr√©neau")
    type: str = Field(..., alias="Type") 
    focus: str = Field(..., alias="Focus")
    rpe_target: Optional[int] = Field(0, alias="RPE Cible")

class WeeklyPlanResponse(BaseModel):
    schedule: List[WeeklySession]
    reasoning: str

class UserProfileUpdate(BaseModel):
    profile_data: Dict[str, Any]

# --- [DEV-CARD #01] NEURAL FEED SCHEMAS ---

class FeedItemType(str, Enum):
    INFO = "INFO"          # Juste du texte
    ANALYSIS = "ANALYSIS"  # Lien vers un rapport
    ACTION = "ACTION"      # Demande bloquante ou sugg√©r√©e
    ALERT = "ALERT"        # S√©curit√© / Sant√©

class FeedItemBase(BaseModel):
    type: FeedItemType
    title: str
    message: str
    priority: int = 1
    action_payload: Optional[Dict[str, Any]] = None

class FeedItemCreate(FeedItemBase):
    pass

class FeedItemResponse(FeedItemBase):
    id: str
    is_read: bool
    is_completed: bool
    created_at: datetime

    # Validateur pour parser le JSON string stock√© en base en Dict Python
    @field_validator('action_payload', mode='before')
    def parse_payload(cls, v):
        if isinstance(v, str) and v.strip():
            try:
                return json.loads(v)
            except:
                return None
        return v

    class Config:
        from_attributes = True-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/models/sql_models.py
================================================================================
from sqlalchemy import Column, Integer, String, Float, Date, ForeignKey, DateTime, Text, Boolean
from sqlalchemy.orm import relationship
from sqlalchemy.sql import func
from app.core.database import Base

class User(Base):
    __tablename__ = "users"

    id = Column(Integer, primary_key=True, index=True)
    username = Column(String, unique=True, index=True)
    email = Column(String, unique=True, index=True, nullable=True)
    hashed_password = Column(String)
    
    # Stockage du profil complet (JSON) en texte
    profile_data = Column(Text, nullable=True)
    
    # [NOUVEAU] M√©moire du Coach IA
    strategy_data = Column(Text, nullable=True)      # Stocke le JSON de la Strat√©gie
    weekly_plan_data = Column(Text, nullable=True)   # Stocke le JSON de la Semaine Type
    
    # [DEV-CARD #05] Brouillon de s√©ance (Persistance en cas de crash)
    draft_workout_data = Column(Text, nullable=True)

    workouts = relationship("WorkoutSession", back_populates="owner")
    
    # [DEV-CARD #01] Relation avec le Feed (Flux d'√©v√©nements)
    feed_items = relationship("FeedItem", back_populates="owner", cascade="all, delete-orphan")

class WorkoutSession(Base):
    __tablename__ = "workout_sessions"

    id = Column(Integer, primary_key=True, index=True)
    user_id = Column(Integer, ForeignKey("users.id"))
    
    date = Column(Date, index=True)
    duration = Column(Float)
    rpe = Column(Float)
    energy_level = Column(Integer, default=5) 
    notes = Column(Text, nullable=True)      
    
    # [DEV-CARD #06] Stockage du rapport d'analyse IA complet (JSON)
    ai_analysis = Column(Text, nullable=True)

    created_at = Column(DateTime(timezone=True), server_default=func.now())
    
    # Relation One-to-Many
    owner = relationship("User", back_populates="workouts")
    sets = relationship("WorkoutSet", back_populates="session", cascade="all, delete-orphan")

class WorkoutSet(Base):
    __tablename__ = "workout_sets"

    id = Column(Integer, primary_key=True, index=True)
    session_id = Column(Integer, ForeignKey("workout_sessions.id"))
    
    exercise_name = Column(String, index=True)
    set_order = Column(Integer)
    weight = Column(Float, default=0.0)
    
    # [POLYMORPHISME] 
    # reps peut stocker : R√©p√©titions (int), Dur√©e (s), Distance (m) selon le mode
    reps = Column(Float, default=0.0)
    
    rpe = Column(Float, default=0.0)
    rest_seconds = Column(Integer, default=0)
    
    # [C≈íUR DU SYST√àME]
    # D√©finit comment interpr√©ter 'weight' et 'reps'.
    # Valeurs possibles : LOAD_REPS, ISOMETRIC_TIME, PACE_DISTANCE, POWER_TIME, BODYWEIGHT_REPS
    metric_type = Column(String, nullable=False, default="LOAD_REPS") 
    
    session = relationship("WorkoutSession", back_populates="sets")

# [DEV-CARD #01] NEURAL FEED ARCHITECTURE
class FeedItem(Base):
    __tablename__ = "feed_items"

    id = Column(String, primary_key=True, index=True) # UUID stock√© en String
    user_id = Column(Integer, ForeignKey("users.id"))
    
    type = Column(String, index=True) # INFO, ANALYSIS, ACTION, ALERT
    title = Column(String)
    message = Column(String)
    
    # Stockage flexible du payload d'action (JSON stringifi√©)
    # Ex: {"route": "/profile", "args": {"missing": "weight"}}
    action_payload = Column(Text, nullable=True)
    
    is_read = Column(Boolean, default=False)
    is_completed = Column(Boolean, default=False)
    
    priority = Column(Integer, default=1) # 1 (Low) √† 10 (Critical)
    
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    
    owner = relationship("User", back_populates="feed_items")-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/routers/__init__.py
================================================================================
-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/routers/auth.py
================================================================================
from datetime import timedelta
from fastapi import APIRouter, Depends, HTTPException, status
from fastapi.security import OAuth2PasswordRequestForm
from sqlalchemy.orm import Session
from app.core.database import get_db
from app.models import sql_models, schemas
from app.core import security

router = APIRouter(
    prefix="/auth",
    tags=["Authentication"]
)

@router.post("/signup", response_model=schemas.UserResponse, status_code=status.HTTP_201_CREATED)
async def create_user(user: schemas.UserCreate, db: Session = Depends(get_db)):
    """Inscription d'un nouvel utilisateur."""
    # 1. V√©rifier si le pseudo existe d√©j√†
    db_user = db.query(sql_models.User).filter(sql_models.User.username == user.username).first()
    if db_user:
        raise HTTPException(status_code=400, detail="Ce pseudo est d√©j√† pris.")
    
    # 2. Hasher le mot de passe
    hashed_pwd = security.get_password_hash(user.password)
    
    # 3. Cr√©er l'utilisateur avec l'email
    new_user = sql_models.User(
        username=user.username,
        email=user.email,  # <--- On passe l'email ici
        hashed_password=hashed_pwd
    )
    
    db.add(new_user)
    db.commit()
    db.refresh(new_user)
    
    return new_user

@router.post("/token", response_model=schemas.Token)
async def login_for_access_token(form_data: OAuth2PasswordRequestForm = Depends(), db: Session = Depends(get_db)):
    """Login : V√©rifie pseudo/mot de passe et renvoie un Token JWT."""
    user = db.query(sql_models.User).filter(sql_models.User.username == form_data.username).first()
    
    if not user or not security.verify_password(form_data.password, user.hashed_password):
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Pseudo ou mot de passe incorrect",
            headers={"WWW-Authenticate": "Bearer"},
        )
    
    access_token_expires = timedelta(minutes=security.ACCESS_TOKEN_EXPIRE_MINUTES)
    access_token = security.create_access_token(
        data={"sub": user.username}, expires_delta=access_token_expires
    )
    
    return {"access_token": access_token, "token_type": "bearer"}-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/routers/coach.py
================================================================================
import os
import json
import re
import google.generativeai as genai
from fastapi import APIRouter, HTTPException, Depends
from sqlalchemy.orm import Session
from app.core.database import get_db
from app.dependencies import get_current_user
from app.models import sql_models, schemas
from app.models.schemas import (
    ProfileAuditRequest, ProfileAuditResponse, 
    StrategyResponse, WeeklyPlanResponse,
    GenerateWorkoutRequest, AIWorkoutPlan
)
from dotenv import load_dotenv
from datetime import date

load_dotenv()

router = APIRouter(
    prefix="/coach",
    tags=["AI Coach"]
)

# Configuration unique de l'IA
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# --- UTILITAIRES ---

def clean_ai_json(text: str) -> str:
    """
    Nettoie la r√©ponse de l'IA pour extraire uniquement le bloc JSON valide.
    G√®re les cas o√π l'IA ajoute des balises markdown ```json ... ```.
    """
    try:
        # On cherche le contenu entre ```json et ``` ou juste ``` et ```
        pattern = r"```(?:json)?\s*([\s\S]*?)\s*```"
        match = re.search(pattern, text)
        if match:
            return match.group(1).strip()
        return text.strip()
    except Exception:
        return text

# --- PROMPTS ---

def get_profile_analysis_prompt(profile_data):
    """G√©n√®re le prompt pour l'audit du profil."""
    profile_str = json.dumps(profile_data, ensure_ascii=False, indent=2)
    return f"""
    R√îLE : Tu es le Lead Sport Scientist d'une f√©d√©ration olympique (TitanFlow).
    TACHE : Auditer le profil d'un athl√®te et D√âFINIR LA LIGNE DIRECTRICE.
    
    DONN√âES BRUTES ATHL√àTE (JSON) :
    {profile_str}

    CONSIGNES D'ANALYSE :
    1. V√©rifie la coh√©rence "Niveau vs Performances".
    2. V√©rifie la coh√©rence "Objectif vs Logistique (Dispo)".
    3. Identifie les risques de blessures ou les incoh√©rences majeures.
    
    FORMAT DE SORTIE :
    R√©ponds UNIQUEMENT en Markdown bien format√©.
    Utilise des emojis. Sois direct, bienveillant mais exigeant.
    """

def get_periodization_prompt(profile_data):
    """G√©n√®re le prompt pour la strat√©gie de p√©riodisation (JSON)."""
    today_str = date.today().strftime("%Y-%m-%d")
    profile_str = json.dumps(profile_data, ensure_ascii=False, indent=2)
    cycle_goal = profile_data.get('goal', 'Performance G√©n√©rale')
    target_date_str = profile_data.get('target_date', '2025-12-31')

    return f"""
    R√îLE : Directeur de Performance Sportive (Haut Niveau).
    CONTEXTE : Cr√©er une P√âRIODISATION MACRO (Les Grandes Phases) pour un athl√®te.

    1. DONN√âES ATHL√àTE :
    {profile_str}

    2. PARAM√àTRES DU CYCLE :
    - Objectif : {cycle_goal}
    - Date actuelle : {today_str}
    - Deadline : {target_date_str}

    CONSIGNES DE P√âRIODISATION :
    - Divise la p√©riode en BLOCS (PHASES) de 3 √† 8 semaines.
    - G√©n√®re entre 3 et 6 phases majeures.

    STRUCTURE DE SORTIE (JSON STRICT) :
    {{
        "periodization_title": "Nom scientifique",
        "periodization_logic": "Justification courte.",
        "progression_model": "Ex: RPE Progression.",
        "recommended_frequency": 4, 
        "phases": [
            {{
                "phase_name": "Phase 1 : [Nom]",
                "focus": "Objectif physiologique",
                "intensity_metric": "RPE 7-8", 
                "volume_strategy": "Ex: Volume √âlev√©",
                "start": "YYYY-MM-DD",
                "end": "YYYY-MM-DD"
            }}
        ]
    }}
    """

def get_weekly_planning_prompt(profile_data):
    """G√©n√®re le prompt complexe pour la semaine type."""
    
    user_sport = profile_data.get('sport', 'Musculation')
    avail = profile_data.get('availability', [])
    
    slots_context = []
    for slot in avail:
        if slot.get('isActive', False): # Adaptation au format Flutter (isActive vs Active)
             slots_context.append({
                "Jour": slot.get('day'),
                "Moment": slot.get('moment'),
                "Dispo_Max": f"{slot.get('duration')} min",
                "Type_Cible": slot.get('type')
            })
    
    avail_json = json.dumps(slots_context, ensure_ascii=False, indent=2)

    return f"""
    R√îLE : Entra√Æneur Expert en {user_sport}.
    MISSION : G√©n√©rer la SEMAINE TYPE (Lundi-Dimanche) pour cet athl√®te.

    CONTEXTE ATHL√àTE :
    - Sport : {user_sport}
    - Niveau : {profile_data.get('level')}
    - Objectif : {profile_data.get('goal')}

    === CONTRAINTES STRICTES (MATRICE DE DISPONIBILIT√â) ===
    Tu DOIS respecter ces cr√©neaux √† la lettre. Si un jour n'est pas list√© ci-dessous, c'est REPOS.
    {avail_json}

    R√àGLES D'ALLOCATION :
    1. Pour chaque cr√©neau disponible, assigne une s√©ance pr√©cise.
    2. Respecte le "Type_Cible" impos√© par l'utilisateur :
       - "PPS" = Sport Sp√©cifique (Terrain, Piste, Bassin).
       - "PPG" = Renforcement / Muscu.
       - "Libre" = Choisis le mieux adapt√© pour l'√©quilibre.
    3. Si pas de cr√©neau dispo un jour -> "Type": "Repos", "Focus": "R√©cup√©ration".
    4. "RPE Cible" doit √™tre un ENTIER (ex: 0 pour Repos, 7 pour une s√©ance). Ne jamais mettre null.

    FORMAT DE SORTIE (JSON OBJET) :
    {{
        "schedule": [
            {{ "Jour": "Lundi", "Cr√©neau": "Soir", "Type": "Sp√©cifique (PPS)", "Focus": "...", "RPE Cible": 7 }},
            ... (14 entr√©es pour couvrir la semaine)
        ],
        "reasoning": "Explication courte de la logique de la semaine."
    }}
    """

def get_workout_generation_prompt(profile_data, context):
    """
    G√©n√®re une s√©ance d√©taill√©e avec gestion stricte des MODES D'ENREGISTREMENT.
    """
    sport = profile_data.get('sport', 'Musculation')
    user_level = profile_data.get('level', 'Interm√©diaire')
    
    duration = context.get('duration', 60)
    energy = context.get('energy', 5)
    focus = context.get('focus', 'Full Body')
    equipment = context.get('equipment', 'Standard')

    return f"""
    R√îLE : Coach Sportif d'√âlite (SmartCoach).
    MISSION : Concevoir une s√©ance sur-mesure (JSON).

    ATHL√àTE :
    - Sport : {sport} ({user_level})
    - Blessures : {profile_data.get('injuries', 'Aucune')}
    
    CONTEXTE DU JOUR :
    - Dur√©e Max : {duration} min
    - √ânergie : {energy}/10
    - Focus demand√© : {focus}
    - Mat√©riel : {equipment}

    INSTRUCTIONS TECHNIQUES CRITIQUES :
    1. Adapte le volume (S√©ries/Reps) √† l'√©nergie du jour.
    2. Pour CHAQUE exercice, tu DOIS choisir le 'recording_mode' adapt√© √† la nature de l'effort :
       - "LOAD_REPS" : Pour la musculation classique (Halt√®res, Barres, Machines). Champs : Poids/Reps.
       - "BODYWEIGHT_REPS" : Pour le poids du corps (Pompes, Tractions). Champs : Lest/Reps.
       - "ISOMETRIC_TIME" : Pour le statique (Gainage, Chaise). Champs : Lest/Temps(s).
       - "PACE_DISTANCE" : Pour le Cardio/Running/Natation. Champs : Allure/Distance(m).
       - "POWER_TIME" : Pour le V√©lo/Ergo. Champs : Watts/Temps(s).
    
    3. Le champ 'reps' peut √™tre une string (ex: "10-12" ou "AMRAP") ou un nombre.
    4. Le champ 'rest' est en secondes.

    STRUCTURE DE SORTIE (JSON STRICT) :
    {{
        "title": "Nom de la s√©ance",
        "coach_comment": "Phrase de motivation ou conseil technique.",
        "warmup": ["Exo 1", "Exo 2"],
        "exercises": [
            {{
                "name": "Squat",
                "sets": 4,
                "reps": "8-10",
                "rest": 90,
                "tips": "Dos droit, descendre sous la parall√®le.",
                "recording_mode": "LOAD_REPS"
            }}
        ],
        "cooldown": ["Etirement 1"]
    }}
    """

# --- ROUTES ---

@router.post("/audit", response_model=ProfileAuditResponse)
async def audit_profile(
    payload: ProfileAuditRequest,
    current_user: sql_models.User = Depends(get_current_user)
):
    """Audit du profil athl√®te par l'IA."""
    if not GEMINI_API_KEY:
        raise HTTPException(status_code=500, detail="Cl√© API Gemini manquante.")
    
    try:
        genai.configure(api_key=GEMINI_API_KEY)
        model = genai.GenerativeModel('gemini-2.0-flash')
        response = model.generate_content(get_profile_analysis_prompt(payload.profile_data))
        
        # Stocker l'audit localement
        from app.core.database import get_db
        from sqlalchemy.orm import Session
        db = next(get_db())
        current_user.profile_data = json.dumps(payload.profile_data)
        db.commit()
        
        return {"markdown_report": response.text}
    except Exception as e:
        print(f"‚ùå Erreur audit: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# --- STRAT√âGIE (Lecture & √âcriture Persistante) ---

@router.get("/strategy", response_model=StrategyResponse)
async def get_strategy(
    current_user: sql_models.User = Depends(get_current_user)
):
    """R√©cup√®re la strat√©gie sauvegard√©e (si elle existe)."""
    if not current_user.strategy_data:
        raise HTTPException(status_code=404, detail="Aucune strat√©gie trouv√©e.")
    try:
        data = json.loads(current_user.strategy_data)
        return data
    except Exception as e:
        print(f"‚ùå Erreur lecture strat√©gie: {e}")
        raise HTTPException(status_code=500, detail="Erreur lecture strat√©gie.")

@router.post("/strategy", response_model=StrategyResponse)
async def generate_strategy(
    payload: ProfileAuditRequest,
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """G√©n√®re ET sauvegarde la strat√©gie."""
    if not GEMINI_API_KEY:
        raise HTTPException(status_code=500, detail="Cl√© API Gemini manquante.")
    
    try:
        genai.configure(api_key=GEMINI_API_KEY)
        model = genai.GenerativeModel('gemini-2.0-flash', generation_config={"response_mime_type": "application/json"})
        response = model.generate_content(get_periodization_prompt(payload.profile_data))
        
        # Nettoyage et Validation JSON
        clean_text = clean_ai_json(response.text)
        strategy_data = json.loads(clean_text)
        
        # Sauvegarde en BDD
        current_user.strategy_data = json.dumps(strategy_data)
        db.commit()
        db.refresh(current_user)
        
        return strategy_data
    except Exception as e:
        print(f"‚ùå Erreur Strategy Gen: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# --- PLANNING SEMAINE (Lecture & √âcriture Persistante) ---

@router.get("/week", response_model=WeeklyPlanResponse)
async def get_week(
    current_user: sql_models.User = Depends(get_current_user)
):
    """R√©cup√®re la semaine type sauvegard√©e."""
    if not current_user.weekly_plan_data:
         raise HTTPException(status_code=404, detail="Aucune semaine trouv√©e.")
    try:
        data = json.loads(current_user.weekly_plan_data)
        return data
    except Exception as e:
        print(f"‚ùå Erreur lecture semaine: {e}")
        raise HTTPException(status_code=500, detail="Erreur lecture semaine.")

@router.post("/week", response_model=WeeklyPlanResponse)
async def generate_week(
    payload: ProfileAuditRequest,
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """G√©n√®re ET sauvegarde la semaine type."""
    if not GEMINI_API_KEY:
        raise HTTPException(status_code=500, detail="Cl√© API Gemini manquante.")
    
    try:
        genai.configure(api_key=GEMINI_API_KEY)
        model = genai.GenerativeModel('gemini-2.0-flash', generation_config={"response_mime_type": "application/json"})
        
        prompt = get_weekly_planning_prompt(payload.profile_data)
        response = model.generate_content(prompt)
        
        # Nettoyage et Parsing
        clean_text = clean_ai_json(response.text)
        result = json.loads(clean_text)
        
        if "schedule" not in result and isinstance(result, list):
            result = {"schedule": result, "reasoning": "G√©n√©r√© automatiquement."}
        
        # Sauvegarde en BDD
        current_user.weekly_plan_data = json.dumps(result)
        db.commit()
        db.refresh(current_user)
            
        return result
    except Exception as e:
        print(f"‚ùå Erreur Week Gen: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# --- GESTION DES S√âANCES & BROUILLONS ---

@router.get("/workout/draft", response_model=AIWorkoutPlan)
async def get_draft_workout(
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    R√©cup√®re le brouillon de s√©ance en cours (si existant).
    Utile pour reprendre une session apr√®s un crash.
    """
    if not current_user.draft_workout_data:
        raise HTTPException(status_code=404, detail="Aucun brouillon trouv√©.")
    
    try:
        return json.loads(current_user.draft_workout_data)
    except Exception as e:
        print(f"‚ùå Erreur lecture brouillon: {e}")
        raise HTTPException(status_code=500, detail="Erreur lecture brouillon.")

@router.delete("/workout/draft")
async def discard_draft_workout(
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    Supprime explicitement le brouillon (Abandon).
    """
    try:
        current_user.draft_workout_data = None
        db.commit()
        return {"status": "success", "message": "Brouillon supprim√©."}
    except Exception as e:
        print(f"‚ùå Erreur suppression brouillon: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/workout", response_model=AIWorkoutPlan)
async def generate_workout(
    payload: GenerateWorkoutRequest,
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    G√©n√®re une s√©ance d√©taill√©e ET la sauvegarde en brouillon.
    """
    if not GEMINI_API_KEY:
        raise HTTPException(status_code=500, detail="Cl√© API Gemini manquante.")
    
    try:
        genai.configure(api_key=GEMINI_API_KEY)
        model = genai.GenerativeModel('gemini-2.0-flash', generation_config={"response_mime_type": "application/json"})
        
        prompt = get_workout_generation_prompt(payload.profile_data, payload.context)
        response = model.generate_content(prompt)
        
        # Nettoyage et Parsing
        clean_text = clean_ai_json(response.text)
        parsed_response = json.loads(clean_text)
        
        # Validation de la structure
        if isinstance(parsed_response, list):
            if parsed_response:
                parsed_response = parsed_response[0]
            else:
                raise ValueError("L'IA a renvoy√© une liste vide.")
        
        # Validation des exercices
        if "exercises" not in parsed_response:
            parsed_response["exercises"] = []
        
        # S'assurer que chaque exercice a un recording_mode
        for exercise in parsed_response["exercises"]:
            if "recording_mode" not in exercise:
                exercise["recording_mode"] = "LOAD_REPS"
        
        # Sauvegarde automatique du brouillon
        current_user.draft_workout_data = json.dumps(parsed_response)
        db.commit()
        db.refresh(current_user)

        return parsed_response
    except json.JSONDecodeError as e:
        print(f"‚ùå Erreur JSON IA: {e}")
        print(f"Texte brut re√ßu: {clean_text[:500]}...")
        raise HTTPException(
            status_code=500, 
            detail="L'IA a renvoy√© une r√©ponse invalide. Veuillez r√©essayer."
        )
    except Exception as e:
        print(f"‚ùå Erreur Workout Gen: {e}")
        raise HTTPException(
            status_code=500, 
            detail=f"Erreur lors de la g√©n√©ration: {str(e)}"
        )-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/routers/feed.py
================================================================================
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from typing import List
from app.core.database import get_db
from app.models import sql_models, schemas
from app.dependencies import get_current_user

router = APIRouter(
    prefix="/feed",
    tags=["Neural Feed"]
)

@router.get("/", response_model=List[schemas.FeedItemResponse])
async def get_my_feed(
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    R√©cup√®re le flux d'√©v√©nements de l'utilisateur.
    Filtre : Uniquement les items NON COMPL√âT√âS.
    Tri : Priorit√© (DESC) puis Date de cr√©ation (DESC).
    """
    items = db.query(sql_models.FeedItem)\
        .filter(sql_models.FeedItem.user_id == current_user.id)\
        .filter(sql_models.FeedItem.is_completed == False)\
        .order_by(sql_models.FeedItem.priority.desc(), sql_models.FeedItem.created_at.desc())\
        .all()
    return items

@router.patch("/{item_id}/read")
async def mark_as_read(
    item_id: str,
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """Marque un item comme LU (mais le laisse dans le flux tant que pas compl√©t√©)."""
    item = db.query(sql_models.FeedItem).filter(sql_models.FeedItem.id == item_id, sql_models.FeedItem.user_id == current_user.id).first()
    if not item:
        raise HTTPException(status_code=404, detail="Item introuvable")
    
    item.is_read = True
    db.commit()
    return {"status": "success"}

@router.patch("/{item_id}/complete")
async def mark_as_completed(
    item_id: str,
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """Marque un item comme COMPL√âT√â (Dispara√Æt du flux)."""
    item = db.query(sql_models.FeedItem).filter(sql_models.FeedItem.id == item_id, sql_models.FeedItem.user_id == current_user.id).first()
    if not item:
        raise HTTPException(status_code=404, detail="Item introuvable")
    
    item.is_completed = True
    db.commit()
    return {"status": "success"}-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/routers/performance.py
================================================================================
from fastapi import APIRouter, HTTPException
from app.models.schemas import OneRepMaxRequest, OneRepMaxResponse
from app.domain import calculations

router = APIRouter(
    prefix="/performance",
    tags=["Performance & Metrics"]
)

@router.post("/1rm", response_model=OneRepMaxResponse)
async def compute_one_rep_max(payload: OneRepMaxRequest):
    """
    Calcule le 1RM (One Rep Max) estim√© bas√© sur une performance.
    S√©lectionne automatiquement la meilleure formule (Epley, Brzycki, Wathan).
    """
    try:
        result = calculations.calculate_1rm(payload.weight, payload.reps)
        
        return {
            "estimated_1rm": result["1rm"],
            "method_used": result["method"],
            "input_weight": payload.weight,
            "input_reps": payload.reps
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/routers/safety.py
================================================================================
from fastapi import APIRouter, HTTPException
from app.models.schemas import ACWRRequest, ACWRResponse
from app.domain import safety

router = APIRouter(
    prefix="/safety",
    tags=["Safety & Prevention"]
)

@router.post("/acwr", response_model=ACWRResponse)
async def compute_acwr_metrics(payload: ACWRRequest):
    """
    Calcule le Ratio Aigu/Chronique (ACWR) pour pr√©venir les blessures.
    Envoie l'historique des s√©ances (Date, Dur√©e, RPE).
    Retourne le statut de risque (Optimal, Surcharge, Danger).
    """
    try:
        # Conversion des mod√®les Pydantic en liste de dicts pour Pandas
        history_dicts = [log.dict() for log in payload.history]
        
        result = safety.calculate_acwr(history_dicts)
        
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/routers/user.py
================================================================================
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from app.core.database import get_db
from app.models import sql_models, schemas
from app.dependencies import get_current_user
import json

router = APIRouter(
    prefix="/user",
    tags=["User Profile"]
)

@router.get("/profile", response_model=schemas.UserProfileUpdate)
async def get_profile(
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    R√©cup√®re le profil complet de l'utilisateur (JSON).
    """
    if not current_user.profile_data:
        return {"profile_data": {}}
    
    try:
        # On convertit la string stock√©e en BDD en dictionnaire
        data = json.loads(current_user.profile_data)
        return {"profile_data": data}
    except:
        return {"profile_data": {}}

@router.put("/profile")
async def update_profile(
    profile: schemas.UserProfileUpdate,
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    Sauvegarde le profil complet (Age, Benchmarks, Planning...).
    """
    try:
        # On convertit le dictionnaire re√ßu en string pour le stockage SQL
        json_str = json.dumps(profile.profile_data)
        current_user.profile_data = json_str
        
        db.commit()
        db.refresh(current_user)
        return {"message": "Profil mis √† jour avec succ√®s"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur sauvegarde: {str(e)}")-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/routers/workouts.py
================================================================================
from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks
from sqlalchemy.orm import Session
from typing import List
from app.core.database import get_db
from app.models import sql_models, schemas
from app.dependencies import get_current_user
import json

# [DEV-CARD #03] Imports du Moteur de Feed
from app.services.feed.engine import TriggerEngine
from app.services.feed.triggers.workout_analysis import WorkoutAnalysisTrigger

router = APIRouter(
    prefix="/workouts",
    tags=["Workouts"]
)

@router.post("/", response_model=schemas.WorkoutSessionResponse)
async def create_workout(
    workout: schemas.WorkoutSessionCreate, 
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    # --- VALIDATION PHYSIOLOGIQUE ---
    def validate_physiological_limits(workout: schemas.WorkoutSessionCreate):
        """Valide les limites physiologiques humaines."""
        
        # Dur√©e r√©aliste (10min √† 4h)
        if workout.duration < 10 or workout.duration > 240:
            raise HTTPException(
                status_code=400, 
                detail=f"Dur√©e invalide ({workout.duration} min). Doit √™tre entre 10 et 240 minutes."
            )
        
        # RPE 1-10
        if workout.rpe < 1 or workout.rpe > 10:
            raise HTTPException(
                status_code=400,
                detail=f"RPE invalide ({workout.rpe}). Doit √™tre entre 1 et 10."
            )
        
        # √ânergie 1-10
        if workout.energy_level < 1 or workout.energy_level > 10:
            raise HTTPException(
                status_code=400,
                detail=f"Niveau d'√©nergie invalide ({workout.energy_level}). Doit √™tre entre 1 et 10."
            )
        
        # Validation des sets
        for s in workout.sets:
            # Watts max (record du monde ~2500W)
            if s.metric_type == 'POWER_TIME' and s.weight > 2000:
                raise HTTPException(
                    status_code=400,
                    detail=f"Puissance impossible ({s.weight}W). Record du monde ~2500W."
                )
            
            # Charge max (record +500kg)
            if s.metric_type == 'LOAD_REPS' and s.weight > 500:
                raise HTTPException(
                    status_code=400,
                    detail=f"Charge impossible ({s.weight}kg). Record du monde ~500kg."
                )
            
            # RPE s√©rie
            if s.rpe and (s.rpe < 1 or s.rpe > 10):
                raise HTTPException(
                    status_code=400,
                    detail=f"RPE s√©rie invalide ({s.rpe}). Doit √™tre entre 1 et 10."
                )
        
        return True

    # Appliquer la validation
    validate_physiological_limits(workout)

    """
    Enregistre une s√©ance compl√®te avec gestion du Polymorphisme (Metric Type).
    V√©rifie la coh√©rence des donn√©es (ex: Watts max, RPE bounds).
    [DEV-CARD #05] Supprime le brouillon associ√© une fois la s√©ance valid√©e.
    [DEV-CARD #03] Active le Neural Feed pour l'analyse post-s√©ance.
    """
    # 1. Validation de haut niveau avant insertion
    for s in workout.sets:
        # Validation RPE
        if s.rpe is not None and (s.rpe < 0 or s.rpe > 10):
            # On cap plut√¥t que de crasher
            s.rpe = max(0, min(10, s.rpe))
            
        # Validation Physiologique selon le mode
        if s.metric_type == 'POWER_TIME':
            # Check Watts (weight)
            if s.weight > 2000:
                raise HTTPException(status_code=400, detail=f"Valeur impossible : {s.weight} Watts sur l'exercice {s.exercise_name}. V√©rifiez la saisie.")
        
        elif s.metric_type == 'PACE_DISTANCE':
            # Dans ce mode : weight = Vitesse/Pace (souvent 0 si calcul√©e) ou Distance, reps = Distance ou Temps
            # Standard TitanFlow : Reps = Distance (m), Weight = 0 (ou vitesse m/s)
            if s.reps > 100000: # 100km max par s√©rie pour √™tre s√ªr
                 raise HTTPException(status_code=400, detail=f"Distance suspecte : {s.reps} m√®tres.")

    # 2. Cr√©ation de la Session
    db_workout = sql_models.WorkoutSession(
        date=workout.date,
        duration=workout.duration,
        rpe=workout.rpe,
        energy_level=workout.energy_level,
        notes=workout.notes,
        user_id=current_user.id
    )
    db.add(db_workout)
    db.commit()
    db.refresh(db_workout)
    
    # 3. Ajout des S√©ries (Sets)
    if workout.sets:
        for s in workout.sets:
            # Conversion explicite Pydantic -> SQL Model
            db_set = sql_models.WorkoutSet(
                session_id=db_workout.id,
                exercise_name=s.exercise_name,
                set_order=s.set_order,
                weight=s.weight, # D√©j√† nettoy√© par Pydantic (float)
                reps=s.reps,     # D√©j√† nettoy√© par Pydantic (float, secondes inclues)
                rpe=s.rpe,
                rest_seconds=s.rest_seconds,
                metric_type=s.metric_type # Le fameux recording_mode
            )
            db.add(db_set)
        
        # [DEV-CARD #05] Nettoyage du brouillon apr√®s succ√®s
        current_user.draft_workout_data = None
        
        db.commit()
        db.refresh(db_workout)

    # 4. [DEV-CARD #03] TRIGGER NEURAL FEED (L'IA s'active ici)
    # On lance l'analyse imm√©diatement (await) pour que le feed soit √† jour 
    # quand l'utilisateur revient sur l'accueil.
    try:
        # [MODIF V2] On passe le profil complet dans le contexte via user_data
        # On tente de parser le JSON profile_data s'il existe
        profile_data = {}
        if current_user.profile_data:
            try:
                profile_data = json.loads(current_user.profile_data)
            except:
                pass

        engine = TriggerEngine()
        engine.register(WorkoutAnalysisTrigger())
        await engine.run_all(db, current_user.id, {
            "workout": db_workout,
            "profile": profile_data # <--- ICI, on injecte les donn√©es pour le Bio-Twin
        })
    except Exception as e:
        # On ne bloque pas la r√©ponse si l'IA √©choue, c'est du bonus
        print(f"‚ö†Ô∏è Feed Engine Error: {e}")
    
    return db_workout

@router.get("/", response_model=List[schemas.WorkoutSessionResponse])
async def read_workouts(
    skip: int = 0, 
    limit: int = 100, 
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    R√©cup√®re l'historique complet.
    Les champs polymorphes (weight/reps) sont renvoy√©s tels quels,
    le Frontend utilisera 'metric_type' pour savoir si c'est des kg ou des watts.
    """
    workouts = db.query(sql_models.WorkoutSession)\
        .filter(sql_models.WorkoutSession.user_id == current_user.id)\
        .order_by(sql_models.WorkoutSession.date.desc())\
        .offset(skip)\
        .limit(limit)\
        .all()
    return workouts-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/services/feed/engine.py
================================================================================
import logging
import uuid
import json
from datetime import datetime, timedelta
from typing import List, Dict, Any
from sqlalchemy.orm import Session
from sqlalchemy import and_

from app.models import sql_models, schemas
from app.services.feed.triggers.base import BaseTrigger

# Configuration des logs pour ne pas perdre une miette du match
logger = logging.getLogger(__name__)

class TriggerEngine:
    """
    Le Moteur de Jeu.
    Il poss√®de un registre de Triggers et les ex√©cute tous pour un contexte donn√©.
    Il g√®re aussi la s√©curit√© (Anti-Crash) et la filtration (D√©duplication).
    """
    def __init__(self):
        self._registry: List[BaseTrigger] = []

    def register(self, trigger: BaseTrigger):
        """Enr√¥le un nouveau Trigger dans l'√©quipe."""
        self._registry.append(trigger)
        logger.info(f"‚úÖ Trigger enregistr√© : {trigger.__class__.__name__}")

    async def run_all(self, db: Session, user_id: int, context: Dict[str, Any]) -> List[sql_models.FeedItem]:
        """
        Lance tous les Triggers enregistr√©s.
        
        R√®gles du jeu :
        1. Isolation : Si un trigger plante, les autres continuent.
        2. D√©duplication : On √©vite de spammer le m√™me message (ex: 1x par 24h).
        3. Persistance : Sauvegarde imm√©diate en base.
        """
        generated_events = []

        for trigger in self._registry:
            try:
                # Le Trigger analyse le jeu...
                event_schema = await trigger.check(user_id, context)
                
                if event_schema:
                    # Arbitrage vid√©o (D√©duplication)
                    if not self._should_discard(db, user_id, event_schema):
                        
                        # Transformation Schema -> SQL Model
                        db_item = sql_models.FeedItem(
                            id=str(uuid.uuid4()),
                            user_id=user_id,
                            type=event_schema.type,
                            title=event_schema.title,
                            message=event_schema.message,
                            priority=event_schema.priority,
                            is_read=False,
                            is_completed=False,
                            # Gestion propre du JSON payload
                            action_payload=json.dumps(event_schema.action_payload) if event_schema.action_payload else None
                        )
                        
                        db.add(db_item)
                        generated_events.append(db_item)
                        logger.info(f"üì¢ Event g√©n√©r√© : {db_item.title} ({trigger.__class__.__name__})")
                    else:
                        logger.info(f"üîá Event ignor√© (Doublon) : {event_schema.title}")

            except Exception as e:
                # Carton jaune : Le trigger a plant√©, mais le match continue
                logger.error(f"‚ö†Ô∏è Erreur Trigger {trigger.__class__.__name__}: {str(e)}")
                continue

        # Coup de sifflet final : on valide les buts
        if generated_events:
            db.commit()
            for ev in generated_events:
                db.refresh(ev)
                
        return generated_events

    def _should_discard(self, db: Session, user_id: int, event: schemas.FeedItemCreate) -> bool:
        """
        V√©rifie si un √©v√©nement similaire existe d√©j√† r√©cemment.
        R√®gle actuelle : Pas de doublon (M√™me Titre + M√™me Type) non trait√©.
        Ou pas de doublon identique cr√©√© dans les derni√®res 24h.
        """
        # 1. Chercher si le m√™me event est d√©j√† en attente (Non compl√©t√©)
        existing_active = db.query(sql_models.FeedItem).filter(
            sql_models.FeedItem.user_id == user_id,
            sql_models.FeedItem.type == event.type,
            sql_models.FeedItem.title == event.title,
            sql_models.FeedItem.is_completed == False
        ).first()

        if existing_active:
            return True # On jette, l'utilisateur a d√©j√† √ßa dans son feed

        # 2. Chercher si le m√™me event a √©t√© cr√©√© il y a moins de 24h (Anti-Spam)
        one_day_ago = datetime.utcnow() - timedelta(hours=24)
        recent_duplicate = db.query(sql_models.FeedItem).filter(
            sql_models.FeedItem.user_id == user_id,
            sql_models.FeedItem.type == event.type,
            sql_models.FeedItem.title == event.title,
            sql_models.FeedItem.created_at >= one_day_ago
        ).first()

        if recent_duplicate:
            return True

        return False-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/services/feed/triggers/base.py
================================================================================
from abc import ABC, abstractmethod
from typing import Optional, Dict, Any
from app.models import schemas

class BaseTrigger(ABC):
    """
    Interface abstraite pour tous les d√©clencheurs d'√©v√©nements (Triggers).
    Chaque Trigger est un 'sp√©cialiste' (ex: Sp√©cialiste Analyse, Sp√©cialiste Sant√©).
    """

    @abstractmethod
    async def check(self, user_id: int, context: Dict[str, Any]) -> Optional[schemas.FeedItemCreate]:
        """
        Analyse le contexte et retourne un FeedItemCreate si la condition est remplie.
        Retourne None sinon.
        
        :param user_id: L'ID de l'athl√®te concern√©.
        :param context: Un dictionnaire riche contenant les donn√©es (ex: {'workout': ..., 'profile': ...})
        :return: Un objet FeedItemCreate pr√™t √† √™tre ins√©r√©, ou None.
        """
        pass-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/services/feed/triggers/workout_analysis.py
================================================================================
import os
import json
import re
import google.generativeai as genai
from typing import Dict, Any, Optional
from app.services.feed.triggers.base import BaseTrigger
from app.models import schemas, sql_models
from app.domain.bioenergetics import BioenergeticService

class WorkoutAnalysisTrigger(BaseTrigger):
    """
    Trigger : Analyse Post-S√©ance Avanc√©e (Bio-Twin + Gemini).
    Condition : Une s√©ance vient d'√™tre termin√©e.
    Action : 
        1. Calcule les m√©triques bio√©nerg√©tiques (Kcal, Macros).
        2. G√©n√®re un rapport JSON complet via IA.
        3. Sauvegarde le rapport dans la s√©ance (Persistance).
        4. Cr√©e une carte Feed pour notifier l'athl√®te.
    """
    
    def __init__(self):
        self.api_key = os.getenv("GEMINI_API_KEY")

    async def check(self, user_id: int, context: Dict[str, Any]) -> Optional[schemas.FeedItemCreate]:
        # 1. V√©rifie si le contexte contient les donn√©es requises
        workout: sql_models.WorkoutSession = context.get("workout")
        profile_data: Dict[str, Any] = context.get("profile", {})
        
        if not workout:
            return None

        # 2. Si pas de cl√© API, on sort silencieusement
        if not self.api_key:
            return None

        try:
            # 3. PHASE 1 : CALCULS BIO√âNERG√âTIQUES (Les Maths)
            bio_metrics = BioenergeticService.calculate_needs(
                profile_data, 
                workout.sets, 
                workout.duration, 
                workout.rpe
            )
            
            # 4. PHASE 2 : G√âN√âRATION IA (Le Cerveau)
            sets_summary = "\n".join([
                f"- {s.exercise_name}: {s.weight} (load/watts) x {s.reps} (reps/sec/m) [{s.metric_type}]"
                for s in workout.sets
            ])
            
            prompt = f"""
            R√îLE : Expert en Physiologie Sportive et Nutrition (TitanFlow).
            TACHE : Analyser la s√©ance et g√©n√©rer un rapport JSON strict.

            === DONN√âES ATHL√àTE ===
            - Profil : {json.dumps(profile_data, ensure_ascii=False)}
            
            === DONN√âES S√âANCE ===
            - Dur√©e : {workout.duration} min
            - RPE : {workout.rpe}/10 (Intensit√© Ressentie)
            - Contenu :
            {sets_summary}
            
            === DONN√âES BIO-TWIN (CALCUL√âES) ===
            - D√©pense : ~{bio_metrics['kcal_total']} kcal
            - Besoins Post-Effort (Estim√©s) : 
              * Prot√©ines : {bio_metrics['protein_g']}g
              * Glucides : {bio_metrics['carbs_g']}g
              * Eau : {bio_metrics['water_ml']}ml
            
            === STRUCTURE DE SORTIE (JSON UNIQUEMENT) ===
            {{
              "performance_analysis": "Analyse technique de la charge et du volume en 2 phrases max.",
              "nutrition_comment": "Conseil pr√©cis validant ou ajustant les macros calcul√©es ci-dessus.",
              "recovery_score": 8,
              "coach_questions": ["Question pertinente 1?", "Question pertinente 2?"],
              "food_suggestion": {{
                  "option_shake": "Ex: Whey + Banane",
                  "option_solid": "Ex: Poulet + Riz + L√©gumes"
              }},
              "feed_message": "Une phrase d'accroche tr√®s courte (max 12 mots) pour la notification."
            }}
            """

            genai.configure(api_key=self.api_key)
            model = genai.GenerativeModel('gemini-2.0-flash', generation_config={"response_mime_type": "application/json"})
            response = model.generate_content(prompt)
            
            # Nettoyage JSON
            json_str = self._clean_json(response.text)
            analysis_result = json.loads(json_str)

            # 5. PHASE 3 : PERSISTANCE (Sauvegarde en BDD)
            # On fusionne les m√©triques calcul√©es avec l'analyse IA
            full_report = {
                **analysis_result,
                "bio_metrics": bio_metrics
            }
            
            # On stocke le JSON stringifi√© dans la colonne ai_analysis de la s√©ance
            # Note: L'objet 'workout' est attach√© √† la session DB, donc le commit du TriggerEngine validera cette modif.
            workout.ai_analysis = json.dumps(full_report)

            # 6. PHASE 4 : NOTIFICATION (Le Feed)
            # On utilise le message court g√©n√©r√© par l'IA pour le feed
            feed_msg = analysis_result.get("feed_message", "Analyse de s√©ance disponible.")

            return schemas.FeedItemCreate(
                type=schemas.FeedItemType.ANALYSIS,
                title="Rapport de S√©ance",
                message=feed_msg,
                priority=5,
                action_payload={
                    "route": "/history", 
                    # On pourra passer des args pour ouvrir directement le d√©tail plus tard
                    "args": {"workout_id": workout.id}
                }
            )

        except Exception as e:
            print(f"‚ö†Ô∏è Erreur IA Analysis: {e}")
            # Fallback : Si l'IA plante, on ne cr√©e pas de FeedItem, 
            # ou on pourrait en cr√©er un g√©n√©rique. Ici on choisit la discr√©tion.
            return None

    def _clean_json(self, text: str) -> str:
        """Extrait le JSON si l'IA ajoute du markdown."""
        try:
            pattern = r"```(?:json)?\s*([\s\S]*?)\s*```"
            match = re.search(pattern, text)
            if match:
                return match.group(1).strip()
            return text.strip()
        except:
            return text-e 

-e 
================================================================================
üìÑ FICHIER : backend/check_jwt_config.py
================================================================================
import os
import jwt
from datetime import datetime, timedelta
from dotenv import load_dotenv

load_dotenv()

SECRET_KEY = os.getenv("SECRET_KEY")
ALGORITHM = os.getenv("ALGORITHM", "HS256")
ACCESS_TOKEN_EXPIRE_MINUTES = int(os.getenv("ACCESS_TOKEN_EXPIRE_MINUTES", 30))

print("üîß Configuration JWT Actuelle:")
print(f"   SECRET_KEY: {'SET' if SECRET_KEY else 'NOT SET'}")
print(f"   ALGORITHM: {ALGORITHM}")
print(f"   ACCESS_TOKEN_EXPIRE_MINUTES: {ACCESS_TOKEN_EXPIRE_MINUTES} min")

# V√©rifier si on peut g√©n√©rer un token
if SECRET_KEY:
    print("\nüß™ Test de g√©n√©ration de token...")
    
    data = {"sub": "testuser", "exp": datetime.utcnow() + timedelta(minutes=30)}
    token = jwt.encode(data, SECRET_KEY, algorithm=ALGORITHM)
    
    print(f"   Token g√©n√©r√©: {token[:50]}...")
    
    # V√©rifier qu'on peut le d√©coder
    try:
        decoded = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        print(f"   ‚úÖ Token d√©cod√©: {decoded}")
    except Exception as e:
        print(f"   ‚ùå Erreur d√©codage: {e}")
else:
    print("\n‚ùå SECRET_KEY non d√©finie!")
    print("   D√©finissez-la dans .env:")
    print("   SECRET_KEY=votre_clef_secrete_tres_longue_ici")
-e 

-e 
================================================================================
üìÑ FICHIER : backend/check_model.py
================================================================================
from app.models.sql_models import User
from sqlalchemy import inspect

print("üìã Colonnes d√©finies dans le mod√®le User:")
for column in User.__table__.columns:
    print(f"  ‚Ä¢ {column.name} ({column.type})")
-e 

-e 
================================================================================
üìÑ FICHIER : backend/debug_tokens.py
================================================================================
#!/usr/bin/env python3
"""
Debug JWT tokens - V√©rifie pourquoi les tokens sont rejet√©s
"""

import jwt
import os
from datetime import datetime
import sys

# Charger les variables d'environnement
from dotenv import load_dotenv
load_dotenv()

SECRET_KEY = os.getenv("SECRET_KEY", "fallback_secret_key_if_env_missing")
ALGORITHM = "HS256"

def decode_and_verify(token: str):
    """D√©code et v√©rifie un token JWT"""
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        
        print("‚úÖ TOKEN VALIDE")
        print(f"üìã Payload: {payload}")
        
        # V√©rifier l'expiration
        exp_timestamp = payload.get('exp')
        if exp_timestamp:
            exp_date = datetime.fromtimestamp(exp_timestamp)
            now = datetime.now()
            time_left = exp_date - now
            
            print(f"‚è∞ Expiration: {exp_date}")
            print(f"‚è≥ Temps restant: {time_left}")
            
            if time_left.total_seconds() < 0:
                print("‚ùå TOKEN EXPIRE !")
            else:
                print("‚úÖ Token encore valide")
        
        return payload
    except jwt.ExpiredSignatureError:
        print("‚ùå ERREUR: Token expir√©")
        return None
    except jwt.InvalidTokenError as e:
        print(f"‚ùå ERREUR: Token invalide - {e}")
        return None

def main():
    if len(sys.argv) < 2:
        print("Usage: python debug_tokens.py <token>")
        print("Ou: python debug_tokens.py --check-all")
        return
    
    if sys.argv[1] == "--check-all":
        # V√©rifier les tokens stock√©s dans un √©chantillon
        sample_tokens = []
        # Vous pouvez ajouter des tokens de test ici
        for token in sample_tokens:
            decode_and_verify(token)
    else:
        token = sys.argv[1]
        decode_and_verify(token)

if __name__ == "__main__":
    main()-e 

-e 
================================================================================
üìÑ FICHIER : backend/disable_coach_cache.py
================================================================================
import re

with open('app/routers/coach.py', 'r') as f:
    content = f.read()

# Supprimer TOUTES les r√©f√©rences √† cache dans ce fichier
content = re.sub(r'from app\.core\.cache import[^\n]*\n', '', content)
content = re.sub(r'@cached_response[^\n]*\n', '', content)

with open('app/routers/coach.py', 'w') as f:
    f.write(new_content)

print("‚úÖ Cache compl√®tement d√©sactiv√© pour coach.py")
-e 

-e 
================================================================================
üìÑ FICHIER : backend/fix_coach_cache.py
================================================================================
#!/usr/bin/env python3
"""
Retire le d√©corateur @cached_response de la fonction generate_workout
qui cause l'erreur de s√©rialisation JSON
"""

import re

with open('app/routers/coach.py', 'r') as f:
    content = f.read()

# Trouver la fonction generate_workout
pattern = r'@cached_response\(ttl_hours=6\)\s*\nasync def generate_workout'
match = re.search(pattern, content)

if match:
    print("üîß Retrait du d√©corateur @cached_response probl√©matique...")
    
    # Retirer la ligne du d√©corateur
    new_content = content.replace(match.group(0), 'async def generate_workout')
    
    with open('app/routers/coach.py', 'w') as f:
        f.write(new_content)
    
    print("‚úÖ D√©corateur retir√© avec succ√®s")
else:
    print("‚úÖ Le d√©corateur n'est pas pr√©sent ou a d√©j√† √©t√© retir√©")
-e 

-e 
================================================================================
üìÑ FICHIER : backend/fix_table.py
================================================================================
from sqlalchemy import text
from app.core.database import engine

print("‚ò¢Ô∏è  D√©marrage de l'option nucl√©aire...")

with engine.connect() as connection:
    trans = connection.begin()
    try:
        # On supprime d'abord les s√©ances (qui d√©pendent des users)
        connection.execute(text("DROP TABLE IF EXISTS workout_sessions CASCADE;"))
        print("üí• Table workout_sessions pulv√©ris√©e.")
        
        # On supprime ensuite les users (pour recr√©er la table avec l'email)
        connection.execute(text("DROP TABLE IF EXISTS users CASCADE;"))
        print("üí• Table users pulv√©ris√©e.")
        
        trans.commit()
    except Exception as e:
        trans.rollback()
        print(f"‚ùå Erreur : {e}")

print("‚úÖ Termin√©. Red√©marre le serveur pour recr√©er les tables propres.")-e 

-e 
================================================================================
üìÑ FICHIER : backend/frontend/app.py
================================================================================
import streamlit as st
import requests
import pandas as pd
import os  # <--- Ajout de l'import os

# Configuration de la page
st.set_page_config(page_title="TitanFlow Pro", page_icon="‚ö°", layout="wide")

# L'URL de ton API (Backend)
# En PROD (Render) : Il utilisera la variable d'environnement BACKEND_URL
# En LOCAL (Ton PC) : Il utilisera http://127.0.0.1:8000 par d√©faut
API_URL = os.getenv("BACKEND_URL", "http://127.0.0.1:8000")

# --- GESTION DE LA SESSION (Token) ---
if "token" not in st.session_state:
    st.session_state.token = None

def login():
    st.sidebar.header("üîê Connexion")
    username = st.sidebar.text_input("Pseudo")
    password = st.sidebar.text_input("Mot de passe", type="password")
    
    if st.sidebar.button("Se connecter"):
        try:
            # Appel √† l'API pour r√©cup√©rer le token
            response = requests.post(
                f"{API_URL}/auth/token",
                data={"username": username, "password": password}
            )
            if response.status_code == 200:
                st.session_state.token = response.json()["access_token"]
                st.sidebar.success("Connect√© !")
                st.rerun()
            else:
                st.sidebar.error("Erreur de connexion")
        except Exception as e:
            st.sidebar.error(f"API introuvable : {e}")

def logout():
    if st.sidebar.button("Se d√©connecter"):
        st.session_state.token = None
        st.rerun()

# --- INTERFACE PRINCIPALE ---
st.title("‚ö° TitanFlow : Monitoring Athl√©tique")

# V√©rification de l'√©tat de l'API
try:
    health = requests.get(f"{API_URL}/health").json()
    st.success(f"Backend connect√© v{health['version']}")
except:
    st.error("üö® Le Backend semble √©teint. V√©rifie que l'URL est correcte.")

# Gestion Login/Logout
if not st.session_state.token:
    st.info("Veuillez vous connecter dans la barre lat√©rale pour acc√©der aux donn√©es.")
    login()
else:
    logout()
    st.write("---")
    
    # Onglets de l'application
    tab1, tab2 = st.tabs(["üèãÔ∏è‚Äç‚ôÇÔ∏è Historique", "‚ûï Nouvelle S√©ance"])
    
    # --- ONGLET 1 : HISTORIQUE ---
    with tab1:
        st.subheader("Vos s√©ances enregistr√©es")
        headers = {"Authorization": f"Bearer {st.session_state.token}"}
        
        try:
            res = requests.get(f"{API_URL}/workouts/", headers=headers)
            if res.status_code == 200:
                workouts = res.json()
                if workouts:
                    df = pd.DataFrame(workouts)
                    st.dataframe(df, use_container_width=True)
                else:
                    st.info("Aucune s√©ance trouv√©e.")
            else:
                st.error("Erreur chargement donn√©es")
        except Exception as e:
            st.error(f"Erreur : {e}")

    # --- ONGLET 2 : AJOUTER S√âANCE ---
    with tab2:
        st.subheader("Enregistrer un entra√Ænement")
        with st.form("new_workout"):
            col1, col2 = st.columns(2)
            date = col1.date_input("Date")
            duration = col2.number_input("Dur√©e (min)", min_value=0, value=60)
            rpe = st.slider("Intensit√© (RPE)", 0, 10, 5)
            
            submitted = st.form_submit_button("Sauvegarder")
            
            if submitted:
                payload = {
                    "date": str(date),
                    "duration": duration,
                    "rpe": rpe
                }
                res = requests.post(f"{API_URL}/workouts/", json=payload, headers=headers)
                
                if res.status_code == 200:
                    st.success("S√©ance enregistr√©e ! üéâ")
                    st.rerun()
                else:
                    st.error(f"Erreur : {res.text}")-e 

-e 
================================================================================
üìÑ FICHIER : backend/init_db.py
================================================================================
from app.core.database import engine, Base
from app.models import sql_models

print("üèóÔ∏è  Force-Cr√©ation des tables en cours...")
try:
    Base.metadata.create_all(bind=engine)
    print("‚úÖ  Succ√®s ! Toutes les tables (users + workout_sessions) sont pr√™tes.")
except Exception as e:
    print(f"‚ùå  Erreur : {e}")-e 

-e 
================================================================================
üìÑ FICHIER : backend/migrate_db.py
================================================================================
import os
import sqlalchemy
from sqlalchemy import text, inspect
from dotenv import load_dotenv
import logging
import sys

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Charge les variables locales
load_dotenv()

DATABASE_URL = os.getenv("DATABASE_URL")

# Correction pour Render qui utilise parfois postgres:// au lieu de postgresql://
if DATABASE_URL and DATABASE_URL.startswith("postgres://"):
    DATABASE_URL = DATABASE_URL.replace("postgres://", "postgresql://", 1)

if not DATABASE_URL:
    logger.error("‚ùå Erreur : Pas de DATABASE_URL trouv√©e.")
    sys.exit(1)

logger.info(f"üîå Connexion √† la BDD...")
engine = sqlalchemy.create_engine(DATABASE_URL)

def table_exists(connection, table_name):
    """V√©rifie si une table existe"""
    inspector = inspect(engine)
    return table_name in inspector.get_table_names()

def column_exists(connection, table_name, column_name):
    """V√©rifie si une colonne existe dans une table"""
    inspector = inspect(engine)
    columns = [col['name'] for col in inspector.get_columns(table_name)]
    return column_name in columns

def apply_migration():
    """Applique toutes les migrations n√©cessaires"""
    with engine.connect() as connection:
        trans = connection.begin()
        try:
            logger.info("üõ†Ô∏è D√©but des migrations...")
            
            # 1. CR√âATION DES TABLES DE BASE (si elles n'existent pas)
            
            # Table users
            if not table_exists(connection, "users"):
                logger.info("Cr√©ation de la table 'users'...")
                connection.execute(text("""
                    CREATE TABLE users (
                        id SERIAL PRIMARY KEY,
                        username VARCHAR UNIQUE NOT NULL,
                        email VARCHAR UNIQUE,
                        hashed_password VARCHAR NOT NULL,
                        profile_data TEXT,
                        strategy_data TEXT,
                        weekly_plan_data TEXT,
                        draft_workout_data TEXT
                    );
                """))
                logger.info("‚úÖ Table 'users' cr√©√©e.")
            
            # Table workout_sessions
            if not table_exists(connection, "workout_sessions"):
                logger.info("Cr√©ation de la table 'workout_sessions'...")
                connection.execute(text("""
                    CREATE TABLE workout_sessions (
                        id SERIAL PRIMARY KEY,
                        user_id INTEGER REFERENCES users(id) ON DELETE CASCADE,
                        date DATE NOT NULL,
                        duration FLOAT NOT NULL,
                        rpe FLOAT NOT NULL,
                        energy_level INTEGER DEFAULT 5,
                        notes TEXT,
                        ai_analysis TEXT,
                        created_at TIMESTAMPTZ DEFAULT NOW()
                    );
                """))
                logger.info("‚úÖ Table 'workout_sessions' cr√©√©e.")
            
            # Table workout_sets
            if not table_exists(connection, "workout_sets"):
                logger.info("Cr√©ation de la table 'workout_sets'...")
                connection.execute(text("""
                    CREATE TABLE workout_sets (
                        id SERIAL PRIMARY KEY,
                        session_id INTEGER REFERENCES workout_sessions(id) ON DELETE CASCADE,
                        exercise_name VARCHAR NOT NULL,
                        set_order INTEGER NOT NULL,
                        weight FLOAT DEFAULT 0.0,
                        reps FLOAT DEFAULT 0.0,
                        rpe FLOAT DEFAULT 0.0,
                        rest_seconds INTEGER DEFAULT 0,
                        metric_type VARCHAR DEFAULT 'LOAD_REPS'
                    );
                """))
                logger.info("‚úÖ Table 'workout_sets' cr√©√©e.")
            
            # Table feed_items
            if not table_exists(connection, "feed_items"):
                logger.info("Cr√©ation de la table 'feed_items'...")
                connection.execute(text("""
                    CREATE TABLE feed_items (
                        id VARCHAR PRIMARY KEY,
                        user_id INTEGER REFERENCES users(id) ON DELETE CASCADE,
                        type VARCHAR NOT NULL,
                        title VARCHAR NOT NULL,
                        message VARCHAR NOT NULL,
                        action_payload TEXT,
                        is_read BOOLEAN DEFAULT FALSE,
                        is_completed BOOLEAN DEFAULT FALSE,
                        priority INTEGER DEFAULT 1,
                        created_at TIMESTAMPTZ DEFAULT NOW()
                    );
                """))
                logger.info("‚úÖ Table 'feed_items' cr√©√©e.")
            
            # 2. MIGRATIONS POUR LES COLONNES MANQUANTES (si tables existent)
            
            # Migration pour users
            if table_exists(connection, "users"):
                migrations_users = [
                    ("profile_data", "ALTER TABLE users ADD COLUMN IF NOT EXISTS profile_data TEXT;"),
                    ("strategy_data", "ALTER TABLE users ADD COLUMN IF NOT EXISTS strategy_data TEXT;"),
                    ("weekly_plan_data", "ALTER TABLE users ADD COLUMN IF NOT EXISTS weekly_plan_data TEXT;"),
                    ("draft_workout_data", "ALTER TABLE users ADD COLUMN IF NOT EXISTS draft_workout_data TEXT;"),
                ]
                
                for col_name, sql in migrations_users:
                    if not column_exists(connection, "users", col_name):
                        logger.info(f"Ajout de la colonne '{col_name}' √† la table 'users'...")
                        connection.execute(text(sql))
                        logger.info(f"‚úÖ Colonne '{col_name}' ajout√©e.")
            
            # Migration pour workout_sessions
            if table_exists(connection, "workout_sessions"):
                migrations_sessions = [
                    ("energy_level", "ALTER TABLE workout_sessions ADD COLUMN IF NOT EXISTS energy_level INTEGER DEFAULT 5;"),
                    ("notes", "ALTER TABLE workout_sessions ADD COLUMN IF NOT EXISTS notes TEXT;"),
                    ("created_at", "ALTER TABLE workout_sessions ADD COLUMN IF NOT EXISTS created_at TIMESTAMPTZ DEFAULT NOW();"),
                    ("ai_analysis", "ALTER TABLE workout_sessions ADD COLUMN IF NOT EXISTS ai_analysis TEXT;"),  # <-- LA COLONNE MANQUANTE
                ]
                
                for col_name, sql in migrations_sessions:
                    if not column_exists(connection, "workout_sessions", col_name):
                        logger.info(f"Ajout de la colonne '{col_name}' √† la table 'workout_sessions'...")
                        connection.execute(text(sql))
                        logger.info(f"‚úÖ Colonne '{col_name}' ajout√©e.")
            
            # Migration pour workout_sets
            if table_exists(connection, "workout_sets"):
                migrations_sets = [
                    ("metric_type", "ALTER TABLE workout_sets ADD COLUMN IF NOT EXISTS metric_type VARCHAR DEFAULT 'LOAD_REPS';"),
                    ("rest_seconds", "ALTER TABLE workout_sets ADD COLUMN IF NOT EXISTS rest_seconds INTEGER DEFAULT 0;"),
                ]
                
                for col_name, sql in migrations_sets:
                    if not column_exists(connection, "workout_sets", col_name):
                        logger.info(f"Ajout de la colonne '{col_name}' √† la table 'workout_sets'...")
                        connection.execute(text(sql))
                        logger.info(f"‚úÖ Colonne '{col_name}' ajout√©e.")
            
            # 3. CR√âATION DES INDEX POUR LES PERFORMANCES
            logger.info("Cr√©ation des index...")
            indexes = [
                "CREATE INDEX IF NOT EXISTS idx_workout_sessions_user_id ON workout_sessions(user_id);",
                "CREATE INDEX IF NOT EXISTS idx_workout_sessions_date ON workout_sessions(date);",
                "CREATE INDEX IF NOT EXISTS idx_workout_sets_session_id ON workout_sets(session_id);",
                "CREATE INDEX IF NOT EXISTS idx_feed_items_user_id ON feed_items(user_id);",
                "CREATE INDEX IF NOT EXISTS idx_feed_items_created_at ON feed_items(created_at);",
                "CREATE INDEX IF NOT EXISTS idx_feed_items_type ON feed_items(type);",
            ]
            
            for idx_sql in indexes:
                try:
                    connection.execute(text(idx_sql))
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Impossible de cr√©er l'index: {e}")
            
            trans.commit()
            logger.info("üéâ Toutes les migrations ont √©t√© appliqu√©es avec succ√®s !")
            
            # 4. V√âRIFICATION FINALE
            logger.info("üîç V√©rification finale des colonnes...")
            check_tables = ["users", "workout_sessions", "workout_sets", "feed_items"]
            
            for table in check_tables:
                if table_exists(connection, table):
                    inspector = inspect(engine)
                    columns = inspector.get_columns(table)
                    logger.info(f"Table '{table}' a {len(columns)} colonnes:")
                    for col in columns:
                        logger.info(f"  - {col['name']} ({col['type']})")
            
            return True
            
        except Exception as e:
            trans.rollback()
            logger.error(f"‚ùå Erreur lors de la migration : {str(e)}")
            return False

if __name__ == "__main__":
    logger.info("üöÄ Lancement du script de migration de la base de donn√©es...")
    
    success = apply_migration()
    
    if success:
        logger.info("‚úÖ Migration termin√©e avec succ√®s !")
        sys.exit(0)
    else:
        logger.error("‚ùå La migration a √©chou√©.")
        sys.exit(1)-e 

-e 
================================================================================
üìÑ FICHIER : backend/remove_cache_decorator.py
================================================================================
import re

with open('app/routers/coach.py', 'r') as f:
    content = f.read()

# Supprimer toutes les occurrences de @cached_response
new_content = re.sub(r'@cached_response\([^)]*\)\s*\n', '', content)

with open('app/routers/coach.py', 'w') as f:
    f.write(new_content)

print("‚úÖ Tous les d√©corateurs @cached_response retir√©s")
-e 

-e 
================================================================================
üìÑ FICHIER : frontend/app.py
================================================================================
import streamlit as st
import requests
import pandas as pd
from datetime import datetime

# Configuration de la page
st.set_page_config(page_title="TitanFlow Pro", page_icon="‚ö°", layout="wide")

# L'URL de ton API (Backend)
API_URL = "http://127.0.0.1:8000"

# --- GESTION DE LA SESSION (Token) ---
if "token" not in st.session_state:
    st.session_state.token = None

def login():
    st.sidebar.header("üîê Connexion")
    username = st.sidebar.text_input("Pseudo")
    password = st.sidebar.text_input("Mot de passe", type="password")
    
    if st.sidebar.button("Se connecter"):
        try:
            # Appel √† l'API pour r√©cup√©rer le token
            response = requests.post(
                f"{API_URL}/auth/token",
                data={"username": username, "password": password}
            )
            if response.status_code == 200:
                st.session_state.token = response.json()["access_token"]
                st.sidebar.success("Connect√© !")
                st.rerun()
            else:
                st.sidebar.error("Erreur de connexion")
        except Exception as e:
            st.sidebar.error(f"API introuvable : {e}")

def logout():
    if st.sidebar.button("Se d√©connecter"):
        st.session_state.token = None
        st.rerun()

# --- INTERFACE PRINCIPALE ---
st.title("‚ö° TitanFlow : Monitoring Athl√©tique")

# V√©rification de l'√©tat de l'API
try:
    health = requests.get(f"{API_URL}/health").json()
    st.success(f"Backend connect√© v{health['version']}")
except:
    st.error("üö® Le Backend semble √©teint. V√©rifie que le Terminal 1 tourne bien !")

# Gestion Login/Logout
if not st.session_state.token:
    st.info("Veuillez vous connecter dans la barre lat√©rale pour acc√©der aux donn√©es.")
    login()
else:
    logout()
    st.write("---")
    
    # Onglets de l'application
    tab1, tab2 = st.tabs(["üèãÔ∏è‚Äç‚ôÇÔ∏è Historique", "‚ûï Nouvelle S√©ance"])
    
    # --- ONGLET 1 : HISTORIQUE ---
    with tab1:
        st.subheader("Vos s√©ances enregistr√©es")
        headers = {"Authorization": f"Bearer {st.session_state.token}"}
        
        try:
            res = requests.get(f"{API_URL}/workouts/", headers=headers)
            if res.status_code == 200:
                workouts = res.json()
                if workouts:
                    df = pd.DataFrame(workouts)
                    st.dataframe(df, use_container_width=True)
                else:
                    st.info("Aucune s√©ance trouv√©e.")
            else:
                st.error("Erreur chargement donn√©es")
        except Exception as e:
            st.error(f"Erreur : {e}")

    # --- ONGLET 2 : AJOUTER S√âANCE ---
    with tab2:
        st.subheader("Enregistrer un entra√Ænement")
        with st.form("new_workout"):
            col1, col2 = st.columns(2)
            date = col1.date_input("Date")
            duration = col2.number_input("Dur√©e (min)", min_value=0, value=60)
            rpe = st.slider("Intensit√© (RPE)", 0, 10, 5)
            
            submitted = st.form_submit_button("Sauvegarder")
            
            if submitted:
                # Pr√©paration du JSON
                payload = {
                    "date": str(date),
                    "duration": duration,
                    "rpe": rpe
                }
                # Envoi √† l'API
                headers = {"Authorization": f"Bearer {st.session_state.token}"}
                res = requests.post(f"{API_URL}/workouts/", json=payload, headers=headers)
                
                if res.status_code == 200:
                    st.success("S√©ance enregistr√©e ! üéâ")
                    # Petit hack pour rafra√Æchir l'historique
                    st.rerun()
                else:
                    st.error(f"Erreur : {res.text}")
-e 


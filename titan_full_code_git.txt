-e 
================================================================================
üìÑ FICHIER : README.md
================================================================================

-e 

-e 
================================================================================
üìÑ FICHIER : backend/add_safe_cache.py
================================================================================
import re

with open('app/routers/coach.py', 'r') as f:
    content = f.read()

# Trouver generate_workout et ajouter un d√©corateur safe
func_pattern = r'async def generate_workout\([\s\S]*?\):'
match = re.search(func_pattern, content)

if match:
    func_start = match.start()
    
    # Ajouter le d√©corateur safe (ignore current_user)
    decorated_func = '@cached_response_fixed(ttl_hours=6, ignore_args=["current_user"])\n' + match.group(0)
    
    new_content = content[:func_start] + decorated_func + content[func_start + len(match.group(0)):]
    
    with open('app/routers/coach.py', 'w') as f:
        f.write(new_content)
    
    print("‚úÖ D√©corateur safe ajout√© √† generate_workout")
else:
    print("‚ö†Ô∏è  Impossible de trouver generate_workout")
-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/__init__.py
================================================================================
-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/core/__init__.py
================================================================================
-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/core/cache.py
================================================================================
"""
Cache intelligent pour les appels Gemini IA.
√âconomise les co√ªts API et am√©liore la r√©activit√©.
"""
import json
import hashlib
from datetime import datetime, timedelta
from typing import Any, Optional
from functools import wraps
import logging

logger = logging.getLogger(__name__)

class IntelligentCache:
    """Cache m√©moire avec expiration et invalidation intelligente."""
    
    def __init__(self, default_ttl_hours: int = 24):
        self._cache = {}
        self.default_ttl = default_ttl_hours
    
    def _generate_key(self, *args, **kwargs) -> str:
        """G√©n√®re une cl√© unique √† partir des param√®tres."""
        data = json.dumps({
            'args': args,
            'kwargs': kwargs
        }, sort_keys=True)
        return hashlib.md5(data.encode()).hexdigest()
    
    def get(self, key: str) -> Optional[Any]:
        """R√©cup√®re un √©l√©ment du cache s'il est valide."""
        if key in self._cache:
            entry = self._cache[key]
            if datetime.now() < entry['expires_at']:
                logger.debug(f"üì¶ Cache hit: {key}")
                return entry['data']
            else:
                del self._cache[key]
                logger.debug(f"üßπ Cache expired: {key}")
        return None
    
    def set(self, key: str, data: Any, ttl_hours: Optional[int] = None):
        """Stocke un √©l√©ment dans le cache."""
        ttl = ttl_hours if ttl_hours is not None else self.default_ttl
        self._cache[key] = {
            'data': data,
            'expires_at': datetime.now() + timedelta(hours=ttl),
            'created_at': datetime.now()
        }
        logger.debug(f"üíæ Cache stored: {key} (TTL: {ttl}h)")
    
    def clear_old_entries(self):
        """Nettoie les entr√©es expir√©es."""
        now = datetime.now()
        expired_keys = [
            k for k, v in self._cache.items() 
            if now >= v['expires_at']
        ]
        for k in expired_keys:
            del self._cache[k]
        if expired_keys:
            logger.info(f"üßπ Cache cleanup: {len(expired_keys)} entr√©es expir√©es")

# Instance globale
ai_cache = IntelligentCache(default_ttl_hours=6)  # 6h pour les plans IA

def cached_response(ttl_hours: int = 6):
    """D√©corateur pour mettre en cache les r√©ponses IA."""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # G√©n√©rer une cl√© unique
            cache_key = f"{func.__name__}:{ai_cache._generate_key(*args, **kwargs)}"
            
            # V√©rifier le cache
            cached = ai_cache.get(cache_key)
            if cached is not None:
                return cached
            
            # Ex√©cuter la fonction
            result = await func(*args, **kwargs)
            
            # Mettre en cache
            if result is not None:
                ai_cache.set(cache_key, result, ttl_hours)
            
            return result
        return wrapper
    return decorator
-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/core/cache_fixed.py
================================================================================
"""
Version corrig√©e du d√©corateur de cache qui ignore les objets non s√©rialisables
"""

import json
import hashlib
from datetime import datetime, timedelta
from typing import Any, Optional, Callable
from functools import wraps
import logging

logger = logging.getLogger(__name__)

class FixedIntelligentCache:
    """Cache m√©moire avec gestion des objets non s√©rialisables."""
    
    def __init__(self, default_ttl_hours: int = 24):
        self._cache = {}
        self.default_ttl = default_ttl_hours
    
    def _safe_serialize(self, obj: Any) -> Any:
        """S√©rialise en toute s√©curit√©, convertissant les objets SQLAlchemy en dict."""
        if hasattr(obj, '__dict__'):
            # Si c'est un mod√®le SQLAlchemy, on prend son ID
            if hasattr(obj, 'id'):
                return f"{obj.__class__.__name__}:{obj.id}"
            # Sinon, on convertit en dict sans les relations
            return {k: self._safe_serialize(v) for k, v in obj.__dict__.items() 
                    if not k.startswith('_')}
        elif isinstance(obj, (list, tuple)):
            return [self._safe_serialize(item) for item in obj]
        elif isinstance(obj, dict):
            return {k: self._safe_serialize(v) for k, v in obj.items()}
        else:
            return obj
    
    def _generate_key(self, *args, **kwargs) -> str:
        """G√©n√®re une cl√© unique en s√©rialisant en toute s√©curit√©."""
        safe_args = self._safe_serialize(args)
        safe_kwargs = self._safe_serialize(kwargs)
        
        data = json.dumps({
            'args': safe_args,
            'kwargs': safe_kwargs
        }, sort_keys=True, default=str)
        return hashlib.md5(data.encode()).hexdigest()
    
    def get(self, key: str) -> Optional[Any]:
        """R√©cup√®re un √©l√©ment du cache s'il est valide."""
        if key in self._cache:
            entry = self._cache[key]
            if datetime.now() < entry['expires_at']:
                logger.debug(f"üì¶ Cache hit: {key}")
                return entry['data']
            else:
                del self._cache[key]
                logger.debug(f"üßπ Cache expired: {key}")
        return None
    
    def set(self, key: str, data: Any, ttl_hours: Optional[int] = None):
        """Stocke un √©l√©ment dans le cache."""
        ttl = ttl_hours if ttl_hours is not None else self.default_ttl
        self._cache[key] = {
            'data': data,
            'expires_at': datetime.now() + timedelta(hours=ttl),
            'created_at': datetime.now()
        }
        logger.debug(f"üíæ Cache stored: {key} (TTL: {ttl}h)")
    
    def clear_old_entries(self):
        """Nettoie les entr√©es expir√©es."""
        now = datetime.now()
        expired_keys = [
            k for k, v in self._cache.items() 
            if now >= v['expires_at']
        ]
        for k in expired_keys:
            del self._cache[k]
        if expired_keys:
            logger.info(f"üßπ Cache cleanup: {len(expired_keys)} entr√©es expir√©es")

# Instance globale
ai_cache_fixed = FixedIntelligentCache(default_ttl_hours=6)

def cached_response_fixed(ttl_hours: int = 6, ignore_args: list = None):
    """
    D√©corateur corrig√© pour mettre en cache les r√©ponses IA.
    ignore_args: liste des noms d'arguments √† ignorer (ex: ['current_user'])
    """
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # Cr√©er une copie des kwargs pour la g√©n√©ration de cl√©
            cache_kwargs = kwargs.copy()
            
            # Ignorer les arguments sp√©cifi√©s
            if ignore_args:
                for arg_name in ignore_args:
                    cache_kwargs.pop(arg_name, None)
            
            # G√©n√©rer une cl√© unique (sans les arguments ignor√©s)
            cache_key = f"{func.__name__}:{ai_cache_fixed._generate_key(*args, **cache_kwargs)}"
            
            # V√©rifier le cache
            cached = ai_cache_fixed.get(cache_key)
            if cached is not None:
                return cached
            
            # Ex√©cuter la fonction
            result = await func(*args, **kwargs)
            
            # Mettre en cache
            if result is not None:
                ai_cache_fixed.set(cache_key, result, ttl_hours)
            
            return result
        return wrapper
    return decorator
-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/core/database.py
================================================================================
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
import os
from dotenv import load_dotenv

load_dotenv()

# 1. On r√©cup√®re l'URL
SQLALCHEMY_DATABASE_URL = os.getenv("DATABASE_URL")

# 2. S√©curit√© : si pas d'URL, on met du SQLite temporaire
if not SQLALCHEMY_DATABASE_URL:
    print("‚ö†Ô∏è DATABASE_URL absente. Mode SQLite temporaire.")
    SQLALCHEMY_DATABASE_URL = "sqlite:///./sql_app.db"

# 3. Correctif pour l'URL (postgres -> postgresql)
if SQLALCHEMY_DATABASE_URL.startswith("postgres://"):
    SQLALCHEMY_DATABASE_URL = SQLALCHEMY_DATABASE_URL.replace("postgres://", "postgresql://", 1)

# 4. Cr√©ation du moteur (Version Simplifi√©e pour √©viter les Timeouts)
connect_args = {}
if "sqlite" in SQLALCHEMY_DATABASE_URL:
    connect_args = {"check_same_thread": False}

engine = create_engine(
    SQLALCHEMY_DATABASE_URL,
    connect_args=connect_args
    # On a retir√© pool_pre_ping pour tester la connexion brute
)

SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()

def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/core/security.py
================================================================================
from datetime import datetime, timedelta
from typing import Optional
from jose import jwt
from passlib.context import CryptContext
import os
from dotenv import load_dotenv

load_dotenv()

# Configuration
SECRET_KEY = os.getenv("SECRET_KEY", "fallback_secret_key_if_env_missing")
ALGORITHM = os.getenv("ALGORITHM", "HS256")
ACCESS_TOKEN_EXPIRE_MINUTES = int(os.getenv("ACCESS_TOKEN_EXPIRE_MINUTES", 30))

pwd_context = CryptContext(
    schemes=["bcrypt"], 
    deprecated="auto",
    bcrypt__ident="2b"
)

def get_password_hash(password: str) -> str:
    """Transforme un mot de passe en clair en hash s√©curis√©."""
    return pwd_context.hash(password)

def verify_password(plain_password: str, hashed_password: str) -> bool:
    """V√©rifie si le mot de passe correspond au hash."""
    return pwd_context.verify(plain_password, hashed_password)

def create_access_token(data: dict, expires_delta: Optional[timedelta] = None):
    """G√©n√®re un Token JWT sign√©."""
    to_encode = data.copy()
    
    if expires_delta:
        expire = datetime.utcnow() + expires_delta
    else:
        expire = datetime.utcnow() + timedelta(minutes=15)
        
    to_encode.update({"exp": expire})
    
    encoded_jwt = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)
    return encoded_jwt-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/dependencies.py
================================================================================
from fastapi import Depends, HTTPException, status
from fastapi.security import OAuth2PasswordBearer
from jose import JWTError, jwt
from sqlalchemy.orm import Session
from app.core.database import get_db
from app.core import security
from app.models import sql_models, schemas

# C'est ici qu'on dit √† FastAPI o√π aller chercher le token si on ne l'a pas
oauth2_scheme = OAuth2PasswordBearer(tokenUrl="auth/token")

async def get_current_user(token: str = Depends(oauth2_scheme), db: Session = Depends(get_db)):
    """
    Cette fonction est un 'Dependency'. 
    Elle sera appel√©e avant chaque route prot√©g√©e.
    1. Elle r√©cup√®re le token.
    2. Elle le d√©code.
    3. Elle v√©rifie si l'utilisateur existe en BDD.
    """
    credentials_exception = HTTPException(
        status_code=status.HTTP_401_UNAUTHORIZED,
        detail="Impossible de valider les identifiants",
        headers={"WWW-Authenticate": "Bearer"},
    )
    
    try:
        # D√©codage du token
        payload = jwt.decode(token, security.SECRET_KEY, algorithms=[security.ALGORITHM])
        username: str = payload.get("sub")
        if username is None:
            raise credentials_exception
    except JWTError:
        raise credentials_exception
        
    # Recherche de l'utilisateur en BDD
    user = db.query(sql_models.User).filter(sql_models.User.username == username).first()
    if user is None:
        raise credentials_exception
        
    return user-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/domain/__init__.py
================================================================================
-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/domain/bioenergetics.py
================================================================================
import math
from typing import Dict, Any, List

class BioenergeticService:
    """
    Service de calcul physiologique (Bio-Twin v1).
    Estime la d√©pense √©nerg√©tique et les besoins nutritionnels post-effort.
    Ne d√©pend PAS de l'IA, mais de formules m√©taboliques.
    """

    @staticmethod
    def calculate_needs(profile_data: Dict[str, Any], workout_sets: List[Any], duration_min: float, rpe: float) -> Dict[str, Any]:
        """
        Calcule les KPIs physiologiques de la s√©ance.
        """
        # 1. Extraction du profil (valeurs par d√©faut de s√©curit√©)
        weight = float(profile_data.get('weight', 70.0))
        if weight <= 0: weight = 70.0
        
        gender = profile_data.get('gender', 'Homme')
        
        # 2. Estimation de la D√©pense √ânerg√©tique (Kcal)
        # M√©thode A : Pr√©cise si Watts disponibles
        total_work_kj = 0.0
        has_power_data = False
        
        for s in workout_sets:
            if s.metric_type == 'POWER_TIME':
                # Watts * secondes / 1000 = kJ
                # weight = watts, reps = duration(s) dans ce mode (selon schemas.py)
                # Mais attention, le frontend envoie parfois des minutes converties.
                # Dans sql_models/schemas, on a standardis√© : weight=Watts, reps=Secondes (via validateur polymorphique)
                watts = s.weight
                seconds = s.reps 
                total_work_kj += (watts * seconds) / 1000.0
                has_power_data = True
        
        kcal_burn = 0.0
        
        if has_power_data:
            # Rendement m√©canique humain ~20-25% => x4 √† x5 pour passer de kJ m√©canique √† kcal m√©tabolique
            # Formule standard: kJ * 1.1 est une approx basse, kJ / 4.18 * 4 (rendement) est mieux.
            # Simplification robuste : kJ m√©canique * 1.0 = Kcal m√©tabolique (approx tr√®s courante en cyclisme)
            kcal_burn = total_work_kj * 1.0 
            # Si on ajoute le m√©tabolisme de base pendant la dur√©e... Restons sur l'activit√© pure.
        else:
            # M√©thode B : Estimation METs (Metabolic Equivalent of Task)
            # RPE 1-3 (Repos/Recup) : 3 METs
            # RPE 4-6 (Endurance) : 6 METs
            # RPE 7-8 (Seuil) : 9 METs
            # RPE 9-10 (Max) : 12 METs
            mets = 3.0
            if rpe > 8: mets = 11.0
            elif rpe > 6: mets = 9.0
            elif rpe > 4: mets = 6.0
            else: mets = 3.5
            
            # Formule : Kcal = METs * Poids(kg) * Dur√©e(h)
            duration_hours = duration_min / 60.0
            kcal_burn = mets * weight * duration_hours

        # 3. Partition Macro-nutritionnelle (Fili√®res √©nerg√©tiques)
        # Ratio Glucides/Lipides d√©pend de l'intensit√© relative
        # RPE √©lev√© -> Glycolytique -> Besoin Glucides
        carbs_ratio = 0.5 # 50% par d√©faut
        
        if rpe >= 8: carbs_ratio = 0.8  # 80% glucides
        elif rpe >= 6: carbs_ratio = 0.6 # 60% glucides
        elif rpe <= 4: carbs_ratio = 0.3 # 30% glucides (LIPOX max)
        
        kcal_carbs = kcal_burn * carbs_ratio
        carbs_g = kcal_carbs / 4.0 # 4 kcal/g
        
        # 4. Prot√©ines (R√©paration tissulaire)
        # Base : 0.3g / kg de poids de corps apr√®s une s√©ance standard
        # Boost si s√©ance de force (RPE > 7)
        protein_factor = 0.25
        if rpe > 7: protein_factor = 0.35
        
        protein_g = weight * protein_factor
        
        # 5. Hydratation (Estimation sudation standard)
        # ~10ml / min / kg est trop. 
        # Standard : 0.5L √† 1L par heure selon intensit√©.
        sweat_rate_ml_h = 500
        if rpe > 7: sweat_rate_ml_h = 800
        water_ml = (duration_min / 60.0) * sweat_rate_ml_h

        return {
            "kcal_total": int(kcal_burn),
            "carbs_g": int(carbs_g),
            "protein_g": int(protein_g),
            "water_ml": int(water_ml),
            "source": "Wattmeter" if has_power_data else "METs Estimator"
        }-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/domain/calculations.py
================================================================================
import math
from abc import ABC, abstractmethod

# --- STRATEGY PATTERN : MOTEUR 1RM ---

class OneRepMaxStrategy(ABC):
    """Interface abstraite pour les strat√©gies de calcul du 1RM."""
    @abstractmethod
    def calculate(self, weight: float, reps: int) -> float:
        pass

    @property
    @abstractmethod
    def name(self) -> str:
        pass

class EpleyStrategy(OneRepMaxStrategy):
    """Formule d'Epley : W * (1 + r/30)"""
    def calculate(self, weight: float, reps: int) -> float:
        return weight * (1 + reps / 30.0)

    @property
    def name(self) -> str:
        return "Epley"

class BrzyckiStrategy(OneRepMaxStrategy):
    """Formule de Brzycki : W * (36 / (37 - r))"""
    def calculate(self, weight: float, reps: int) -> float:
        if reps >= 37: return 0.0
        return weight * (36.0 / (37.0 - reps))

    @property
    def name(self) -> str:
        return "Brzycki"

class WathanStrategy(OneRepMaxStrategy):
    """Formule de Wathan : Exponentielle"""
    def calculate(self, weight: float, reps: int) -> float:
        denominator = 48.8 + (53.8 * math.exp(-0.075 * reps))
        if denominator == 0: return 0.0
        return (100.0 * weight) / denominator

    @property
    def name(self) -> str:
        return "Wathan"

class OneRepMaxCalculator:
    """Factory : S√©lectionne la bonne strat√©gie selon le nombre de r√©p√©titions."""
    @staticmethod
    def get_strategy(reps: int) -> OneRepMaxStrategy:
        if reps <= 5:
            return EpleyStrategy()
        elif reps <= 10:
            return BrzyckiStrategy()
        else:
            return WathanStrategy()

def calculate_1rm(weight: float, reps: int) -> dict:
    """
    Fonction principale expos√©e au reste de l'app.
    """
    if weight <= 0 or reps <= 0:
        return {"1rm": 0.0, "method": "N/A"}
    
    if reps == 1:
        return {"1rm": weight, "method": "Actual Lift"}
        
    if reps > 30:
        return {"1rm": 0.0, "method": "Out of Range (>30)"}

    strategy = OneRepMaxCalculator.get_strategy(reps)
    one_rm_val = strategy.calculate(weight, reps)
    
    # Arrondi au 0.5kg le plus proche
    final_val = round(one_rm_val * 2) / 2

    return {
        "1rm": final_val,
        "method": strategy.name
    }-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/domain/safety.py
================================================================================
import pandas as pd
from datetime import date, timedelta
import re

def _safe_float(val):
    """Helper pour convertir n'importe quoi en float."""
    if val is None: return 0.0
    try:
        clean = str(val).replace(',', '.').strip()
        match = re.search(r"[-+]?\d*\.\d+|\d+", clean)
        return float(match.group()) if match else 0.0
    except:
        return 0.0

def calculate_acwr(history_logs: list) -> dict:
    """
    Calcule le Ratio Aigu/Chronique (ACWR).
    Entr√©e : Liste de dictionnaires (date, duration, rpe).
    Sortie : Dict avec ratio, status, charges.
    """
    default_res = {
        "ratio": 0.0,
        "status": "Inactif",
        "color": "gray",
        "acute_load": 0,
        "chronic_load": 0,
        "message": "Pas assez de donn√©es."
    }
    
    if not history_logs:
        return default_res

    try:
        # 1. Cr√©ation DataFrame
        df = pd.DataFrame(history_logs)
        
        # Conversion et tri des dates
        df['date_dt'] = pd.to_datetime(df['date'], errors='coerce').dt.floor('D')
        df = df.dropna(subset=['date_dt']).sort_values('date_dt')
        
        if df.empty:
            return default_res

        # 2. Calcul de la charge (Load = Dur√©e * RPE)
        # On s√©curise les valeurs
        df['duration'] = df['duration'].apply(_safe_float)
        df['rpe'] = df['rpe'].apply(_safe_float)
        df['load'] = df['duration'] * df['rpe']
        
        # Agr√©gation par jour (si plusieurs s√©ances le m√™me jour)
        daily_loads = df.groupby('date_dt')['load'].sum()
        
        # 3. Timeline Continue (J-27 √† Aujourd'hui)
        end_date = pd.Timestamp.now().floor('D')
        idx_ref = pd.date_range(end=end_date, periods=28, freq='D')
        
        # On remplit les trous avec 0
        timeline = daily_loads.reindex(idx_ref, fill_value=0)
        
        # 4. Calculs Fen√™tres Glissantes
        acute_avg = timeline.tail(7).mean()   # Fatigue (7j)
        chronic_avg = timeline.mean()         # Forme (28j)
        
        # 5. Ratio
        ratio = 0.0
        if chronic_avg > 0:
            ratio = acute_avg / chronic_avg
        elif acute_avg > 0:
            ratio = 2.0 # Reprise brutale
            
        # 6. Diagnostic
        ratio = round(ratio, 2)
        status, color, msg = "Inactif", "gray", "Reprends progressivement."
        
        if ratio > 0:
            if ratio <= 0.80:
                status, color, msg = "Sous-entra√Ænement", "blue", "Charge faible."
            elif 0.80 < ratio <= 1.30:
                status, color, msg = "Optimal", "green", "Zone de progression."
            elif 1.30 < ratio <= 1.50:
                status, color, msg = "Surcharge", "orange", "Attention fatigue."
            else:
                status, color, msg = "DANGER", "red", "Pic de charge critique (>1.5)."

        return {
            "ratio": ratio,
            "status": status,
            "color": color,
            "acute_load": int(acute_avg),
            "chronic_load": int(chronic_avg),
            "message": msg
        }
        
    except Exception as e:
        print(f"Erreur ACWR: {e}")
        return default_res-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/jobs/daily_coach_memory_update.py
================================================================================
"""
Job quotidien pour mettre √† jour les m√©moires du coach
Ex√©cut√© automatiquement √† 02:00 chaque jour
"""
import logging
import asyncio
from datetime import datetime, timedelta
from sqlalchemy.orm import Session
from sqlalchemy import and_

from app.core.database import SessionLocal
from app.models import sql_models
from app.services.coach_memory.service import CoachMemoryService

logger = logging.getLogger(__name__)

async def daily_coach_memory_update():
    """
    Met √† jour toutes les m√©moires du coach quotidiennement
    """
    logger.info("üöÄ D√©marrage du job quotidien de mise √† jour des m√©moires du coach")
    
    db = SessionLocal()
    try:
        # R√©cup√©rer toutes les m√©moires actives
        coach_memories = db.query(sql_models.CoachMemory).all()
        
        logger.info(f"üìä {len(coach_memories)} m√©moires √† mettre √† jour")
        
        updated_count = 0
        error_count = 0
        
        for memory in coach_memories:
            try:
                # R√©cup√©rer le profil associ√©
                athlete_profile = db.query(sql_models.AthleteProfile).filter(
                    sql_models.AthleteProfile.id == memory.athlete_profile_id
                ).first()
                
                if not athlete_profile:
                    logger.warning(f"Profil non trouv√© pour la m√©moire {memory.id}")
                    continue
                
                # Mettre √† jour le contexte avec des valeurs par d√©faut
                default_checkin = {
                    "sleep_quality": 7,
                    "sleep_duration": 7.5,
                    "perceived_stress": 5,
                    "muscle_soreness": 3,
                    "energy_level": 7
                }
                
                # Mettre √† jour le contexte
                CoachMemoryService.update_daily_context(memory, default_checkin, db)
                
                # Mettre √† jour les m√©tadonn√©es
                metadata = json.loads(memory.metadata) if memory.metadata else {}
                metadata['last_daily_update'] = datetime.utcnow().isoformat()
                metadata['total_updates'] = metadata.get('total_updates', 0) + 1
                memory.metadata = json.dumps(metadata)
                
                updated_count += 1
                
                # Log tous les 10 profils
                if updated_count % 10 == 0:
                    logger.info(f"‚úÖ {updated_count} m√©moires mises √† jour")
                
            except Exception as e:
                error_count += 1
                logger.error(f"‚ùå Erreur mise √† jour m√©moire {memory.id}: {str(e)}")
                continue
        
        db.commit()
        
        logger.info(f"üéâ Job termin√©: {updated_count} mises √† jour, {error_count} erreurs")
        
        # G√©n√©rer un rapport
        report = {
            "timestamp": datetime.now().isoformat(),
            "total_memories": len(coach_memories),
            "updated": updated_count,
            "errors": error_count,
            "success_rate": (updated_count / len(coach_memories) * 100) if coach_memories else 100
        }
        
        logger.info(f"üìà Rapport: {report}")
        
        return report
        
    except Exception as e:
        logger.error(f"üí• Erreur critique dans le job quotidien: {str(e)}")
        db.rollback()
        raise
    finally:
        db.close()

async def update_memory_flags_batch():
    """
    Met √† jour les flags de m√©moire en batch
    """
    logger.info("üöÄ Mise √† jour batch des flags de m√©moire")
    
    db = SessionLocal()
    try:
        # R√©cup√©rer les m√©moires avec contexte r√©cent
        one_day_ago = datetime.utcnow() - timedelta(days=1)
        
        coach_memories = db.query(sql_models.CoachMemory).filter(
            sql_models.CoachMemory.last_updated >= one_day_ago
        ).all()
        
        for memory in coach_memories:
            try:
                context = json.loads(memory.current_context) if memory.current_context else {}
                readiness = context.get('readiness_score', 70)
                
                memory_flags = json.loads(memory.memory_flags) if memory.memory_flags else {}
                
                # Mettre √† jour les flags bas√©s sur le contexte
                memory_flags['needs_deload'] = readiness < 40
                memory_flags['adaptation_window_open'] = readiness > 70
                memory_flags['pr_potential'] = readiness > 80 and context.get('fatigue_state') == 'fresh'
                
                memory.memory_flags = json.dumps(memory_flags)
                
            except Exception as e:
                logger.error(f"‚ùå Erreur mise √† jour flags m√©moire {memory.id}: {str(e)}")
                continue
        
        db.commit()
        logger.info(f"‚úÖ Flags mis √† jour pour {len(coach_memories)} m√©moires")
        
    except Exception as e:
        logger.error(f"üí• Erreur batch flags: {str(e)}")
        db.rollback()
    finally:
        db.close()

async def cleanup_old_data():
    """
    Nettoie les donn√©es anciennes
    """
    logger.info("üßπ Nettoyage des donn√©es anciennes")
    
    db = SessionLocal()
    try:
        # Supprimer les profils incomplets de plus de 30 jours
        thirty_days_ago = datetime.utcnow() - timedelta(days=30)
        
        incomplete_profiles = db.query(sql_models.AthleteProfile).filter(
            and_(
                sql_models.AthleteProfile.is_complete == False,
                sql_models.AthleteProfile.created_at < thirty_days_ago
            )
        ).all()
        
        deleted_count = 0
        for profile in incomplete_profiles:
            try:
                db.delete(profile)
                deleted_count += 1
            except Exception as e:
                logger.error(f"‚ùå Erreur suppression profil {profile.id}: {str(e)}")
        
        db.commit()
        logger.info(f"üóëÔ∏è  {deleted_count} profils incomplets supprim√©s")
        
    except Exception as e:
        logger.error(f"üí• Erreur nettoyage: {str(e)}")
        db.rollback()
    finally:
        db.close()

if __name__ == "__main__":
    # Pour ex√©cution manuelle
    import asyncio
    import json
    
    async def main():
        logger.info("üß™ Ex√©cution manuelle du job quotidien")
        
        # Ex√©cuter les t√¢ches
        report = await daily_coach_memory_update()
        await update_memory_flags_batch()
        await cleanup_old_data()
        
        print(json.dumps(report, indent=2))
    
    asyncio.run(main())
-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/main.py
================================================================================
from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
import logging
from sqlalchemy import text, inspect, create_engine
from datetime import datetime

from app.core.database import engine, Base
# Import des mod√®les
from app.models import sql_models 

# --- IMPORTS DES ROUTEURS ---
from .routers import (
    performance, 
    safety, 
    auth, 
    workouts, 
    coach, 
    user, 
    feed,
    athlete_profiles,
    coach_memories  # ‚úÖ NOUVEL IMPORT CRITIQUE (DEV-CARD #01)
)

# Configuration des logs
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --- DATABASE INIT ---
try:
    Base.metadata.create_all(bind=engine)
    logger.info("‚úÖ Tables SQL v√©rifi√©es.")
except Exception as e:
    logger.error(f"ERREUR INIT DB : {e}")

app = FastAPI(
    title="TitanFlow API",
    description="API Backend pour l'application TitanFlow",
    version="2.5.0", # Bump version pour marquer le changement
    docs_url="/docs",
    redoc_url="/redoc"
)

# --- CONFIGURATION CORS ---
origins = ["*"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"], 
    allow_headers=["*"], 
)

# --- GLOBAL EXCEPTION HANDLER ---
@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    logger.error(f"üî• CRASH : {str(exc)}")
    return JSONResponse(
        status_code=500,
        content={"detail": f"Erreur serveur : {str(exc)}"},
    )

# --- ROUTEURS ACTIFS ---

# 1. Auth
app.include_router(auth.router)

# 2. PROFILES & MEMORY
app.include_router(
    athlete_profiles.router, 
    prefix="/api/v1/profiles", 
    tags=["Profiles"]
)

# ‚úÖ AJOUT DU ROUTEUR M√âMOIRE (DEV-CARD #02)
# Les pr√©fixes sont d√©j√† d√©finis dans le routeur lui-m√™me (/api/v1/coach-memories)
app.include_router(coach_memories.router)

# 3. Autres features
app.include_router(workouts.router)
app.include_router(performance.router)
app.include_router(safety.router)
app.include_router(coach.router)
app.include_router(feed.router)

# --- ROUTES SYST√àME ---

@app.get("/health", tags=["System"])
async def health_check():
    return {
        "status": "active",
        "version": "2.5.0",
        "database": "connected"
    }

@app.get("/db_status", tags=["System"])
async def database_status():
    """Diagnostic DB"""
    try:
        inspector = inspect(engine)
        tables = inspector.get_table_names()
        
        columns_user = []
        if 'users' in tables:
            columns_user = [c['name'] for c in inspector.get_columns('users')]
            
        return {
            "status": "success",
            "tables": tables,
            "json_profile_ready": 'profile_data' in columns_user,
            "engrams_ready": 'coach_engrams' in tables, # ‚úÖ Check Engrams
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        return {"status": "error", "message": str(e)}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("app.main:app", host="0.0.0.0", port=8000, reload=True)-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/models/__init__.py
================================================================================
-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/models/domain.py
================================================================================
from pydantic import BaseModel, Field, field_validator, model_validator, ConfigDict
from typing import List, Optional, Dict, Any
from app.models.enums import CoachingMandate, SlotStatus, LocationContext, EnergyLevel

# --- SOUS-STRUCTURES ---

class ExternalLoad(BaseModel):
    """
    Charge externe impos√©e (ex: Entra√Ænement Club, Match).
    """
    type: str = Field(..., description="Type d'activit√© (ex: Rugby, Match, Crossfit Class)")
    estimated_rpe: int = Field(..., ge=1, le=10, description="Intensit√© per√ßue (1-10)")
    duration_min: int = Field(..., gt=0, description="Dur√©e en minutes")

    @field_validator('estimated_rpe')
    def validate_rpe(cls, v):
        if not 1 <= v <= 10:
            raise ValueError("Le RPE doit √™tre compris entre 1 et 10")
        return v

class TimeSlot(BaseModel):
    """
    Repr√©sente un cr√©neau unique dans la matrice hebdomadaire.
    """
    day_of_week: str = Field(..., pattern="^(Lundi|Mardi|Mercredi|Jeudi|Vendredi|Samedi|Dimanche)$")
    time_of_day: str = Field(..., pattern="^(Matin|Midi|Soir)$")
    status: SlotStatus = Field(default=SlotStatus.AVAILABLE)
    location: Optional[LocationContext] = None
    energy_level: EnergyLevel = Field(default=EnergyLevel.MEDIUM)
    
    # Charge externe (optionnel, seulement si SlotStatus.EXTERNAL_LOCKED)
    external_load: Optional[ExternalLoad] = None
    
    # [NOUVEAU] Tags pour le moteur de contraintes (ex: "RESTRICTED_LEG_VOLUME", "NO_DEADLIFT")
    tags: List[str] = Field(default_factory=list)

    model_config = ConfigDict(from_attributes=True)

    @model_validator(mode='after')
    def validate_slot_coherence(self):
        """V√©rifie la coh√©rence entre le statut et la charge."""
        if self.status == SlotStatus.EXTERNAL_LOCKED and not self.external_load:
            raise ValueError("Un cr√©neau EXTERNAL_LOCKED doit avoir une 'external_load' d√©finie.")
        return self

# --- PROFIL ATHL√âTIQUE V2 (DOMAINE) ---

class AthleteProfileDomain(BaseModel):
    """
    Repr√©sentation 'Domaine' du profil athl√®te.
    C'est la version stricte utilis√©e par le moteur IA, ind√©pendante de la BDD.
    """
    # Identit√© Sportive
    primary_sport: str
    mandate: CoachingMandate = Field(default=CoachingMandate.SUPPORT_HYBRID)
    
    # Matrice Temporelle (Disponibilit√©s)
    time_matrix: List[TimeSlot] = []

    model_config = ConfigDict(from_attributes=True)

    @model_validator(mode='after')
    def validate_sport_locations(self):
        """
        R√®gle M√©tier : Interdire LocationContext.POOL si le sport principal 
        n'est pas li√© √† la natation (Triathlon, Natation, etc.).
        """
        water_sports = ["Natation", "Triathlon", "Swimrun", "Water-polo"]
        is_water_sport = self.primary_sport in water_sports

        for slot in self.time_matrix:
            if slot.location in [LocationContext.POOL_25M, LocationContext.POOL_50M]:
                if not is_water_sport:
                    raise ValueError(
                        f"Impossible d'assigner une piscine ({slot.location}) "
                        f"pour le sport '{self.primary_sport}'. R√©serv√© aux sports aquatiques."
                    )
        return self-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/models/enums.py
================================================================================
from enum import Enum

class MemoryType(str, Enum):
    INJURY_REPORT = "INJURY_REPORT"          # ‚úÖ CORRIG√â
    LIFE_CONSTRAINT = "LIFE_CONSTRAINT"      # ‚úÖ CORRIG√â
    STRATEGIC_OVERRIDE = "STRATEGIC_OVERRIDE" # ‚úÖ CORRIG√â
    BIOFEEDBACK_LOG = "BIOFEEDBACK_LOG"      # ‚úÖ CORRIG√â (Remplace PREFERENCE/OTHER)

class ImpactLevel(str, Enum):
    SEVERE = "SEVERE"      # ‚úÖ CORRIG√â (Remplace HIGH)
    MODERATE = "MODERATE"  # ‚úÖ CORRIG√â (Remplace MEDIUM)
    INFO = "INFO"          # ‚úÖ CORRIG√â (Remplace LOW)

class MemoryStatus(str, Enum):
    ACTIVE = "ACTIVE"
    RESOLVED = "RESOLVED"
    ARCHIVED = "ARCHIVED"
    FORGOTTEN = "FORGOTTEN"

class FeedItemType(str, Enum):
    INFO = "INFO"
    ANALYSIS = "ANALYSIS"
    ACTION = "ACTION"
    ALERT = "ALERT"
    WORKOUT_LOG = "WORKOUT_LOG"
    COACH_INSIGHT = "COACH_INSIGHT"
    PERSONAL_RECORD = "PERSONAL_RECORD"
    SYSTEM_ALERT = "SYSTEM_ALERT"
    DAILY_TIP = "DAILY_TIP"

class SportType(str, Enum):
    RUGBY = "Rugby"
    FOOTBALL = "Football"
    CROSSFIT = "CrossFit"
    HYBRID = "Hybrid"
    RUNNING = "Running"
    OTHER = "Autre"
    BODYBUILDING = "BODYBUILDING"
    CYCLING = "CYCLING"
    TRIATHLON = "TRIATHLON"
    POWERLIFTING = "POWERLIFTING"
    SWIMMING = "SWIMMING"
    COMBAT_SPORTS = "COMBAT_SPORTS"

class EquipmentType(str, Enum):
    PERFORMANCE_LAB = "PERFORMANCE_LAB"
    COMMERCIAL_GYM = "COMMERCIAL_GYM"
    HOME_GYM_BARBELL = "HOME_GYM_BARBELL"
    HOME_GYM_DUMBBELL = "HOME_GYM_DUMBBELL"
    CALISTHENICS_KIT = "CALISTHENICS_KIT"
    BODYWEIGHT_ZERO = "BODYWEIGHT_ZERO"
    ENDURANCE_SUITE = "ENDURANCE_SUITE"
    STANDARD = "Standard"
    HOME_GYM_FULL = "HOME_GYM_FULL"
    CROSSFIT_BOX = "CROSSFIT_BOX"
    DUMBBELLS = "DUMBBELLS"
    BARBELL = "BARBELL"
    KETTLEBELLS = "KETTLEBELLS"
    PULL_UP_BAR = "PULL_UP_BAR"
    BENCH = "BENCH"
    DIP_STATION = "DIP_STATION"
    BANDS = "BANDS"
    RINGS_TRX = "RINGS_TRX"
    JUMP_ROPE = "JUMP_ROPE"
    WEIGHT_VEST = "WEIGHT_VEST"
    BIKE = "BIKE"
    HOME_TRAINER = "HOME_TRAINER"
    ROWER = "ROWER"
    TREADMILL = "TREADMILL"
    POOL = "POOL"-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/models/schemas.py
================================================================================
from pydantic import BaseModel, Field, field_validator, model_validator, ConfigDict
from typing import List, Optional, Dict, Any, Union
from datetime import date, datetime
from enum import Enum
import json
import re
import logging

# ‚úÖ IMPORT CENTRALIS√â (On r√©cup√®re tout depuis enums.py)
from app.models.enums import (
    MemoryType, ImpactLevel, MemoryStatus, 
    SportType, EquipmentType, FeedItemType
)

logger = logging.getLogger(__name__)

# --- PERFORMANCE METRICS SUB-SCHEMAS ---

class CyclingMetrics(BaseModel):
    cycling_max_power_15s: Optional[int] = Field(None, description="Puissance Max 15s (Watts)")
    cycling_max_power_3min: Optional[int] = Field(None, description="Puissance Max 3min (Watts)")
    cycling_max_power_20min: Optional[int] = Field(None, description="Puissance Max 20min (Watts)")
    cycling_ftp: Optional[int] = Field(None, description="Functional Threshold Power (Watts)")

class RunningMetrics(BaseModel):
    running_time_5k: Optional[Union[int, str]] = Field(None, description="Temps 5k (Secondes)")
    running_time_10k: Optional[Union[int, str]] = Field(None, description="Temps 10k (Secondes)")
    running_time_21k: Optional[Union[int, str]] = Field(None, description="Temps Semi (Secondes)")
    running_max_sprint_time: Optional[Union[int, str]] = Field(None, description="Sprint Max (Secondes)")

    @field_validator('running_time_5k', 'running_time_10k', 'running_time_21k', 'running_max_sprint_time', mode='before')
    def transform_time_to_seconds(cls, v):
        if v is None: return None
        if isinstance(v, int): return v
        if isinstance(v, float): return int(v)
        if isinstance(v, str):
            v = v.strip()
            if not v: return None
            parts = v.split(':')
            try:
                if len(parts) == 3:
                    return int(parts[0]) * 3600 + int(parts[1]) * 60 + int(float(parts[2]))
                elif len(parts) == 2:
                    return int(parts[0]) * 60 + int(float(parts[1]))
                elif v.isdigit():
                    return int(v)
            except ValueError:
                return v
        return v

class SwimmingMetrics(BaseModel):
    swimming_time_200m: Optional[Union[int, str]] = Field(None, description="Temps 200m (Secondes)")
    swimming_time_400m: Optional[Union[int, str]] = Field(None, description="Temps 400m (Secondes)")

    @field_validator('swimming_time_200m', 'swimming_time_400m', mode='before')
    def transform_swim_time(cls, v):
        if v is None: return None
        if isinstance(v, int): return v
        if isinstance(v, float): return int(v)
        if isinstance(v, str):
            v = v.strip()
            if not v: return None
            parts = v.split(':')
            try:
                if len(parts) == 3:
                    return int(parts[0]) * 3600 + int(parts[1]) * 60 + int(float(parts[2]))
                elif len(parts) == 2:
                    return int(parts[0]) * 60 + int(float(parts[1]))
                elif v.isdigit():
                    return int(v)
            except ValueError:
                return v
        return v

class PerformanceBaselineSchema(CyclingMetrics, RunningMetrics, SwimmingMetrics):
    # Champs suppl√©mentaires pour accepter les donn√©es brutes du mobile
    run_vma_est: Optional[str] = None
    cycling_ftp_est: Optional[str] = None
    swim_css_est: Optional[str] = None
    run_short_dist: Optional[float] = None
    run_short_min: Optional[float] = None
    run_short_sec: Optional[float] = None
    run_long_dist: Optional[float] = None
    run_long_min: Optional[float] = None
    run_long_sec: Optional[float] = None
    bike_short_min: Optional[float] = None
    bike_short_sec: Optional[float] = None
    bike_short_watts: Optional[float] = None
    bike_long_min: Optional[float] = None
    bike_long_sec: Optional[float] = None
    bike_long_watts: Optional[float] = None
    swim_200_min: Optional[float] = None
    swim_200_sec: Optional[float] = None
    swim_400_min: Optional[float] = None
    swim_400_sec: Optional[float] = None
    run_vma: Optional[float] = None
    bike_peak_5s: Optional[float] = None
    run_sprint_max: Optional[float] = None
    squat_1rm: Optional[float] = None
    bench_1rm: Optional[float] = None
    deadlift_1rm: Optional[float] = None
    pull_load: Optional[float] = None
    
    model_config = ConfigDict(extra='allow')

    @field_validator('*', mode='before')
    def clean_none_values(cls, v, info):
        """Nettoie les valeurs None et cha√Ænes vides."""
        if info.field_name not in ['run_vma_est', 'cycling_ftp_est', 'swim_css_est']:
            if v in [None, "", "null", "undefined"]:
                return None
        return v

# --- SUB-SCHEMAS FOR PROFILE ---

class BasicInfo(BaseModel):
    pseudo: Optional[str] = None
    email: Optional[str] = None
    birth_date: Optional[str] = None
    training_age: Optional[int] = 0
    biological_sex: Optional[str] = "MALE"

class PhysicalMetrics(BaseModel):
    height: float = 0
    weight: float = 0
    body_fat: Optional[float] = None
    resting_hr: Optional[int] = None
    sleep_quality_avg: Optional[int] = 5

class SportContext(BaseModel):
    sport: SportType = SportType.OTHER
    position: Optional[str] = None
    level: Optional[str] = "INTERMEDIATE"
    equipment: List[EquipmentType] = [EquipmentType.BODYWEIGHT_ZERO]

    @field_validator('equipment', mode='before')
    def migrate_legacy_equipment(cls, v):
        """Transforme les anciennes valeurs 'Standard' en 'COMMERCIAL_GYM'."""
        if not v:
            return [EquipmentType.BODYWEIGHT_ZERO]
        
        cleaned_list = []
        if isinstance(v, list):
            for item in v:
                if item == "Standard":
                    cleaned_list.append(EquipmentType.COMMERCIAL_GYM)
                else:
                    cleaned_list.append(item)
        return cleaned_list

class TrainingPreferences(BaseModel):
    days_available: List[str] = []
    duration_min: int = 60
    preferred_split: str = "Upper/Lower"

# --- MAIN PROFILE SCHEMAS ---

class AthleteProfileBase(BaseModel):
    basic_info: BasicInfo = Field(default_factory=BasicInfo)
    physical_metrics: PhysicalMetrics = Field(default_factory=PhysicalMetrics)
    sport_context: SportContext = Field(default_factory=SportContext)
    training_preferences: TrainingPreferences = Field(default_factory=TrainingPreferences)
    goals: Dict[str, Any] = {}
    constraints: Dict[str, Any] = {}
    injury_prevention: Dict[str, Any] = {}
    
    performance_baseline: Dict[str, Any] = Field(default_factory=dict)

    @field_validator('performance_baseline', mode='before')
    def parse_performance(cls, v):
        """Nettoie et valide les donn√©es de performance."""
        if v is None:
            return {}
        try:
            if isinstance(v, dict):
                cleaned = {}
                for key, value in v.items():
                    if value in [None, "", "null", "undefined"]:
                        continue
                    if key in ['run_vma_est', 'cycling_ftp_est', 'swim_css_est'] and value == "":
                        continue
                    cleaned[key] = value
                return cleaned
            return {}
        except Exception as e:
            logger.error(f"Erreur validation performance_baseline: {e}")
            return {}

class AthleteProfileCreate(AthleteProfileBase):
    pass

class AthleteProfileResponse(AthleteProfileBase):
    id: int
    user_id: int
    created_at: Optional[datetime] = None
    updated_at: Optional[datetime] = None
    
    class Config:
        from_attributes = True

# --- ENGRAM SCHEMAS ---

class CoachEngramBase(BaseModel):
    type: MemoryType
    impact: ImpactLevel = ImpactLevel.INFO
    status: MemoryStatus = MemoryStatus.ACTIVE
    content: str
    tags: List[str] = []
    end_date: Optional[datetime] = None

class CoachEngramCreate(CoachEngramBase):
    pass

class CoachEngramUpdate(BaseModel):
    """
    Schema pour la mise √† jour (PUT) d'un souvenir.
    Tous les champs sont optionnels.
    """
    content: Optional[str] = None
    type: Optional[MemoryType] = None
    impact: Optional[ImpactLevel] = None
    status: Optional[MemoryStatus] = None
    tags: Optional[List[str]] = None
    end_date: Optional[datetime] = None

class CoachEngramResponse(CoachEngramBase):
    id: int
    memory_id: int
    author: str
    created_at: Optional[datetime] = None
    
    class Config:
        from_attributes = True

# --- MEMORY SCHEMAS ---

class CoachMemoryResponse(BaseModel):
    id: int
    readiness_score: int = Field(alias="current_context", default=50)
    current_phase: str = "G√©n√©ral"
    flags: Dict[str, bool] = {}
    insights: Dict[str, Any] = {}
    
    # Liste des souvenirs (Engrammes)
    engrams: List[CoachEngramResponse] = []

    @field_validator('readiness_score', mode='before')
    def extract_readiness(cls, v):
        if isinstance(v, dict):
            return v.get('readiness_score', 50)
        return v

    class Config:
        from_attributes = True

class CoachMemoryOut(BaseModel):
    id: int
    athlete_profile_id: int
    current_context: Optional[Dict[str, Any]] = None
    memory_flags: Optional[Dict[str, Any]] = None
    
    class Config:
        from_attributes = True

class CoachMemoryCreate(BaseModel):
    type: str
    impact: str = "INFO"
    status: str = "ACTIVE"
    content: str
    tags: List[str] = []
    start_date: Optional[datetime] = None
    end_date: Optional[datetime] = None
    user_id: Optional[int] = None

# --- WORKOUT SCHEMAS ---

class WorkoutSetBase(BaseModel):
    exercise_name: str
    set_order: int
    weight: Union[float, str] = 0.0
    reps: Union[float, str] = 0.0
    rpe: Optional[float] = 0.0
    rest_seconds: int = 0
    metric_type: str = "LOAD_REPS"

    @field_validator('weight', 'reps', mode='before')
    def parse_polymorphic_fields(cls, v):
        if isinstance(v, str):
            v = v.strip().replace(',', '.')
            if ':' in v:
                parts = v.split(':')
                try:
                    seconds = 0.0
                    if len(parts) == 2:
                        seconds = float(parts[0]) * 60 + float(parts[1])
                    elif len(parts) == 3:
                        seconds = float(parts[0]) * 3600 + float(parts[1]) * 60 + float(parts[2])
                    return seconds
                except ValueError:
                    return 0.0
            try:
                return float(v)
            except ValueError:
                return 0.0
        return v

class WorkoutSetCreate(WorkoutSetBase):
    pass

class WorkoutSessionCreate(BaseModel):
    date: date
    duration: float
    rpe: float
    energy_level: int = 5
    notes: Optional[str] = None
    ai_analysis: Optional[str] = None
    sets: List[WorkoutSetCreate] = []

class WorkoutSetResponse(WorkoutSetBase):
    id: int
    weight: float
    reps: float
    class Config:
        from_attributes = True

class WorkoutSessionResponse(WorkoutSessionCreate):
    id: int
    ai_analysis: Optional[str] = None
    sets: List[WorkoutSetResponse] = []
    class Config:
        from_attributes = True

# --- AI & GENERATION ---

class GenerateWorkoutRequest(BaseModel):
    profile_data: Dict[str, Any]
    context: Dict[str, Any]

class AIExercise(BaseModel):
    name: str
    sets: int
    reps: Union[str, int]
    rest: int
    tips: str
    recording_mode: str = "LOAD_REPS"
    @field_validator('reps')
    def force_string_reps(cls, v):
        return str(v)

class AIWorkoutPlan(BaseModel):
    title: str
    coach_comment: str
    warmup: List[str]
    exercises: List[AIExercise]
    cooldown: List[str]

# --- USER & AUTH ---

class UserCreate(BaseModel):
    username: str
    email: Optional[str] = None
    password: str

class UserResponse(BaseModel):
    id: int
    username: str
    email: Optional[str] = None
    profile_data: Optional[Dict[str, Any]] = None 
    
    @field_validator('profile_data', mode='before')
    def parse_profile_data(cls, v):
        if v is None:
            return {}
        if isinstance(v, dict):
            return v
        if isinstance(v, str):
            if not v.strip():
                return {}
            try:
                return json.loads(v)
            except json.JSONDecodeError:
                return {}
        return v

    class Config:
        from_attributes = True

class Token(BaseModel):
    access_token: str
    token_type: str

class TokenData(BaseModel):
    username: Optional[str] = None

# --- FEED ---

class FeedItemCreate(BaseModel):
    type: FeedItemType
    title: str
    message: str
    priority: int = 1
    action_payload: Optional[Dict[str, Any]] = None

class FeedItemResponse(FeedItemCreate):
    id: str
    is_read: bool
    is_completed: bool
    created_at: datetime
    
    @field_validator('action_payload', mode='before')
    def parse_payload(cls, v):
        if isinstance(v, str) and v.strip():
            try: return json.loads(v)
            except: return None
        return v
    class Config:
        from_attributes = True

# --- PERFORMANCE & MISC ---

class OneRepMaxRequest(BaseModel):
    weight: float
    reps: int
class OneRepMaxResponse(BaseModel):
    estimated_1rm: float
    method_used: str
class ACWRRequest(BaseModel):
    history: List[Dict[str, Any]]
class ACWRResponse(BaseModel):
    ratio: float
    status: str
    color: str
    message: str
class ProfileAuditRequest(BaseModel):
    profile_data: Dict[str, Any]
class ProfileAuditResponse(BaseModel):
    markdown_report: str
class StrategyResponse(BaseModel):
    periodization_title: str
    phases: List[Any]
class WeeklyPlanResponse(BaseModel):
    schedule: List[Any]
    reasoning: str

class ProfileUpdate(BaseModel):
    profile_data: Dict[str, Any]
    
class ProfileSectionUpdate(BaseModel):
    section_data: Dict[str, Any]

class DailyMetrics(BaseModel):
    date: str
    weight: Optional[float] = None
    sleep_quality: Optional[int] = None
    resting_heart_rate: Optional[int] = None
    hrv: Optional[int] = None
    energy_level: Optional[int] = None
    muscle_soreness: Optional[int] = None
    perceived_stress: Optional[int] = None
    sleep_duration: Optional[float] = None

class GoalProgressUpdate(BaseModel):
    progress_value: int
    progress_note: Optional[str] = None
    achieved: bool = False

class AthleteProfileUpdate(AthleteProfileBase):
    pass-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/models/sql_models.py
================================================================================
from sqlalchemy import Column, Integer, String, Float, Date, ForeignKey, DateTime, Text, Boolean, JSON, Enum as SQLEnum
from sqlalchemy.orm import relationship
from sqlalchemy.sql import func
from app.core.database import Base
from app.models.enums import MemoryType, ImpactLevel, MemoryStatus

class User(Base):
    __tablename__ = "users"

    id = Column(Integer, primary_key=True, index=True)
    username = Column(String, unique=True, index=True)
    email = Column(String, unique=True, index=True, nullable=True)
    hashed_password = Column(String)
    
    # Legacy fields
    profile_data = Column(JSON, default={}) 
    strategy_data = Column(Text, nullable=True)
    weekly_plan_data = Column(Text, nullable=True)
    draft_workout_data = Column(Text, nullable=True)

    workouts = relationship("WorkoutSession", back_populates="owner")
    feed_items = relationship("FeedItem", back_populates="owner", cascade="all, delete-orphan")
    athlete_profile = relationship("AthleteProfile", back_populates="user", uselist=False, cascade="all, delete-orphan")

class AthleteProfile(Base):
    __tablename__ = "athlete_profiles"

    id = Column(Integer, primary_key=True, index=True)
    user_id = Column(Integer, ForeignKey("users.id"), unique=True)

    basic_info = Column(JSON, default={})
    physical_metrics = Column(JSON, default={})
    sport_context = Column(JSON, default={})
    performance_baseline = Column(JSON, default={})
    injury_prevention = Column(JSON, default={})
    training_preferences = Column(JSON, default={})
    goals = Column(JSON, default={})
    constraints = Column(JSON, default={})

    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), onupdate=func.now())

    user = relationship("User", back_populates="athlete_profile")
    coach_memory = relationship("CoachMemory", back_populates="athlete_profile", uselist=False, cascade="all, delete-orphan")

    @property
    def completion_percentage(self):
        sections = [
            self.basic_info, self.physical_metrics, self.sport_context,
            self.performance_baseline, self.injury_prevention,
            self.training_preferences, self.goals, self.constraints
        ]
        filled = sum(1 for section in sections if section and section != {})
        total = len(sections)
        return int((filled / total) * 100) if total > 0 else 0

    @property
    def is_complete(self):
        return self.completion_percentage >= 80

class CoachMemory(Base):
    __tablename__ = "coach_memories"

    id = Column(Integer, primary_key=True, index=True)
    athlete_profile_id = Column(Integer, ForeignKey("athlete_profiles.id"), unique=True)

    metadata_info = Column(JSON, default={}) 
    current_context = Column(JSON, default={})
    response_patterns = Column(JSON, default={})
    performance_baselines = Column(JSON, default={})
    adaptation_signals = Column(JSON, default={})
    sport_specific_insights = Column(JSON, default={})
    training_history_summary = Column(JSON, default={})
    athlete_preferences = Column(JSON, default={})
    coach_notes = Column(JSON, default={})
    memory_flags = Column(JSON, default={})

    last_updated = Column(DateTime(timezone=True), server_default=func.now())

    athlete_profile = relationship("AthleteProfile", back_populates="coach_memory")
    
    # ‚úÖ R√âINT√âGRATION : Relation vers les Engrammes
    engrams = relationship("CoachEngram", back_populates="memory", cascade="all, delete-orphan")

class CoachEngram(Base):
    """
    Unit√© de m√©moire structur√©e (Souvenir/R√®gle) pour le Coach IA.
    """
    __tablename__ = "coach_engrams"

    id = Column(Integer, primary_key=True, index=True)
    memory_id = Column(Integer, ForeignKey("coach_memories.id"), nullable=False)
    
    author = Column(String, default="COACH_AI")
    type = Column(SQLEnum(MemoryType), nullable=False)
    impact = Column(SQLEnum(ImpactLevel), nullable=False, default=ImpactLevel.INFO)
    status = Column(SQLEnum(MemoryStatus), nullable=False, default=MemoryStatus.ACTIVE)
    
    content = Column(Text, nullable=False)
    
    start_date = Column(DateTime(timezone=True), server_default=func.now())
    end_date = Column(DateTime(timezone=True), nullable=True)
    
    tags = Column(JSON, default=[])

    # Relation parente
    memory = relationship("CoachMemory", back_populates="engrams")

class WorkoutSession(Base):
    __tablename__ = "workout_sessions"
    id = Column(Integer, primary_key=True, index=True)
    user_id = Column(Integer, ForeignKey("users.id"))
    date = Column(Date, index=True)
    duration = Column(Float)
    rpe = Column(Float)
    energy_level = Column(Integer, default=5) 
    notes = Column(Text, nullable=True)       
    ai_analysis = Column(Text, nullable=True)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    owner = relationship("User", back_populates="workouts")
    sets = relationship("WorkoutSet", back_populates="session", cascade="all, delete-orphan")

class WorkoutSet(Base):
    __tablename__ = "workout_sets"
    id = Column(Integer, primary_key=True, index=True)
    session_id = Column(Integer, ForeignKey("workout_sessions.id"))
    exercise_name = Column(String, index=True)
    set_order = Column(Integer)
    weight = Column(Float, default=0.0)
    reps = Column(Float, default=0.0)
    rpe = Column(Float, default=0.0)
    rest_seconds = Column(Integer, default=0)
    metric_type = Column(String, nullable=False, default="LOAD_REPS") 
    session = relationship("WorkoutSession", back_populates="sets")

class FeedItem(Base):
    __tablename__ = "feed_items"
    id = Column(String, primary_key=True, index=True)
    user_id = Column(Integer, ForeignKey("users.id"))
    type = Column(String, index=True)
    title = Column(String)
    message = Column(String)
    action_payload = Column(Text, nullable=True)
    is_read = Column(Boolean, default=False)
    is_completed = Column(Boolean, default=False)
    priority = Column(Integer, default=1)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    owner = relationship("User", back_populates="feed_items")-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/routers/__init__.py
================================================================================
-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/routers/athlete_profiles.py
================================================================================
"""
Routeur unifi√© pour la gestion des profils athl√®tes
G√®re toutes les routes /api/v1/profiles/*
"""
import json
import logging
import re
from typing import List, Dict, Any, Optional
from fastapi import APIRouter, Depends, HTTPException, status
from sqlalchemy.orm import Session
from sqlalchemy.exc import IntegrityError
from sqlalchemy.sql import func

from app.core.database import get_db
from app.dependencies import get_current_user
from app.models import sql_models, schemas
from app.services.coach_memory.service import initialize_coach_memory
from app.validators.athlete_profile_validators import validate_athlete_profile

# Configuration du Logger pour le debugging
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

router = APIRouter(
    tags=["Profiles"]  # Tags unifi√©s
)

def transform_mobile_performance_data(raw_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    Transforme les donn√©es brutes du mobile en format API compatible.
    """
    if not raw_data:
        return {}
    
    # Nettoyer d'abord les valeurs vides, nulles ou invalides
    cleaned_data = {}
    for key, value in raw_data.items():
        # Filtrer les valeurs vraiment vides
        if value is None:
            continue
        if isinstance(value, str) and value.strip() == "":
            continue
        if value == 0 or value == "0" or value == 0.0:
            continue
        if value == "null" or value == "undefined":
            continue
        
        cleaned_data[key] = value
    
    transformed = {}
    
    # 1. Extraire les valeurs num√©riques des r√©sultats format√©s
    if cleaned_data.get('run_vma_est'):
        try:
            match = re.search(r'(\d+\.?\d*)', str(cleaned_data['run_vma_est']))
            if match:
                vma_value = float(match.group(1))
                transformed['run_vma'] = vma_value
                # Optionnel: calculer le temps 5k √©quivalent
                if vma_value > 0:
                    transformed['running_time_5k'] = int(5000 / (vma_value * 1000/3600))
        except Exception as e:
            logger.debug(f"Erreur extraction run_vma_est: {e}")
    
    if cleaned_data.get('cycling_ftp_est'):
        try:
            match = re.search(r'(\d+\.?\d*)', str(cleaned_data['cycling_ftp_est']))
            if match:
                transformed['cycling_ftp'] = int(float(match.group(1)))
        except Exception as e:
            logger.debug(f"Erreur extraction cycling_ftp_est: {e}")
    
    if cleaned_data.get('swim_css_est'):
        try:
            match = re.search(r'(\d+):(\d+)', str(cleaned_data['swim_css_est']))
            if match:
                minutes = int(match.group(1))
                seconds = int(match.group(2))
                transformed['swimming_time_200m'] = minutes * 60 + seconds
        except Exception as e:
            logger.debug(f"Erreur extraction swim_css_est: {e}")
    
    # 2. Calculer les valeurs d√©riv√©es √† partir des inputs bruts
    # Course √† pied - Calcul VMA/CS
    try:
        required_fields = ['run_short_dist', 'run_short_min', 'run_short_sec', 
                          'run_long_dist', 'run_long_min', 'run_long_sec']
        
        # V√©rifier que tous les champs requis sont pr√©sents et valides
        fields_present = all(k in cleaned_data for k in required_fields)
        fields_valid = all(cleaned_data.get(k) not in [None, "", 0, 0.0] for k in required_fields)
        
        if fields_present and fields_valid:
            d1 = float(cleaned_data['run_short_dist'])
            t1 = float(cleaned_data['run_short_min']) * 60 + float(cleaned_data['run_short_sec'])
            d2 = float(cleaned_data['run_long_dist'])
            t2 = float(cleaned_data['run_long_min']) * 60 + float(cleaned_data['run_long_sec'])
            
            if t2 > t1 and d2 > d1:
                cs_mps = (d2 - d1) / (t2 - t1)
                vma_kmh = cs_mps * 3.6
                if 'run_vma' not in transformed:
                    transformed['run_vma'] = round(vma_kmh, 1)
                # Convertir en temps 5k pour compatibilit√© API
                if vma_kmh > 0 and 'running_time_5k' not in transformed:
                    transformed['running_time_5k'] = int(5000 / (vma_kmh * 1000/3600))
    except Exception as e:
        logger.debug(f"Calcul running non effectu√©: {e}")
    
    # V√©lo - Calcul FTP/CP
    try:
        required_fields = ['bike_short_min', 'bike_short_sec', 'bike_short_watts',
                          'bike_long_min', 'bike_long_sec', 'bike_long_watts']
        
        fields_present = all(k in cleaned_data for k in required_fields)
        fields_valid = all(cleaned_data.get(k) not in [None, "", 0, 0.0] for k in required_fields)
        
        if fields_present and fields_valid:
            t1 = float(cleaned_data['bike_short_min']) * 60 + float(cleaned_data['bike_short_sec'])
            p1 = float(cleaned_data['bike_short_watts'])
            t2 = float(cleaned_data['bike_long_min']) * 60 + float(cleaned_data['bike_long_sec'])
            p2 = float(cleaned_data['bike_long_watts'])
            
            if t2 != t1:
                w1 = p1 * t1
                w2 = p2 * t2
                cp = (w2 - w1) / (t2 - t1)
                if 'cycling_ftp' not in transformed:
                    transformed['cycling_ftp'] = int(cp)
    except Exception as e:
        logger.debug(f"Calcul cycling non effectu√©: {e}")
    
    # 3. Copier les autres champs num√©riques directement
    numeric_fields = ['run_sprint_max', 'bike_peak_5s', 'squat_1rm', 'bench_1rm', 
                     'deadlift_1rm', 'pull_load', 'run_vma', 'cycling_ftp']
    
    for field in numeric_fields:
        if field in cleaned_data:
            try:
                transformed[field] = float(cleaned_data[field])
            except (ValueError, TypeError):
                pass
    
    # 4. Pour compatibilit√© avec le sch√©ma Pydantic
    # Convertir les champs sp√©cifiques vers les noms d'API attendus
    if 'run_vma' in transformed:
        transformed['running_vma'] = transformed.pop('run_vma')
    
    # 5. Garder les donn√©es brutes nettoy√©es pour r√©f√©rence
    if cleaned_data:
        transformed['raw_mobile_data'] = cleaned_data
    
    logger.info(f"üìä Donn√©es performance transform√©es: {transformed}")
    return transformed

# --- ROUTE CRITIQUE POUR LE MOBILE ---

@router.get("/me", response_model=schemas.AthleteProfileResponse)
async def get_my_profile(
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    R√©cup√®re le profil de l'utilisateur connect√©.
    Si aucun profil n'existe, cr√©e un profil vide automatiquement.
    Route appel√©e par le mobile: GET /api/v1/profiles/me
    """
    profile = db.query(sql_models.AthleteProfile).filter(
        sql_models.AthleteProfile.user_id == current_user.id
    ).first()
    
    if not profile:
        logger.info(f"üìù Aucun profil trouv√© pour user {current_user.id}, cr√©ation d'un profil vide")
        
        # Cr√©er un profil vide
        profile = sql_models.AthleteProfile(
            user_id=current_user.id,
            basic_info={"pseudo": current_user.username},
            physical_metrics={},
            sport_context={},
            performance_baseline={},
            injury_prevention={},
            training_preferences={},
            goals={},
            constraints={}
        )
        
        db.add(profile)
        db.commit()
        db.refresh(profile)
        
        logger.info(f"‚úÖ Profil vide cr√©√© pour user {current_user.id}")
    
    return profile

@router.put("/me", response_model=schemas.AthleteProfileResponse)
async def update_my_profile(
    profile_update: schemas.AthleteProfileUpdate,
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    Met √† jour le profil de l'utilisateur connect√©.
    Si le profil n'existe pas, le cr√©e automatiquement.
    Route appel√©e par le mobile: PUT /api/v1/profiles/me
    """
    logger.info(f"‚ö° UPDATE /me demand√© pour user : {current_user.id}")
    
    profile = db.query(sql_models.AthleteProfile).filter(
        sql_models.AthleteProfile.user_id == current_user.id
    ).first()
    
    if not profile:
        logger.info(f"üìù Cr√©ation de profil via PUT /me pour user {current_user.id}")
        
        # Valider les donn√©es du profil
        try:
            validate_athlete_profile(profile_update.model_dump(exclude_unset=True))
        except ValueError as e:
            logger.error(f"Erreur de validation : {str(e)}")
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=str(e)
            )
        
        # Cr√©er le profil
        profile = sql_models.AthleteProfile(
            user_id=current_user.id,
            basic_info=profile_update.basic_info or {},
            physical_metrics=profile_update.physical_metrics or {},
            sport_context=profile_update.sport_context or {},
            performance_baseline=profile_update.performance_baseline or {},
            injury_prevention=profile_update.injury_prevention or {},
            training_preferences=profile_update.training_preferences or {},
            goals=profile_update.goals or {},
            constraints=profile_update.constraints or {}
        )
        
        db.add(profile)
        db.commit()
        db.refresh(profile)
        
        logger.info(f"‚úÖ Profil cr√©√© via PUT /me pour user {current_user.id}")
        return profile
    
    # Si profil existe, mise √† jour
    # Conversion Pydantic -> Dict en excluant les valeurs None
    update_dict = profile_update.model_dump(exclude_unset=True)
    
    # Traiter les donn√©es de performance sp√©cialement
    if 'performance_baseline' in update_dict:
        perf_data = update_dict['performance_baseline']
        if perf_data:
            logger.info(f"üìä Donn√©es performance brutes re√ßues: {perf_data}")
            transformed_perf = transform_mobile_performance_data(perf_data)
            logger.info(f"üîÑ Donn√©es performance transform√©es: {transformed_perf}")
            update_dict['performance_baseline'] = transformed_perf
    
    # Liste des champs JSON dans le mod√®le SQL
    json_fields = [
        'basic_info', 'physical_metrics', 'sport_context',
        'performance_baseline', 'injury_prevention', 
        'training_preferences', 'goals', 'constraints'
    ]
    
    try:
        updated_sections = []
        
        for section, data in update_dict.items():
            if section in json_fields and data is not None:
                setattr(profile, section, data)
                updated_sections.append(section)
            elif hasattr(profile, section) and data is not None:
                setattr(profile, section, data)
        
        # Mettre √† jour le timestamp
        profile.updated_at = func.now()
        
        db.commit()
        db.refresh(profile)
        
        logger.info(f"‚úÖ Profil /me mis √† jour. Sections: {updated_sections}")
        return profile
        
    except Exception as e:
        db.rollback()
        logger.error(f"‚ùå Erreur update /me: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Erreur de mise √† jour: {str(e)}"
        )

@router.post("/complete", response_model=schemas.AthleteProfileResponse, status_code=status.HTTP_201_CREATED)
async def create_complete_profile(
    profile_data: Dict[str, Any],
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    Cr√©e un profil athl√®te complet via le wizard
    Route alternative pour cr√©ation via wizard
    """
    logger.info(f"Cr√©ation de profil wizard pour l'utilisateur : {current_user.id}")
    
    # V√©rifier si l'utilisateur a d√©j√† un profil
    existing_profile = db.query(sql_models.AthleteProfile).filter(
        sql_models.AthleteProfile.user_id == current_user.id
    ).first()
    
    if existing_profile:
        logger.warning(f"Profil d√©j√† existant pour user {current_user.id}")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Un profil existe d√©j√† pour cet utilisateur. Utilisez PUT /me pour mettre √† jour."
        )
    
    # Valider les donn√©es du profil
    try:
        validate_athlete_profile(profile_data)
    except ValueError as e:
        logger.error(f"Erreur de validation : {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )
    
    # Traiter les donn√©es de performance
    if 'performance_baseline' in profile_data:
        perf_data = profile_data['performance_baseline']
        if perf_data:
            transformed_perf = transform_mobile_performance_data(perf_data)
            profile_data['performance_baseline'] = transformed_perf
    
    # Cr√©er le profil
    athlete_profile = sql_models.AthleteProfile(
        user_id=current_user.id,
        basic_info=profile_data.get('basic_info', {}),
        physical_metrics=profile_data.get('physical_metrics', {}),
        sport_context=profile_data.get('sport_context', {}),
        performance_baseline=profile_data.get('performance_baseline', {}),
        injury_prevention=profile_data.get('injury_prevention', {}),
        training_preferences=profile_data.get('training_preferences', {}),
        goals=profile_data.get('goals', {}),
        constraints=profile_data.get('constraints', {})
    )
    
    try:
        db.add(athlete_profile)
        db.commit()
        db.refresh(athlete_profile)
        
        # Initialiser la m√©moire du coach
        initialize_coach_memory(athlete_profile, db)
        
        logger.info(f"Profil wizard cr√©√© avec succ√®s pour user {current_user.id}")
        return athlete_profile
        
    except IntegrityError as e:
        db.rollback()
        logger.error(f"Erreur d'int√©grit√© DB : {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Erreur d'int√©grit√© des donn√©es"
        )

# --- AUTRES ROUTES (optionnelles, pour compatibilit√©) ---

@router.get("/{profile_id}", response_model=schemas.AthleteProfileResponse)
async def get_profile(
    profile_id: int,
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    R√©cup√®re un profil athl√®te par ID
    """
    profile = db.query(sql_models.AthleteProfile).filter(
        sql_models.AthleteProfile.id == profile_id,
        sql_models.AthleteProfile.user_id == current_user.id
    ).first()
    
    if not profile:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Profil non trouv√©"
        )
    
    return profile

@router.put("/{profile_id}", response_model=schemas.AthleteProfileResponse)
async def update_profile(
    profile_id: int,
    profile_update: schemas.AthleteProfileUpdate,
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    Met √† jour compl√®tement un profil par ID
    """
    profile = db.query(sql_models.AthleteProfile).filter(
        sql_models.AthleteProfile.id == profile_id,
        sql_models.AthleteProfile.user_id == current_user.id
    ).first()
    
    if not profile:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Profil non trouv√©"
        )
    
    # Conversion Pydantic -> Dict
    update_dict = profile_update.model_dump(exclude_unset=True)
    
    # Traiter les donn√©es de performance
    if 'performance_baseline' in update_dict:
        perf_data = update_dict['performance_baseline']
        if perf_data:
            transformed_perf = transform_mobile_performance_data(perf_data)
            update_dict['performance_baseline'] = transformed_perf
    
    # Mettre √† jour chaque section
    for section, data in update_dict.items():
        if data is not None and hasattr(profile, section):
            setattr(profile, section, data)
    
    # Mettre √† jour le timestamp
    profile.updated_at = func.now()
    
    db.commit()
    db.refresh(profile)
    
    return profile

@router.patch("/{profile_id}/section/{section_name}")
async def update_profile_section(
    profile_id: int,
    section_name: str,
    section_update: schemas.ProfileSectionUpdate,
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    Met √† jour une section sp√©cifique du profil
    """
    profile = db.query(sql_models.AthleteProfile).filter(
        sql_models.AthleteProfile.id == profile_id,
        sql_models.AthleteProfile.user_id == current_user.id
    ).first()
    
    if not profile:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Profil non trouv√©"
        )
    
    # V√©rifier que la section existe
    valid_sections = [
        'basic_info', 'physical_metrics', 'sport_context',
        'performance_baseline', 'injury_prevention',
        'training_preferences', 'goals', 'constraints'
    ]
    
    if section_name not in valid_sections:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Section invalide. Options: {', '.join(valid_sections)}"
        )
    
    # Traiter les donn√©es de performance sp√©cialement
    if section_name == 'performance_baseline':
        perf_data = section_update.section_data
        if perf_data:
            transformed_perf = transform_mobile_performance_data(perf_data)
            setattr(profile, section_name, transformed_perf)
        else:
            setattr(profile, section_name, {})
    else:
        setattr(profile, section_name, section_update.section_data)
    
    # Mettre √† jour le timestamp
    profile.updated_at = func.now()
    
    db.commit()
    
    return {
        "message": "Section mise √† jour avec succ√®s"
    }

@router.get("/{profile_id}/completion")
async def get_profile_completion(
    profile_id: int,
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    R√©cup√®re le statut de compl√©tion du profil
    """
    profile = db.query(sql_models.AthleteProfile).filter(
        sql_models.AthleteProfile.id == profile_id,
        sql_models.AthleteProfile.user_id == current_user.id
    ).first()
    
    if not profile:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Profil non trouv√©"
        )
    
    # Calculer les sections manquantes
    sections = {
        'basic_info': profile.basic_info,
        'physical_metrics': profile.physical_metrics,
        'sport_context': profile.sport_context,
        'performance_baseline': profile.performance_baseline,
        'injury_prevention': profile.injury_prevention,
        'training_preferences': profile.training_preferences,
        'goals': profile.goals,
        'constraints': profile.constraints
    }
    
    missing_sections = []
    for name, value in sections.items():
        if not value or value == {}:
            missing_sections.append(name)
    
    total_sections = 8
    completed_sections = total_sections - len(missing_sections)
    completion_percentage = int((completed_sections / total_sections) * 100)
    
    return {
        "completion_percentage": completion_percentage,
        "is_complete": completion_percentage >= 80,
        "missing_sections": missing_sections,
        "total_sections": total_sections,
        "completed_sections": completed_sections
    }-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/routers/auth.py
================================================================================
from datetime import datetime, timedelta
from fastapi import APIRouter, Depends, HTTPException, status
from fastapi.security import OAuth2PasswordRequestForm
from sqlalchemy.orm import Session
from app.core.database import get_db
from app.models import sql_models, schemas
from app.core import security

router = APIRouter(
    prefix="/auth",
    tags=["Authentication"]
)

@router.post("/signup", response_model=schemas.UserResponse, status_code=status.HTTP_201_CREATED)
async def create_user(user: schemas.UserCreate, db: Session = Depends(get_db)):
    """
    Inscription d'un nouvel utilisateur.
    Initialise imm√©diatement le 'sac de sport' (profile_data) pour √©viter les erreurs 500.
    """
    # 1. V√©rifier si le pseudo existe d√©j√†
    db_user = db.query(sql_models.User).filter(sql_models.User.username == user.username).first()
    if db_user:
        raise HTTPException(status_code=400, detail="Ce pseudo est d√©j√† pris.")
    
    # 2. V√©rifier si l'email existe d√©j√† (si fourni)
    if user.email:
        db_email = db.query(sql_models.User).filter(sql_models.User.email == user.email).first()
        if db_email:
            raise HTTPException(status_code=400, detail="Cet email est d√©j√† utilis√©.")
    
    # 3. Hasher le mot de passe
    hashed_pwd = security.get_password_hash(user.password)
    
    # 4. Pr√©parer le Casier (Profile Data JSON)
    # On initialise une structure propre pour que le reste de l'app ne plante pas sur du NULL.
    initial_profile_data = {
        "basic_info": {
            "pseudo": user.username,
            "email": user.email,
            "created_at": datetime.utcnow().isoformat()
        },
        "onboarding_completed": False,
        "physical_metrics": {},
        "goals": {},
        "stats": {
            "level": 1,
            "xp": 0
        }
    }
    
    # 5. Cr√©er l'utilisateur
    new_user = sql_models.User(
        username=user.username,
        email=user.email,
        hashed_password=hashed_pwd,
        profile_data=initial_profile_data # <--- C'est ici que la magie op√®re
    )
    
    try:
        db.add(new_user)
        db.commit()
        db.refresh(new_user)
        return new_user
    except Exception as e:
        db.rollback()
        # On log l'erreur pour le debug serveur, mais on renvoie une erreur propre au client
        print(f"üî• ERREUR CRITIQUE SIGNUP DB : {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, 
            detail="Erreur lors de la cr√©ation du compte. V√©rifiez les donn√©es."
        )

@router.post("/token", response_model=schemas.Token)
async def login_for_access_token(form_data: OAuth2PasswordRequestForm = Depends(), db: Session = Depends(get_db)):
    """Login : V√©rifie pseudo/mot de passe et renvoie un Token JWT."""
    user = db.query(sql_models.User).filter(sql_models.User.username == form_data.username).first()
    
    if not user or not security.verify_password(form_data.password, user.hashed_password):
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Pseudo ou mot de passe incorrect",
            headers={"WWW-Authenticate": "Bearer"},
        )
    
    access_token_expires = timedelta(minutes=security.ACCESS_TOKEN_EXPIRE_MINUTES)
    access_token = security.create_access_token(
        data={"sub": user.username}, expires_delta=access_token_expires
    )
    
    return {"access_token": access_token, "token_type": "bearer"}-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/routers/coach.py
================================================================================
import os
import json
import re
import google.generativeai as genai
from fastapi import APIRouter, HTTPException, Depends
from sqlalchemy.orm import Session
from app.core.database import get_db
from app.dependencies import get_current_user
from app.models import sql_models, schemas
from app.models.schemas import (
    ProfileAuditRequest, ProfileAuditResponse, 
    StrategyResponse, WeeklyPlanResponse,
    GenerateWorkoutRequest, AIWorkoutPlan
)
from dotenv import load_dotenv
from datetime import date

load_dotenv()

router = APIRouter(
    prefix="/coach",
    tags=["AI Coach"]
)

# Configuration unique de l'IA
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# --- UTILITAIRES ---

def clean_ai_json(text: str) -> str:
    """
    Nettoie la r√©ponse de l'IA pour extraire uniquement le bloc JSON valide.
    G√®re les cas o√π l'IA ajoute des balises markdown ```json ... ```.
    """
    try:
        # On cherche le contenu entre ```json et ``` ou juste ``` et ```
        pattern = r"```(?:json)?\s*([\s\S]*?)\s*```"
        match = re.search(pattern, text)
        if match:
            return match.group(1).strip()
        return text.strip()
    except Exception:
        return text

# --- PROMPTS ---

def get_profile_analysis_prompt(profile_data):
    """G√©n√®re le prompt pour l'audit du profil."""
    profile_str = json.dumps(profile_data, ensure_ascii=False, indent=2)
    return f"""
    R√îLE : Tu es le Lead Sport Scientist d'une f√©d√©ration olympique (TitanFlow).
    TACHE : Auditer le profil d'un athl√®te et D√âFINIR LA LIGNE DIRECTRICE.
    
    DONN√âES BRUTES ATHL√àTE (JSON) :
    {profile_str}

    CONSIGNES D'ANALYSE :
    1. V√©rifie la coh√©rence "Niveau vs Performances".
    2. V√©rifie la coh√©rence "Objectif vs Logistique (Dispo)".
    3. Identifie les risques de blessures ou les incoh√©rences majeures.
    
    FORMAT DE SORTIE :
    R√©ponds UNIQUEMENT en Markdown bien format√©.
    Utilise des emojis. Sois direct, bienveillant mais exigeant.
    """

def get_periodization_prompt(profile_data):
    """G√©n√®re le prompt pour la strat√©gie de p√©riodisation (JSON)."""
    today_str = date.today().strftime("%Y-%m-%d")
    profile_str = json.dumps(profile_data, ensure_ascii=False, indent=2)
    cycle_goal = profile_data.get('goal', 'Performance G√©n√©rale')
    target_date_str = profile_data.get('target_date', '2025-12-31')

    return f"""
    R√îLE : Directeur de Performance Sportive (Haut Niveau).
    CONTEXTE : Cr√©er une P√âRIODISATION MACRO (Les Grandes Phases) pour un athl√®te.

    1. DONN√âES ATHL√àTE :
    {profile_str}

    2. PARAM√àTRES DU CYCLE :
    - Objectif : {cycle_goal}
    - Date actuelle : {today_str}
    - Deadline : {target_date_str}

    CONSIGNES DE P√âRIODISATION :
    - Divise la p√©riode en BLOCS (PHASES) de 3 √† 8 semaines.
    - G√©n√®re entre 3 et 6 phases majeures.

    STRUCTURE DE SORTIE (JSON STRICT) :
    {{
        "periodization_title": "Nom scientifique",
        "periodization_logic": "Justification courte.",
        "progression_model": "Ex: RPE Progression.",
        "recommended_frequency": 4, 
        "phases": [
            {{
                "phase_name": "Phase 1 : [Nom]",
                "focus": "Objectif physiologique",
                "intensity_metric": "RPE 7-8", 
                "volume_strategy": "Ex: Volume √âlev√©",
                "start": "YYYY-MM-DD",
                "end": "YYYY-MM-DD"
            }}
        ]
    }}
    """

def get_weekly_planning_prompt(profile_data):
    """G√©n√®re le prompt complexe pour la semaine type."""
    
    user_sport = profile_data.get('sport', 'Musculation')
    avail = profile_data.get('availability', [])
    
    slots_context = []
    for slot in avail:
        if slot.get('isActive', False): # Adaptation au format Flutter (isActive vs Active)
             slots_context.append({
                "Jour": slot.get('day'),
                "Moment": slot.get('moment'),
                "Dispo_Max": f"{slot.get('duration')} min",
                "Type_Cible": slot.get('type')
            })
    
    avail_json = json.dumps(slots_context, ensure_ascii=False, indent=2)

    return f"""
    R√îLE : Entra√Æneur Expert en {user_sport}.
    MISSION : G√©n√©rer la SEMAINE TYPE (Lundi-Dimanche) pour cet athl√®te.

    CONTEXTE ATHL√àTE :
    - Sport : {user_sport}
    - Niveau : {profile_data.get('level')}
    - Objectif : {profile_data.get('goal')}

    === CONTRAINTES STRICTES (MATRICE DE DISPONIBILIT√â) ===
    Tu DOIS respecter ces cr√©neaux √† la lettre. Si un jour n'est pas list√© ci-dessous, c'est REPOS.
    {avail_json}

    R√àGLES D'ALLOCATION :
    1. Pour chaque cr√©neau disponible, assigne une s√©ance pr√©cise.
    2. Respecte le "Type_Cible" impos√© par l'utilisateur :
       - "PPS" = Sport Sp√©cifique (Terrain, Piste, Bassin).
       - "PPG" = Renforcement / Muscu.
       - "Libre" = Choisis le mieux adapt√© pour l'√©quilibre.
    3. Si pas de cr√©neau dispo un jour -> "Type": "Repos", "Focus": "R√©cup√©ration".
    4. "RPE Cible" doit √™tre un ENTIER (ex: 0 pour Repos, 7 pour une s√©ance). Ne jamais mettre null.

    FORMAT DE SORTIE (JSON OBJET) :
    {{
        "schedule": [
            {{ "Jour": "Lundi", "Cr√©neau": "Soir", "Type": "Sp√©cifique (PPS)", "Focus": "...", "RPE Cible": 7 }},
            ... (14 entr√©es pour couvrir la semaine)
        ],
        "reasoning": "Explication courte de la logique de la semaine."
    }}
    """

def get_workout_generation_prompt(profile_data, context):
    """
    G√©n√®re une s√©ance d√©taill√©e avec gestion stricte des MODES D'ENREGISTREMENT.
    """
    sport = profile_data.get('sport', 'Musculation')
    user_level = profile_data.get('level', 'Interm√©diaire')
    
    duration = context.get('duration', 60)
    energy = context.get('energy', 5)
    focus = context.get('focus', 'Full Body')
    equipment = context.get('equipment', 'Standard')

    return f"""
    R√îLE : Coach Sportif d'√âlite (SmartCoach).
    MISSION : Concevoir une s√©ance sur-mesure (JSON).

    ATHL√àTE :
    - Sport : {sport} ({user_level})
    - Blessures : {profile_data.get('injuries', 'Aucune')}
    
    CONTEXTE DU JOUR :
    - Dur√©e Max : {duration} min
    - √ânergie : {energy}/10
    - Focus demand√© : {focus}
    - Mat√©riel : {equipment}

    INSTRUCTIONS TECHNIQUES CRITIQUES :
    1. Adapte le volume (S√©ries/Reps) √† l'√©nergie du jour.
    2. Pour CHAQUE exercice, tu DOIS choisir le 'recording_mode' adapt√© √† la nature de l'effort :
       - "LOAD_REPS" : Pour la musculation classique (Halt√®res, Barres, Machines). Champs : Poids/Reps.
       - "BODYWEIGHT_REPS" : Pour le poids du corps (Pompes, Tractions). Champs : Lest/Reps.
       - "ISOMETRIC_TIME" : Pour le statique (Gainage, Chaise). Champs : Lest/Temps(s).
       - "PACE_DISTANCE" : Pour le Cardio/Running/Natation. Champs : Allure/Distance(m).
       - "POWER_TIME" : Pour le V√©lo/Ergo. Champs : Watts/Temps(s).
    
    3. Le champ 'reps' peut √™tre une string (ex: "10-12" ou "AMRAP") ou un nombre.
    4. Le champ 'rest' est en secondes.

    STRUCTURE DE SORTIE (JSON STRICT) :
    {{
        "title": "Nom de la s√©ance",
        "coach_comment": "Phrase de motivation ou conseil technique.",
        "warmup": ["Exo 1", "Exo 2"],
        "exercises": [
            {{
                "name": "Squat",
                "sets": 4,
                "reps": "8-10",
                "rest": 90,
                "tips": "Dos droit, descendre sous la parall√®le.",
                "recording_mode": "LOAD_REPS"
            }}
        ],
        "cooldown": ["Etirement 1"]
    }}
    """

# --- ROUTES ---

@router.post("/audit", response_model=ProfileAuditResponse)
async def audit_profile(
    payload: ProfileAuditRequest,
    current_user: sql_models.User = Depends(get_current_user)
):
    """Audit du profil athl√®te par l'IA."""
    if not GEMINI_API_KEY:
        raise HTTPException(status_code=500, detail="Cl√© API Gemini manquante.")
    
    try:
        genai.configure(api_key=GEMINI_API_KEY)
        model = genai.GenerativeModel('gemini-2.0-flash')
        response = model.generate_content(get_profile_analysis_prompt(payload.profile_data))
        
        # Stocker l'audit localement
        from app.core.database import get_db
        from sqlalchemy.orm import Session
        db = next(get_db())
        current_user.profile_data = json.dumps(payload.profile_data)
        db.commit()
        
        return {"markdown_report": response.text}
    except Exception as e:
        print(f"‚ùå Erreur audit: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# --- STRAT√âGIE (Lecture & √âcriture Persistante) ---

@router.get("/strategy", response_model=StrategyResponse)
async def get_strategy(
    current_user: sql_models.User = Depends(get_current_user)
):
    """R√©cup√®re la strat√©gie sauvegard√©e (si elle existe)."""
    if not current_user.strategy_data:
        raise HTTPException(status_code=404, detail="Aucune strat√©gie trouv√©e.")
    try:
        data = json.loads(current_user.strategy_data)
        return data
    except Exception as e:
        print(f"‚ùå Erreur lecture strat√©gie: {e}")
        raise HTTPException(status_code=500, detail="Erreur lecture strat√©gie.")

@router.post("/strategy", response_model=StrategyResponse)
async def generate_strategy(
    payload: ProfileAuditRequest,
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """G√©n√®re ET sauvegarde la strat√©gie."""
    if not GEMINI_API_KEY:
        raise HTTPException(status_code=500, detail="Cl√© API Gemini manquante.")
    
    try:
        genai.configure(api_key=GEMINI_API_KEY)
        model = genai.GenerativeModel('gemini-2.0-flash', generation_config={"response_mime_type": "application/json"})
        response = model.generate_content(get_periodization_prompt(payload.profile_data))
        
        # Nettoyage et Validation JSON
        clean_text = clean_ai_json(response.text)
        strategy_data = json.loads(clean_text)
        
        # Sauvegarde en BDD
        current_user.strategy_data = json.dumps(strategy_data)
        db.commit()
        db.refresh(current_user)
        
        return strategy_data
    except Exception as e:
        print(f"‚ùå Erreur Strategy Gen: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# --- PLANNING SEMAINE (Lecture & √âcriture Persistante) ---

@router.get("/week", response_model=WeeklyPlanResponse)
async def get_week(
    current_user: sql_models.User = Depends(get_current_user)
):
    """R√©cup√®re la semaine type sauvegard√©e."""
    if not current_user.weekly_plan_data:
         raise HTTPException(status_code=404, detail="Aucune semaine trouv√©e.")
    try:
        data = json.loads(current_user.weekly_plan_data)
        return data
    except Exception as e:
        print(f"‚ùå Erreur lecture semaine: {e}")
        raise HTTPException(status_code=500, detail="Erreur lecture semaine.")

@router.post("/week", response_model=WeeklyPlanResponse)
async def generate_week(
    payload: ProfileAuditRequest,
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """G√©n√®re ET sauvegarde la semaine type."""
    if not GEMINI_API_KEY:
        raise HTTPException(status_code=500, detail="Cl√© API Gemini manquante.")
    
    try:
        genai.configure(api_key=GEMINI_API_KEY)
        model = genai.GenerativeModel('gemini-2.0-flash', generation_config={"response_mime_type": "application/json"})
        
        prompt = get_weekly_planning_prompt(payload.profile_data)
        response = model.generate_content(prompt)
        
        # Nettoyage et Parsing
        clean_text = clean_ai_json(response.text)
        result = json.loads(clean_text)
        
        if "schedule" not in result and isinstance(result, list):
            result = {"schedule": result, "reasoning": "G√©n√©r√© automatiquement."}
        
        # Sauvegarde en BDD
        current_user.weekly_plan_data = json.dumps(result)
        db.commit()
        db.refresh(current_user)
            
        return result
    except Exception as e:
        print(f"‚ùå Erreur Week Gen: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# --- GESTION DES S√âANCES & BROUILLONS ---

@router.get("/workout/draft", response_model=AIWorkoutPlan)
async def get_draft_workout(
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    R√©cup√®re le brouillon de s√©ance en cours (si existant).
    Utile pour reprendre une session apr√®s un crash.
    """
    if not current_user.draft_workout_data:
        raise HTTPException(status_code=404, detail="Aucun brouillon trouv√©.")
    
    try:
        return json.loads(current_user.draft_workout_data)
    except Exception as e:
        print(f"‚ùå Erreur lecture brouillon: {e}")
        raise HTTPException(status_code=500, detail="Erreur lecture brouillon.")

@router.delete("/workout/draft")
async def discard_draft_workout(
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    Supprime explicitement le brouillon (Abandon).
    """
    try:
        current_user.draft_workout_data = None
        db.commit()
        return {"status": "success", "message": "Brouillon supprim√©."}
    except Exception as e:
        print(f"‚ùå Erreur suppression brouillon: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/workout", response_model=AIWorkoutPlan)
async def generate_workout(
    payload: GenerateWorkoutRequest,
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    G√©n√®re une s√©ance d√©taill√©e ET la sauvegarde en brouillon.
    """
    if not GEMINI_API_KEY:
        raise HTTPException(status_code=500, detail="Cl√© API Gemini manquante.")
    
    try:
        genai.configure(api_key=GEMINI_API_KEY)
        model = genai.GenerativeModel('gemini-2.0-flash', generation_config={"response_mime_type": "application/json"})
        
        prompt = get_workout_generation_prompt(payload.profile_data, payload.context)
        response = model.generate_content(prompt)
        
        # Nettoyage et Parsing
        clean_text = clean_ai_json(response.text)
        parsed_response = json.loads(clean_text)
        
        # Validation de la structure
        if isinstance(parsed_response, list):
            if parsed_response:
                parsed_response = parsed_response[0]
            else:
                raise ValueError("L'IA a renvoy√© une liste vide.")
        
        # Validation des exercices
        if "exercises" not in parsed_response:
            parsed_response["exercises"] = []
        
        # S'assurer que chaque exercice a un recording_mode
        for exercise in parsed_response["exercises"]:
            if "recording_mode" not in exercise:
                exercise["recording_mode"] = "LOAD_REPS"
        
        # Sauvegarde automatique du brouillon
        current_user.draft_workout_data = json.dumps(parsed_response)
        db.commit()
        db.refresh(current_user)

        return parsed_response
    except json.JSONDecodeError as e:
        print(f"‚ùå Erreur JSON IA: {e}")
        print(f"Texte brut re√ßu: {clean_text[:500]}...")
        raise HTTPException(
            status_code=500, 
            detail="L'IA a renvoy√© une r√©ponse invalide. Veuillez r√©essayer."
        )
    except Exception as e:
        print(f"‚ùå Erreur Workout Gen: {e}")
        raise HTTPException(
            status_code=500, 
            detail=f"Erreur lors de la g√©n√©ration: {str(e)}"
        )-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/routers/coach_memories.py
================================================================================
from typing import List, Optional
from datetime import datetime
from fastapi import APIRouter, Depends, HTTPException, status
from sqlalchemy.orm import Session, selectinload
from sqlalchemy import select, desc

# Imports Core
from app.core.database import get_db
from app.dependencies import get_current_user
from app.models import sql_models, schemas
from app.models.enums import MemoryStatus

router = APIRouter(
    prefix="/api/v1/coach-memories",
    tags=["Coach Memory v2"]
)

# ==============================================================================
# üß† GET MY MEMORY (Route Principale - AVEC AUTO-HEALING)
# ==============================================================================
@router.get("/me", response_model=schemas.CoachMemoryResponse)
async def get_my_coach_memory(
    current_user: sql_models.User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """
    R√©cup√®re la m√©moire du coach pour l'utilisateur connect√©.
    AUTO-HEALING : Si la m√©moire n'existe pas, elle est cr√©√©e imm√©diatement.
    """
    # 1. V√©rifier le profil
    if not current_user.athlete_profile:
        raise HTTPException(status_code=404, detail="Profil athl√®te introuvable. Veuillez compl√©ter votre profil.")
    
    profile_id = current_user.athlete_profile.id

    # 2. Requ√™te Explicite avec Chargement Eager (Imm√©diat) des Engrammes
    memory = db.query(sql_models.CoachMemory)\
        .options(selectinload(sql_models.CoachMemory.engrams))\
        .filter(sql_models.CoachMemory.athlete_profile_id == profile_id)\
        .first()

    # [CORRECTIF CRITIQUE] : Auto-healing
    # Si pas de m√©moire, on la cr√©e √† la vol√©e pour ne pas bloquer l'UI
    if not memory:
        memory = sql_models.CoachMemory(athlete_profile_id=profile_id)
        db.add(memory)
        db.commit()
        db.refresh(memory)
    
    # 3. HYGI√àNE DES DONN√âES : Filtrage Python
    # On garde ACTIVE et RESOLVED (historique visible), on vire ARCHIVED (poubelle).
    if memory.engrams:
        active_engrams = [
            e for e in memory.engrams 
            if e.status != MemoryStatus.ARCHIVED
        ]
        memory.engrams = active_engrams
    
    return memory

# ==============================================================================
# üì• GET ALL MEMORIES (Admin / Debug)
# ==============================================================================
@router.get("/", response_model=List[schemas.CoachMemoryOut])
async def get_memories(
    db: Session = Depends(get_db),
    limit: int = 50,
    status: Optional[str] = None
):
    query = select(sql_models.CoachMemory)
    if status:
        query = query.where(sql_models.CoachMemory.status == status)
    query = query.order_by(desc(sql_models.CoachMemory.last_updated))
    query = query.limit(limit)
    result = db.execute(query)
    return result.scalars().all()

# ==============================================================================
# ‚ûï ADD ENGRAM (LA ROUTE QUI MANQUAIT)
# ==============================================================================
@router.post("/engrams", response_model=schemas.CoachEngramResponse, status_code=status.HTTP_201_CREATED)
async def create_engram(
    engram_in: schemas.CoachEngramCreate,
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    Ajoute un nouvel engramme (souvenir/contrainte) √† la m√©moire du coach.
    """
    # 1. R√©cup√©rer la m√©moire du coach via le user connect√©
    if not current_user.athlete_profile:
        raise HTTPException(status_code=404, detail="Profil introuvable.")
    
    profile_id = current_user.athlete_profile.id
    
    memory = db.query(sql_models.CoachMemory).filter(
        sql_models.CoachMemory.athlete_profile_id == profile_id
    ).first()

    # S√©curit√© : Si pas de m√©moire, on la cr√©e (Auto-healing backend)
    if not memory:
        memory = sql_models.CoachMemory(athlete_profile_id=profile_id)
        db.add(memory)
        db.commit()
        db.refresh(memory)

    # 2. Cr√©ation de l'engramme
    new_engram = sql_models.CoachEngram(
        memory_id=memory.id,
        content=engram_in.content,
        type=engram_in.type,
        impact=engram_in.impact,
        status=engram_in.status,
        tags=engram_in.tags,
        start_date=datetime.utcnow()
    )

    db.add(new_engram)
    
    # 3. Update Meta (Pour dire √† l'IA qu'il y a du nouveau)
    memory.last_updated = datetime.utcnow()
    
    try:
        db.commit()
        db.refresh(new_engram)
        return new_engram
    except Exception as e:
        db.rollback()
        raise HTTPException(status_code=500, detail=f"Erreur DB: {str(e)}")

# ==============================================================================
# üì§ POST NEW MEMORY (Container Principal)
# ==============================================================================
@router.post("/", response_model=schemas.CoachMemoryOut, status_code=status.HTTP_201_CREATED)
async def create_memory(
    memory_in: schemas.CoachMemoryCreate,
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    Cr√©e une nouvelle instance de m√©moire Coach (Container).
    """
    if not current_user.athlete_profile:
        raise HTTPException(status_code=400, detail="Aucun profil athl√®te associ√©.")

    profile_id = current_user.athlete_profile.id

    existing_memory = db.query(sql_models.CoachMemory).filter(
        sql_models.CoachMemory.athlete_profile_id == profile_id
    ).first()

    if existing_memory:
        raise HTTPException(status_code=409, detail="Une m√©moire existe d√©j√†.")
    
    new_memory = sql_models.CoachMemory(
        athlete_profile_id=profile_id,
        metadata_info={"type": memory_in.type, "content": memory_in.content}
    )
    
    try:
        db.add(new_memory)
        db.commit()
        db.refresh(new_memory)
        return new_memory
    except Exception as e:
        db.rollback()
        raise HTTPException(status_code=500, detail=str(e))

# ==============================================================================
# üîÑ UPDATE ENGRAM (Logique Temporelle & R√©activation)
# ==============================================================================
@router.put("/engrams/{engram_id}", response_model=schemas.CoachEngramResponse)
async def update_engram(
    engram_id: int,
    engram_update: schemas.CoachEngramCreate,
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    Met √† jour un souvenir (Engramme).
    G√®re la logique temporelle : RESOLVED vs ACTIVE.
    """
    # 1. Fetch & Check de propri√©t√© via Jointure
    engram = db.query(sql_models.CoachEngram)\
        .join(sql_models.CoachMemory)\
        .join(sql_models.AthleteProfile)\
        .filter(
            sql_models.CoachEngram.id == engram_id,
            sql_models.AthleteProfile.user_id == current_user.id
        ).first()

    if not engram:
        raise HTTPException(status_code=404, detail="Engramme introuvable.")

    # 2. LOGIQUE TEMPORELLE
    if engram_update.status == MemoryStatus.RESOLVED:
        if not engram.end_date:
            engram.end_date = datetime.utcnow()
    
    elif engram_update.status == MemoryStatus.ACTIVE:
        engram.end_date = None

    # 3. Application des mises √† jour
    engram.content = engram_update.content
    engram.type = engram_update.type
    engram.impact = engram_update.impact
    engram.status = engram_update.status
    engram.tags = engram_update.tags
    
    if engram_update.end_date is not None and engram_update.status != MemoryStatus.ACTIVE:
        engram.end_date = engram_update.end_date

    engram.memory.last_updated = datetime.utcnow()

    db.commit()
    db.refresh(engram)
    
    return engram

# ==============================================================================
# üóëÔ∏è DELETE MEMORY
# ==============================================================================
@router.delete("/{memory_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_memory(
    memory_id: int,
    db: Session = Depends(get_db)
):
    query = select(sql_models.CoachMemory).where(sql_models.CoachMemory.id == memory_id)
    result = db.execute(query)
    memory = result.scalar_one_or_none()

    if not memory:
        raise HTTPException(status_code=404, detail="Souvenir introuvable")

    db.delete(memory)
    db.commit()
    return None

# ==============================================================================
# üóëÔ∏è DELETE ENGRAM (La route manquante pour corriger l'erreur 405)
# ==============================================================================
@router.delete("/engrams/{engram_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_engram(
    engram_id: int,
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    Supprime un souvenir sp√©cifique (Engramme).
    """
    # 1. V√©rification de propri√©t√© (S√©curit√©)
    # On s'assure que l'engramme appartient bien √† une m√©moire li√©e au user connect√©
    engram = db.query(sql_models.CoachEngram)\
        .join(sql_models.CoachMemory)\
        .join(sql_models.AthleteProfile)\
        .filter(
            sql_models.CoachEngram.id == engram_id,
            sql_models.AthleteProfile.user_id == current_user.id
        ).first()

    if not engram:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND, 
            detail="Engramme introuvable ou acc√®s refus√©."
        )

    # 2. Suppression physique
    db.delete(engram)
    
    # 3. Update Meta (Optionnel : dire √† la m√©moire qu'elle a chang√©)
    engram.memory.last_updated = datetime.utcnow()
    
    db.commit()
    return None-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/routers/feed.py
================================================================================
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from typing import List
from app.core.database import get_db
from app.models import sql_models, schemas
from app.dependencies import get_current_user

router = APIRouter(
    prefix="/feed",
    tags=["Neural Feed"]
)

@router.get("/", response_model=List[schemas.FeedItemResponse])
async def get_my_feed(
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    R√©cup√®re le flux d'√©v√©nements de l'utilisateur.
    Filtre : Uniquement les items NON COMPL√âT√âS.
    Tri : Priorit√© (DESC) puis Date de cr√©ation (DESC).
    """
    items = db.query(sql_models.FeedItem)\
        .filter(sql_models.FeedItem.user_id == current_user.id)\
        .filter(sql_models.FeedItem.is_completed == False)\
        .order_by(sql_models.FeedItem.priority.desc(), sql_models.FeedItem.created_at.desc())\
        .all()
    return items

@router.patch("/{item_id}/read")
async def mark_as_read(
    item_id: str,
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """Marque un item comme LU (mais le laisse dans le flux tant que pas compl√©t√©)."""
    item = db.query(sql_models.FeedItem).filter(sql_models.FeedItem.id == item_id, sql_models.FeedItem.user_id == current_user.id).first()
    if not item:
        raise HTTPException(status_code=404, detail="Item introuvable")
    
    item.is_read = True
    db.commit()
    return {"status": "success"}

@router.patch("/{item_id}/complete")
async def mark_as_completed(
    item_id: str,
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """Marque un item comme COMPL√âT√â (Dispara√Æt du flux)."""
    item = db.query(sql_models.FeedItem).filter(sql_models.FeedItem.id == item_id, sql_models.FeedItem.user_id == current_user.id).first()
    if not item:
        raise HTTPException(status_code=404, detail="Item introuvable")
    
    item.is_completed = True
    db.commit()
    return {"status": "success"}-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/routers/performance.py
================================================================================
from fastapi import APIRouter, HTTPException
from app.models.schemas import OneRepMaxRequest, OneRepMaxResponse
from app.domain import calculations

router = APIRouter(
    prefix="/performance",
    tags=["Performance & Metrics"]
)

@router.post("/1rm", response_model=OneRepMaxResponse)
async def compute_one_rep_max(payload: OneRepMaxRequest):
    """
    Calcule le 1RM (One Rep Max) estim√© bas√© sur une performance.
    S√©lectionne automatiquement la meilleure formule (Epley, Brzycki, Wathan).
    """
    try:
        result = calculations.calculate_1rm(payload.weight, payload.reps)
        
        return {
            "estimated_1rm": result["1rm"],
            "method_used": result["method"],
            "input_weight": payload.weight,
            "input_reps": payload.reps
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/routers/profiles.py
================================================================================
from fastapi import APIRouter, Depends, HTTPException, status
from sqlalchemy.orm import Session
from app.core.database import get_db
from app.models import sql_models, schemas
from app.dependencies import get_current_user
from app.services.coach_logic import CoachLogic

router = APIRouter(
    prefix="/api/v1",
    tags=["Athlete Profile & Memory"]
)

@router.get("/profiles/me", response_model=schemas.AthleteProfileResponse)
async def get_my_profile(
    current_user: sql_models.User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    if not current_user.athlete_profile:
        profile = sql_models.AthleteProfile(user_id=current_user.id)
        db.add(profile)
        db.commit()
        db.refresh(profile)
        return profile
    return current_user.athlete_profile

@router.post("/profiles/complete", response_model=schemas.AthleteProfileResponse)
async def complete_profile(
    profile_data: schemas.AthleteProfileCreate,
    current_user: sql_models.User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    sport = profile_data.sport_context.sport
    pos = profile_data.sport_context.position
    if not CoachLogic.validate_sport_position(sport, pos):
        raise HTTPException(status_code=400, detail=f"Position {pos} invalide pour le sport {sport}")

    db_profile = current_user.athlete_profile
    if not db_profile:
        db_profile = sql_models.AthleteProfile(user_id=current_user.id)
        db.add(db_profile)
    
    db_profile.basic_info = profile_data.basic_info.dict()
    db_profile.physical_metrics = profile_data.physical_metrics.dict()
    db_profile.sport_context = profile_data.sport_context.dict()
    db_profile.training_preferences = profile_data.training_preferences.dict()
    db_profile.goals = profile_data.goals
    db_profile.constraints = profile_data.constraints
    
    if not db_profile.coach_memory:
        memory = CoachLogic.initialize_memory(db_profile)
        db.add(memory)
    
    db.commit()
    db.refresh(db_profile)
    return db_profile

@router.get("/coach-memories/me", response_model=schemas.CoachMemoryResponse)
async def get_my_coach_memory(
    current_user: sql_models.User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    if not current_user.athlete_profile or not current_user.athlete_profile.coach_memory:
        raise HTTPException(status_code=404, detail="Profil ou M√©moire introuvable. Compl√©tez votre profil.")
    return current_user.athlete_profile.coach_memory

@router.post("/coach-memories/recalculate")
async def force_recalculate(
    current_user: sql_models.User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    profile = current_user.athlete_profile
    if not profile or not profile.coach_memory:
        raise HTTPException(status_code=404, detail="Introuvable")
        
    CoachLogic.update_daily(profile.coach_memory, profile)
    db.commit()
    return {"status": "updated", "new_readiness": profile.coach_memory.current_context.get('readiness_score')}
-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/routers/safety.py
================================================================================
from fastapi import APIRouter, HTTPException
from app.models.schemas import ACWRRequest, ACWRResponse
from app.domain import safety

router = APIRouter(
    prefix="/safety",
    tags=["Safety & Prevention"]
)

@router.post("/acwr", response_model=ACWRResponse)
async def compute_acwr_metrics(payload: ACWRRequest):
    """
    Calcule le Ratio Aigu/Chronique (ACWR) pour pr√©venir les blessures.
    Envoie l'historique des s√©ances (Date, Dur√©e, RPE).
    Retourne le statut de risque (Optimal, Surcharge, Danger).
    """
    try:
        # Conversion des mod√®les Pydantic en liste de dicts pour Pandas
        history_dicts = [log.dict() for log in payload.history]
        
        result = safety.calculate_acwr(history_dicts)
        
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/routers/user.py
================================================================================
import json
from fastapi import APIRouter, Depends, HTTPException, status
from sqlalchemy.orm import Session
from typing import Dict, Any, Optional

from app.core.database import get_db
from app.models import sql_models, schemas
from app.dependencies import get_current_user

# üö® CORRECTIF ROUTING : On retire le pr√©fixe ici.
# Il sera inject√© depuis le main.py pour plus de contr√¥le.
router = APIRouter()

# --- ENDPOINTS PROFIL (Nouvelle Route: /api/v1/profiles/...) ---

@router.get("/me", response_model=schemas.UserResponse)
async def get_my_profile_data(
    current_user: sql_models.User = Depends(get_current_user),
):
    """
    R√©cup√®re le profil connect√©.
    URL Finale : GET /api/v1/profiles/me
    """
    if current_user.profile_data is None:
        current_user.profile_data = {}
    
    # SQLAlchemy avec type JSON renvoie d√©j√† un Dict, donc pas de parsing n√©cessaire ici.
    # Le validateur Pydantic (schemas.py) est l√† en s√©curit√© si jamais c'√©tait une string.
        
    return current_user

@router.post("/complete", response_model=schemas.UserResponse)
async def complete_profile(
    profile_update: schemas.ProfileUpdate,
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    Sauvegarde le profil complet.
    Logic : Input (Dict) -> DB (JSON Type - Auto Serialize) -> Output (Dict)
    """
    try:
        # 1. R√©cup√©ration des donn√©es brutes (Dict)
        data_dict = profile_update.profile_data
        
        # 2. [CORRECTION] Assignation DIRECTE du Dictionnaire
        # Le mod√®le SQL 'User' utilise le type JSON, SQLAlchemy g√®re la s√©rialisation.
        # On passe directement le dictionnaire Python.
        current_user.profile_data = data_dict
        
        # 3. Mise √† jour des champs relationnels (User table)
        if "basic_info" in data_dict:
            basic = data_dict["basic_info"]
            
            # Mise √† jour email si fourni et non vide
            if "email" in basic and basic["email"]:
                current_user.email = basic["email"]
            
            # Mise √† jour pseudo si fourni et non vide
            if "pseudo" in basic and basic["pseudo"]:
                current_user.username = basic["pseudo"]

        # Sauvegarde SQL effective
        db.commit()
        db.refresh(current_user)
        
        # 4. Retour
        # SQLAlchemy a mis √† jour current_user.profile_data qui est maintenant un Dict (gr√¢ce au type JSON).
        # Pydantic (UserResponse) attend un Dict. Tout est align√©.
        return current_user
        
    except Exception as e:
        db.rollback()
        # On log l'erreur pour le debug serveur
        print(f"üî• ERREUR CRITIQUE SAVE PROFILE: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Erreur sauvegarde profil : {str(e)}"
        )

# --- SUPPORT LEGACY (Optionnel, conserv√© pour compatibilit√© existante) ---
# Ces routes seront d√©sormais pr√©fix√©es par /api/v1/profiles aussi via le main.py

@router.post("/sections/{section}")
async def update_profile_section(
    section: str,
    section_data: schemas.ProfileSectionUpdate,
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    Mise √† jour partielle d'une section.
    Adapt√© pour le type JSON de SQLAlchemy.
    """
    try:
        # 1. Chargement des donn√©es existantes
        # Avec le type JSON, SQLAlchemy nous renvoie directement un Dict ou None
        raw_data = current_user.profile_data
        
        current_data = {}
        if raw_data:
            if isinstance(raw_data, dict):
                current_data = raw_data.copy() # Copie pour √©viter les effets de bord
            elif isinstance(raw_data, str):
                # S√©curit√© au cas o√π d'anciennes donn√©es String tra√Ænent
                try:
                    current_data = json.loads(raw_data)
                except json.JSONDecodeError:
                    current_data = {}
        
        # 2. Mise √† jour de la section
        current_data[section] = section_data.section_data
        
        # 3. Sauvegarde (Direct Dict -> JSON Column)
        current_user.profile_data = current_data
        
        db.commit()
        
        return {
            "status": "success",
            "section": section,
            "updated_data": section_data.section_data
        }
    except Exception as e:
        db.rollback()
        print(f"üî• ERREUR LEGACY SECTION: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/routers/workouts.py
================================================================================
from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks
from sqlalchemy.orm import Session
from typing import List
from app.core.database import get_db
from app.models import sql_models, schemas
from app.dependencies import get_current_user
import json

# Imports du Moteur de Feed
from app.services.feed.engine import TriggerEngine
from app.services.feed.triggers.workout_analysis import WorkoutAnalysisTrigger

router = APIRouter(
    prefix="/workouts",
    tags=["Workouts"]
)

@router.post("/", response_model=schemas.WorkoutSessionResponse)
async def create_workout(
    workout: schemas.WorkoutSessionCreate, 
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    Enregistre une s√©ance compl√®te avec gestion du Polymorphisme (Metric Type).
    V√©rifie la coh√©rence des donn√©es (ex: Watts max, RPE bounds).
    Supprime le brouillon associ√© une fois la s√©ance valid√©e.
    Active le Neural Feed pour l'analyse post-s√©ance.
    """

    # --- VALIDATION PHYSIOLOGIQUE ---
    def validate_physiological_limits(workout: schemas.WorkoutSessionCreate):
        """Valide les limites physiologiques humaines."""
        
        # Dur√©e r√©aliste (10min √† 4h)
        if workout.duration < 10 or workout.duration > 240:
            raise HTTPException(
                status_code=400, 
                detail=f"Dur√©e invalide ({workout.duration} min). Doit √™tre entre 10 et 240 minutes."
            )
        
        # RPE 1-10
        if workout.rpe < 1 or workout.rpe > 10:
            raise HTTPException(
                status_code=400,
                detail=f"RPE invalide ({workout.rpe}). Doit √™tre entre 1 et 10."
            )
        
        # √ânergie 1-10
        if workout.energy_level < 1 or workout.energy_level > 10:
            raise HTTPException(
                status_code=400,
                detail=f"Niveau d'√©nergie invalide ({workout.energy_level}). Doit √™tre entre 1 et 10."
            )
        
        # Validation des sets
        for s in workout.sets:
            # Watts max (record du monde ~2500W)
            if s.metric_type == 'POWER_TIME' and s.weight > 2000:
                raise HTTPException(
                    status_code=400,
                    detail=f"Puissance impossible ({s.weight}W). Record du monde ~2500W."
                )
            
            # Charge max (record +500kg)
            if s.metric_type == 'LOAD_REPS' and s.weight > 500:
                raise HTTPException(
                    status_code=400,
                    detail=f"Charge impossible ({s.weight}kg). Record du monde ~500kg."
                )
            
            # RPE s√©rie
            if s.rpe and (s.rpe < 1 or s.rpe > 10):
                raise HTTPException(
                    status_code=400,
                    detail=f"RPE s√©rie invalide ({s.rpe}). Doit √™tre entre 1 et 10."
                )
        
        return True

    # Appliquer la validation
    validate_physiological_limits(workout)

    # 1. Validation de haut niveau avant insertion
    for s in workout.sets:
        # Validation RPE
        if s.rpe is not None and (s.rpe < 0 or s.rpe > 10):
            # On cap plut√¥t que de crasher
            s.rpe = max(0, min(10, s.rpe))
            
        # Validation Physiologique selon le mode
        if s.metric_type == 'POWER_TIME':
            # Check Watts (weight)
            if s.weight > 2000:
                raise HTTPException(status_code=400, detail=f"Valeur impossible : {s.weight} Watts sur l'exercice {s.exercise_name}. V√©rifiez la saisie.")
        
        elif s.metric_type == 'PACE_DISTANCE':
            # Standard TitanFlow : Reps = Distance (m), Weight = 0 (ou vitesse m/s)
            if s.reps > 100000: # 100km max par s√©rie pour √™tre s√ªr
                 raise HTTPException(status_code=400, detail=f"Distance suspecte : {s.reps} m√®tres.")

    # 2. Cr√©ation de la Session (INSERT)
    db_workout = sql_models.WorkoutSession(
        date=workout.date,
        duration=workout.duration,
        rpe=workout.rpe,
        energy_level=workout.energy_level,
        notes=workout.notes,
        ai_analysis=workout.ai_analysis, # <--- AJOUT CRITIQUE POUR BE-03
        user_id=current_user.id
    )
    db.add(db_workout)
    db.commit()
    db.refresh(db_workout)
    
    # 3. Ajout des S√©ries (Sets)
    if workout.sets:
        for s in workout.sets:
            # Conversion explicite Pydantic -> SQL Model
            db_set = sql_models.WorkoutSet(
                session_id=db_workout.id,
                exercise_name=s.exercise_name,
                set_order=s.set_order,
                weight=s.weight, # D√©j√† nettoy√© par Pydantic (float)
                reps=s.reps,     # D√©j√† nettoy√© par Pydantic (float, secondes inclues)
                rpe=s.rpe,
                rest_seconds=s.rest_seconds, # <--- DEJA SUPPORTE PAR SQL MODEL
                metric_type=s.metric_type    # <--- DEJA SUPPORTE PAR SQL MODEL
            )
            db.add(db_set)
        
        # Nettoyage du brouillon apr√®s succ√®s
        current_user.draft_workout_data = None
        
        db.commit()
        db.refresh(db_workout)

    # 4. TRIGGER NEURAL FEED (L'IA s'active ici)
    try:
        # On passe le profil complet dans le contexte via user_data
        profile_data = {}
        if current_user.profile_data:
            try:
                profile_data = json.loads(current_user.profile_data)
            except:
                pass

        engine = TriggerEngine()
        engine.register(WorkoutAnalysisTrigger())
        await engine.run_all(db, current_user.id, {
            "workout": db_workout,
            "profile": profile_data
        })
    except Exception as e:
        # On ne bloque pas la r√©ponse si l'IA √©choue, c'est du bonus
        print(f"‚ö†Ô∏è Feed Engine Error: {e}")
    
    return db_workout

@router.get("/", response_model=List[schemas.WorkoutSessionResponse])
async def read_workouts(
    skip: int = 0, 
    limit: int = 100, 
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    R√©cup√®re l'historique complet.
    Les champs polymorphes (weight/reps) sont renvoy√©s tels quels,
    le Frontend utilisera 'metric_type' pour savoir si c'est des kg ou des watts.
    """
    workouts = db.query(sql_models.WorkoutSession)\
        .filter(sql_models.WorkoutSession.user_id == current_user.id)\
        .order_by(sql_models.WorkoutSession.date.desc())\
        .offset(skip)\
        .limit(limit)\
        .all()
    return workouts-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/services/coach_logic.py
================================================================================
from datetime import date
from typing import Dict, Any
from app.models import sql_models

VALID_SPORT_POSITIONS = {
    'Rugby': ['Pilier', 'Talonneur', '2√®me ligne', '3√®me ligne', 'Demi', 'Centre', 'Ailier', 'Arri√®re'],
    'Football': ['Gardien', 'D√©fenseur', 'Milieu', 'Attaquant'],
}

class CoachLogic:
    @staticmethod
    def validate_sport_position(sport: str, position: str) -> bool:
        if sport in VALID_SPORT_POSITIONS:
            if position and position not in VALID_SPORT_POSITIONS[sport]:
                return False
        return True

    @staticmethod
    def initialize_memory(profile: sql_models.AthleteProfile) -> sql_models.CoachMemory:
        sport = profile.sport_context.get('sport', 'Autre')
        insights = {
            "primary_sport": sport,
            "specificity_index": "High" if sport in ['Rugby', 'Football'] else "Medium",
            "focus_areas": ["Strength", "Hypertrophy"] 
        }
        context = {
            "macrocycle_phase": "Adaptation Anatomique",
            "fatigue_state": "Fresh",
            "readiness_score": 100,
            "season_week": 1
        }
        flags = {
            "needs_deload": False,
            "injury_risk": False,
            "adaptation_window_open": True
        }
        memory = sql_models.CoachMemory(
            athlete_profile_id=profile.id,
            sport_specific_insights=insights,
            current_context=context,
            memory_flags=flags,
            coach_notes={"initialization": f"Profil cr√©√© le {date.today()}"}
        )
        return memory

    @staticmethod
    def calculate_readiness(profile: sql_models.AthleteProfile) -> int:
        base_score = 80
        sleep = profile.physical_metrics.get('sleep_quality_avg', 5)
        if sleep >= 8: base_score += 10
        elif sleep <= 4: base_score -= 20
        stress = profile.constraints.get('work_stress_level', 5)
        if stress >= 8: base_score -= 15
        return max(0, min(100, base_score))

    @staticmethod
    def update_daily(memory: sql_models.CoachMemory, profile: sql_models.AthleteProfile):
        new_readiness = CoachLogic.calculate_readiness(profile)
        current_context = dict(memory.current_context or {})
        current_context['readiness_score'] = new_readiness
        
        if new_readiness < 40:
            current_context['fatigue_state'] = "High"
        elif new_readiness < 70:
            current_context['fatigue_state'] = "Moderate"
        else:
            current_context['fatigue_state'] = "Optimal"
            
        memory.current_context = current_context
        flags = dict(memory.memory_flags or {})
        flags['needs_deload'] = new_readiness < 30
        flags['adaptation_window_open'] = new_readiness > 70
        memory.memory_flags = flags
-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/services/coach_memory/service.py
================================================================================
"""
Service de gestion de la m√©moire du coach
"""
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, Any, Optional, List
from sqlalchemy.orm import Session

from app.models import sql_models
from app.domain.bioenergetics import BioenergeticService

logger = logging.getLogger(__name__)

class CoachMemoryService:
    """Service principal pour la m√©moire du coach"""
    
    @staticmethod
    def initialize_coach_memory(athlete_profile: sql_models.AthleteProfile, db: Session) -> sql_models.CoachMemory:
        """Initialise la m√©moire du coach √† partir du profil"""
        logger.info(f"Initialisation de la m√©moire du coach pour l'athl√®te {athlete_profile.user_id}")
        
        # Extraire les donn√©es du profil
        basic_info = json.loads(athlete_profile.basic_info) if athlete_profile.basic_info else {}
        sport_context = json.loads(athlete_profile.sport_context) if athlete_profile.sport_context else {}
        performance_baseline = json.loads(athlete_profile.performance_baseline) if athlete_profile.performance_baseline else {}
        
        # Calculer les insights initiaux
        sport_insights = CoachMemoryService._calculate_initial_sport_insights(sport_context, basic_info)
        performance_baselines = CoachMemoryService._extract_initial_baselines(performance_baseline)
        initial_phase = CoachMemoryService._determine_initial_phase(athlete_profile)
        
        # Cr√©er la m√©moire
        memory = sql_models.CoachMemory(
            athlete_profile_id=athlete_profile.id,
            # [CORRECTION] Utilisation de metadata_info
            metadata_info=json.dumps({
                "athlete_id": athlete_profile.user_id,
                "created_at": datetime.utcnow().isoformat(),
                "last_updated": datetime.utcnow().isoformat(),
                "total_interactions": 0,
                "trust_score": 50,
                "data_points": 0
            }),
            current_context=json.dumps({
                'season_week': 1,
                'macrocycle_phase': initial_phase,
                'mesocycle_focus': 'base_fitness',
                'training_priority': 'volume',
                'next_competition': None,
                'days_to_competition': None,
                'fatigue_state': 'fresh',
                'readiness_score': 80,
                'current_constraints': json.loads(athlete_profile.constraints) if athlete_profile.constraints else {},
                'environmental_factors': {},
                'last_session_type': None,
                'last_session_rpe': None
            }),
            response_patterns=json.dumps({
                "volume_response": "neutral",
                "optimal_volumes": {},
                "intensity_tolerance": "medium",
                "recovery_profile": "normal",
                "fatigue_indicators": []
            }),
            performance_baselines=json.dumps(performance_baselines),
            adaptation_signals=json.dumps({
                "positive_adaptations": [],
                "last_adaptation_phase": None,
                "current_adaptation_status": "initial",
                "stagnation_signals": [],
                "regression_signals": [],
                "adaptation_windows": [],
                "next_suggested_focus": "base_fitness"
            }),
            sport_specific_insights=json.dumps(sport_insights),
            training_history_summary=json.dumps({
                "total_volume_by_type": {},
                "average_rpe_by_type": {},
                "successful_strategies": [],
                "failed_strategies": [],
                "lessons_learned": [],
                "seasonal_patterns": {},
                "best_training_weeks": [],
                "peak_periods": []
            }),
            athlete_preferences=json.dumps(json.loads(athlete_profile.training_preferences) if athlete_profile.training_preferences else {}),
            coach_notes=json.dumps({}),
            memory_flags=json.dumps({
                "needs_deload": False,
                "approaching_overtraining": False,
                "detraining_risk": False,
                "technique_regression": False,
                "adaptation_window_open": True,
                "pr_potential": False,
                "skill_integration_ready": False,
                "external_stress_high": False,
                "recovery_impaired": False,
                "motivation_low": False
            })
        )
        
        db.add(memory)
        db.commit()
        logger.info(f"M√©moire du coach cr√©√©e avec ID: {memory.id}")
        
        return memory
    
    @staticmethod
    def process_workout_session(
        coach_memory: sql_models.CoachMemory,
        athlete_profile: sql_models.AthleteProfile,
        session_data: Dict[str, Any],
        db: Session
    ) -> None:
        """Traite une s√©ance d'entra√Ænement et met √† jour la m√©moire"""
        logger.info(f"Traitement de la s√©ance pour la m√©moire {coach_memory.id}")
        
        # [CORRECTION] Mettre √† jour les m√©tadonn√©es via metadata_info
        metadata = json.loads(coach_memory.metadata_info) if coach_memory.metadata_info else {}
        metadata['total_interactions'] = metadata.get('total_interactions', 0) + 1
        metadata['last_updated'] = datetime.utcnow().isoformat()
        
        # Mettre √† jour le contexte
        context = json.loads(coach_memory.current_context) if coach_memory.current_context else {}
        context['last_session_type'] = session_data.get('type', 'unknown')
        context['last_session_rpe'] = session_data.get('rpe', 0)
        context['last_session_date'] = datetime.now().isoformat()
        
        # Mettre √† jour l'historique d'entra√Ænement
        history = json.loads(coach_memory.training_history_summary) if coach_memory.training_history_summary else {}
        
        session_type = session_data.get('type', 'strength')
        volume = session_data.get('volume', 0)
        rpe = session_data.get('rpe', 5)
        
        if 'total_volume_by_type' not in history:
            history['total_volume_by_type'] = {}
        
        history['total_volume_by_type'][session_type] = history['total_volume_by_type'].get(session_type, 0) + volume
        
        if 'average_rpe_by_type' not in history:
            history['average_rpe_by_type'] = {}
        
        if session_type not in history['average_rpe_by_type']:
            history['average_rpe_by_type'][session_type] = {'total': 0, 'count': 0}
        
        history['average_rpe_by_type'][session_type]['total'] += rpe
        history['average_rpe_by_type'][session_type]['count'] += 1
        
        # Calculer les r√©ponses √† l'entra√Ænement
        response_patterns = json.loads(coach_memory.response_patterns) if coach_memory.response_patterns else {}
        
        # [CORRECTION] Sauvegarde
        coach_memory.metadata_info = json.dumps(metadata)
        coach_memory.current_context = json.dumps(context)
        coach_memory.training_history_summary = json.dumps(history)
        coach_memory.response_patterns = json.dumps(response_patterns)
        
        db.commit()
        logger.info(f"S√©ance trait√©e pour la m√©moire {coach_memory.id}")
    
    # ... (Le reste des m√©thodes update_daily_context et generate_insights n'utilise pas metadata, on peut les laisser telles quelles)

    @staticmethod
    def recalculate_memory(
        coach_memory: sql_models.CoachMemory,
        athlete_profile: sql_models.AthleteProfile,
        db: Session
    ) -> None:
        """Recalcule compl√®tement la m√©moire"""
        logger.info(f"Recalcul complet de la m√©moire {coach_memory.id}")
        
        # [CORRECTION] Recalculer tous les composants via metadata_info
        metadata = json.loads(coach_memory.metadata_info) if coach_memory.metadata_info else {}
        metadata['last_recalculated'] = datetime.utcnow().isoformat()
        metadata['version'] = metadata.get('version', 1) + 1
        
        # Recalculer les performances de base
        performance_baseline = json.loads(athlete_profile.performance_baseline) if athlete_profile.performance_baseline else {}
        updated_baselines = CoachMemoryService._extract_initial_baselines(performance_baseline)
        
        # [CORRECTION] Mettre √† jour la m√©moire
        coach_memory.metadata_info = json.dumps(metadata)
        coach_memory.performance_baselines = json.dumps(updated_baselines)
        # Note: 'version' n'est pas une colonne SQL, elle est stock√©e dans le JSON metadata_info
        
        db.commit()
        logger.info(f"M√©moire {coach_memory.id} recalcul√©e - version {metadata['version']}")
    
    # ... (Les m√©thodes priv√©es helper et wrappers restent inchang√©es) ...

    # Wrappers pour compatibilit√©
    @staticmethod
    def update_daily_context(coach_memory, checkin_data, db):
        # ... Code existant inchang√© ...
        logger.info(f"Mise √† jour du contexte quotidien pour la m√©moire {coach_memory.id}")
        context = json.loads(coach_memory.current_context) if coach_memory.current_context else {}
        readiness_score = CoachMemoryService._calculate_readiness_score(checkin_data, context)
        context['readiness_score'] = readiness_score
        context['fatigue_state'] = CoachMemoryService._determine_fatigue_state(readiness_score)
        memory_flags = json.loads(coach_memory.memory_flags) if coach_memory.memory_flags else {}
        memory_flags['needs_deload'] = readiness_score < 40
        memory_flags['adaptation_window_open'] = readiness_score > 70
        memory_flags['recovery_impaired'] = checkin_data.get('sleep_quality', 5) < 4
        coach_memory.current_context = json.dumps(context)
        coach_memory.memory_flags = json.dumps(memory_flags)
        db.commit()
        logger.info(f"Contexte mis √† jour - Readiness: {readiness_score}")
        return context

    @staticmethod
    def generate_insights(coach_memory, athlete_profile, db):
        # ... Code existant inchang√© ...
        context = json.loads(coach_memory.current_context) if coach_memory.current_context else {}
        performance_baselines = json.loads(coach_memory.performance_baselines) if coach_memory.performance_baselines else {}
        sport_insights = json.loads(coach_memory.sport_specific_insights) if coach_memory.sport_specific_insights else {}
        insights = {
            "readiness_insight": CoachMemoryService._generate_readiness_insight(context),
            "fatigue_management": CoachMemoryService._generate_fatigue_insight(context),
            "progression_opportunities": CoachMemoryService._generate_progression_insights(performance_baselines),
            "sport_specific_recommendations": CoachMemoryService._generate_sport_recommendations(sport_insights),
            "risk_assessment": CoachMemoryService._generate_risk_assessment(coach_memory)
        }
        return insights

    # Helper methods (inchang√©s mais inclus pour r√©f√©rence de classe)
    @staticmethod
    def _calculate_initial_sport_insights(sport_context, basic_info):
        # ... (Identique √† l'original) ...
        return {"primary_sport": sport_context.get('primary_sport', 'Musculation')} 

    @staticmethod
    def _extract_initial_baselines(performance_baseline):
        # ... (Identique √† l'original) ...
        return {"current_prs": performance_baseline.get('current_prs', {})}

    @staticmethod
    def _determine_initial_phase(athlete_profile):
        # ... (Identique √† l'original) ...
        return "base_fitness"

    @staticmethod
    def _calculate_readiness_score(checkin_data, context):
        # ... (Identique √† l'original) ...
        return 80

    @staticmethod
    def _determine_fatigue_state(readiness_score):
        # ... (Identique √† l'original) ...
        return "fresh" if readiness_score >= 80 else "normal"

    @staticmethod
    def _generate_readiness_insight(context):
        return "Optimal"

    @staticmethod
    def _generate_fatigue_insight(context):
        return "Balanced"

    @staticmethod
    def _generate_progression_insights(performance_baselines):
        return []

    @staticmethod
    def _generate_sport_recommendations(sport_insights):
        return []

    @staticmethod
    def _generate_risk_assessment(coach_memory):
        return {"risk": "low"}

# Fonctions d'interface pour compatibilit√©
def initialize_coach_memory(athlete_profile: sql_models.AthleteProfile, db: Session) -> sql_models.CoachMemory:
    return CoachMemoryService.initialize_coach_memory(athlete_profile, db)

def process_workout_session(coach_memory, athlete_profile, session_data, db) -> None:
    return CoachMemoryService.process_workout_session(coach_memory, athlete_profile, session_data, db)

def update_daily_context(coach_memory, checkin_data, db) -> Dict[str, Any]:
    return CoachMemoryService.update_daily_context(coach_memory, checkin_data, db)

def generate_insights(coach_memory, athlete_profile, db) -> Dict[str, Any]:
    return CoachMemoryService.generate_insights(coach_memory, athlete_profile, db)

def recalculate_memory(coach_memory, athlete_profile, db) -> None:
    return CoachMemoryService.recalculate_memory(coach_memory, athlete_profile, db)-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/services/feed/engine.py
================================================================================
import logging
import uuid
import json
from datetime import datetime, timedelta
from typing import List, Dict, Any
from sqlalchemy.orm import Session
from sqlalchemy import and_

from app.models import sql_models, schemas
from app.services.feed.triggers.base import BaseTrigger

# Configuration des logs pour ne pas perdre une miette du match
logger = logging.getLogger(__name__)

class TriggerEngine:
    """
    Le Moteur de Jeu.
    Il poss√®de un registre de Triggers et les ex√©cute tous pour un contexte donn√©.
    Il g√®re aussi la s√©curit√© (Anti-Crash) et la filtration (D√©duplication).
    """
    def __init__(self):
        self._registry: List[BaseTrigger] = []

    def register(self, trigger: BaseTrigger):
        """Enr√¥le un nouveau Trigger dans l'√©quipe."""
        self._registry.append(trigger)
        logger.info(f"‚úÖ Trigger enregistr√© : {trigger.__class__.__name__}")

    async def run_all(self, db: Session, user_id: int, context: Dict[str, Any]) -> List[sql_models.FeedItem]:
        """
        Lance tous les Triggers enregistr√©s.
        
        R√®gles du jeu :
        1. Isolation : Si un trigger plante, les autres continuent.
        2. D√©duplication : On √©vite de spammer le m√™me message (ex: 1x par 24h).
        3. Persistance : Sauvegarde imm√©diate en base.
        """
        generated_events = []

        for trigger in self._registry:
            try:
                # Le Trigger analyse le jeu...
                event_schema = await trigger.check(user_id, context)
                
                if event_schema:
                    # Arbitrage vid√©o (D√©duplication)
                    if not self._should_discard(db, user_id, event_schema):
                        
                        # Transformation Schema -> SQL Model
                        db_item = sql_models.FeedItem(
                            id=str(uuid.uuid4()),
                            user_id=user_id,
                            type=event_schema.type,
                            title=event_schema.title,
                            message=event_schema.message,
                            priority=event_schema.priority,
                            is_read=False,
                            is_completed=False,
                            # Gestion propre du JSON payload
                            action_payload=json.dumps(event_schema.action_payload) if event_schema.action_payload else None
                        )
                        
                        db.add(db_item)
                        generated_events.append(db_item)
                        logger.info(f"üì¢ Event g√©n√©r√© : {db_item.title} ({trigger.__class__.__name__})")
                    else:
                        logger.info(f"üîá Event ignor√© (Doublon) : {event_schema.title}")

            except Exception as e:
                # Carton jaune : Le trigger a plant√©, mais le match continue
                logger.error(f"‚ö†Ô∏è Erreur Trigger {trigger.__class__.__name__}: {str(e)}")
                continue

        # Coup de sifflet final : on valide les buts
        if generated_events:
            db.commit()
            for ev in generated_events:
                db.refresh(ev)
                
        return generated_events

    def _should_discard(self, db: Session, user_id: int, event: schemas.FeedItemCreate) -> bool:
        """
        V√©rifie si un √©v√©nement similaire existe d√©j√† r√©cemment.
        R√®gle actuelle : Pas de doublon (M√™me Titre + M√™me Type) non trait√©.
        Ou pas de doublon identique cr√©√© dans les derni√®res 24h.
        """
        # 1. Chercher si le m√™me event est d√©j√† en attente (Non compl√©t√©)
        existing_active = db.query(sql_models.FeedItem).filter(
            sql_models.FeedItem.user_id == user_id,
            sql_models.FeedItem.type == event.type,
            sql_models.FeedItem.title == event.title,
            sql_models.FeedItem.is_completed == False
        ).first()

        if existing_active:
            return True # On jette, l'utilisateur a d√©j√† √ßa dans son feed

        # 2. Chercher si le m√™me event a √©t√© cr√©√© il y a moins de 24h (Anti-Spam)
        one_day_ago = datetime.utcnow() - timedelta(hours=24)
        recent_duplicate = db.query(sql_models.FeedItem).filter(
            sql_models.FeedItem.user_id == user_id,
            sql_models.FeedItem.type == event.type,
            sql_models.FeedItem.title == event.title,
            sql_models.FeedItem.created_at >= one_day_ago
        ).first()

        if recent_duplicate:
            return True

        return False-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/services/feed/triggers/base.py
================================================================================
from abc import ABC, abstractmethod
from typing import Optional, Dict, Any
from app.models import schemas

class BaseTrigger(ABC):
    """
    Interface abstraite pour tous les d√©clencheurs d'√©v√©nements (Triggers).
    Chaque Trigger est un 'sp√©cialiste' (ex: Sp√©cialiste Analyse, Sp√©cialiste Sant√©).
    """

    @abstractmethod
    async def check(self, user_id: int, context: Dict[str, Any]) -> Optional[schemas.FeedItemCreate]:
        """
        Analyse le contexte et retourne un FeedItemCreate si la condition est remplie.
        Retourne None sinon.
        
        :param user_id: L'ID de l'athl√®te concern√©.
        :param context: Un dictionnaire riche contenant les donn√©es (ex: {'workout': ..., 'profile': ...})
        :return: Un objet FeedItemCreate pr√™t √† √™tre ins√©r√©, ou None.
        """
        pass-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/services/feed/triggers/workout_analysis.py
================================================================================
import os
import json
import re
import google.generativeai as genai
from typing import Dict, Any, Optional
from app.services.feed.triggers.base import BaseTrigger
from app.models import schemas, sql_models
from app.domain.bioenergetics import BioenergeticService

class WorkoutAnalysisTrigger(BaseTrigger):
    """
    Trigger : Analyse Post-S√©ance Avanc√©e (Bio-Twin + Gemini).
    Condition : Une s√©ance vient d'√™tre termin√©e.
    Action : 
        1. Calcule les m√©triques bio√©nerg√©tiques (Kcal, Macros).
        2. G√©n√®re un rapport JSON complet via IA.
        3. Sauvegarde le rapport dans la s√©ance (Persistance).
        4. Cr√©e une carte Feed pour notifier l'athl√®te.
    """
    
    def __init__(self):
        self.api_key = os.getenv("GEMINI_API_KEY")

    async def check(self, user_id: int, context: Dict[str, Any]) -> Optional[schemas.FeedItemCreate]:
        # 1. V√©rifie si le contexte contient les donn√©es requises
        workout: sql_models.WorkoutSession = context.get("workout")
        profile_data: Dict[str, Any] = context.get("profile", {})
        
        if not workout:
            return None

        # 2. Si pas de cl√© API, on sort silencieusement
        if not self.api_key:
            return None

        try:
            # 3. PHASE 1 : CALCULS BIO√âNERG√âTIQUES (Les Maths)
            bio_metrics = BioenergeticService.calculate_needs(
                profile_data, 
                workout.sets, 
                workout.duration, 
                workout.rpe
            )
            
            # 4. PHASE 2 : G√âN√âRATION IA (Le Cerveau)
            sets_summary = "\n".join([
                f"- {s.exercise_name}: {s.weight} (load/watts) x {s.reps} (reps/sec/m) [{s.metric_type}]"
                for s in workout.sets
            ])
            
            prompt = f"""
            R√îLE : Expert en Physiologie Sportive et Nutrition (TitanFlow).
            TACHE : Analyser la s√©ance et g√©n√©rer un rapport JSON strict.

            === DONN√âES ATHL√àTE ===
            - Profil : {json.dumps(profile_data, ensure_ascii=False)}
            
            === DONN√âES S√âANCE ===
            - Dur√©e : {workout.duration} min
            - RPE : {workout.rpe}/10 (Intensit√© Ressentie)
            - Contenu :
            {sets_summary}
            
            === DONN√âES BIO-TWIN (CALCUL√âES) ===
            - D√©pense : ~{bio_metrics['kcal_total']} kcal
            - Besoins Post-Effort (Estim√©s) : 
              * Prot√©ines : {bio_metrics['protein_g']}g
              * Glucides : {bio_metrics['carbs_g']}g
              * Eau : {bio_metrics['water_ml']}ml
            
            === STRUCTURE DE SORTIE (JSON UNIQUEMENT) ===
            {{
              "performance_analysis": "Analyse technique de la charge et du volume en 2 phrases max.",
              "nutrition_comment": "Conseil pr√©cis validant ou ajustant les macros calcul√©es ci-dessus.",
              "recovery_score": 8,
              "coach_questions": ["Question pertinente 1?", "Question pertinente 2?"],
              "food_suggestion": {{
                  "option_shake": "Ex: Whey + Banane",
                  "option_solid": "Ex: Poulet + Riz + L√©gumes"
              }},
              "feed_message": "Une phrase d'accroche tr√®s courte (max 12 mots) pour la notification."
            }}
            """

            genai.configure(api_key=self.api_key)
            model = genai.GenerativeModel('gemini-2.0-flash', generation_config={"response_mime_type": "application/json"})
            response = model.generate_content(prompt)
            
            # Nettoyage JSON
            json_str = self._clean_json(response.text)
            analysis_result = json.loads(json_str)

            # 5. PHASE 3 : PERSISTANCE (Sauvegarde en BDD)
            # On fusionne les m√©triques calcul√©es avec l'analyse IA
            full_report = {
                **analysis_result,
                "bio_metrics": bio_metrics
            }
            
            # On stocke le JSON stringifi√© dans la colonne ai_analysis de la s√©ance
            # Note: L'objet 'workout' est attach√© √† la session DB, donc le commit du TriggerEngine validera cette modif.
            workout.ai_analysis = json.dumps(full_report)

            # 6. PHASE 4 : NOTIFICATION (Le Feed)
            # On utilise le message court g√©n√©r√© par l'IA pour le feed
            feed_msg = analysis_result.get("feed_message", "Analyse de s√©ance disponible.")

            return schemas.FeedItemCreate(
                type=schemas.FeedItemType.ANALYSIS,
                title="Rapport de S√©ance",
                message=feed_msg,
                priority=5,
                action_payload={
                    "route": "/history", 
                    # On pourra passer des args pour ouvrir directement le d√©tail plus tard
                    "args": {"workout_id": workout.id}
                }
            )

        except Exception as e:
            print(f"‚ö†Ô∏è Erreur IA Analysis: {e}")
            # Fallback : Si l'IA plante, on ne cr√©e pas de FeedItem, 
            # ou on pourrait en cr√©er un g√©n√©rique. Ici on choisit la discr√©tion.
            return None

    def _clean_json(self, text: str) -> str:
        """Extrait le JSON si l'IA ajoute du markdown."""
        try:
            pattern = r"```(?:json)?\s*([\s\S]*?)\s*```"
            match = re.search(pattern, text)
            if match:
                return match.group(1).strip()
            return text.strip()
        except:
            return text-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/services/onboarding.py
================================================================================
import math
from typing import Dict, Any, Optional, List, Generic, TypeVar, Union
from pydantic import BaseModel
from dataclasses import dataclass

# --- TYPES G√âN√âRIQUES POUR LE PATTERN RESULT ---

T = TypeVar('T')

@dataclass
class ValidationError:
    field: str
    message: str
    value: Any

@dataclass
class ServiceResult(Generic[T]):
    success: bool
    data: Optional[T] = None
    errors: List[ValidationError] = None

    @staticmethod
    def ok(data: T) -> 'ServiceResult[T]':
        return ServiceResult(success=True, data=data)

    @staticmethod
    def fail(errors: List[ValidationError]) -> 'ServiceResult[T]':
        return ServiceResult(success=False, errors=errors)

# --- SERVICE D'ONBOARDING ---

class AthleteOnboardingService:
    """
    Service responsable de l'ingestion, de la validation et de l'enrichissement
    du profil athl√®te avant persistance.
    Agit comme un 'Expert Filter'.
    """

    @staticmethod
    def process_profile(raw_profile_data: Dict[str, Any]) -> ServiceResult[Dict[str, Any]]:
        """
        Traite les donn√©es brutes du profil.
        1. Valide les garde-fous (Sanity Checks).
        2. G√®re les Null-States (Flags).
        3. Calcule les m√©triques d√©riv√©es (CSS, Relative Strength).
        """
        errors = []
        enriched_data = raw_profile_data.copy()
        
        # Initialisation des sous-structures si absentes
        if 'performance_baseline' not in enriched_data:
            enriched_data['performance_baseline'] = {}
        if 'physical_metrics' not in enriched_data:
            enriched_data['physical_metrics'] = {}
        if 'memory_flags' not in enriched_data:
            enriched_data['memory_flags'] = {}

        perf = enriched_data['performance_baseline']
        phys = enriched_data['physical_metrics']
        flags = enriched_data['memory_flags']

        # --- 1. NULL-STATE LOGIC (Les drapeaux) ---
        
        # Check Force
        squat_1rm = perf.get('squat_1rm')
        if not squat_1rm:
            flags['NEEDS_TESTING_FORCE'] = True
            flags['force_status'] = "Unknown"
        else:
            flags['NEEDS_TESTING_FORCE'] = False
            flags['force_status'] = "Tested"

        # Check A√©robie (VMA)
        vma = perf.get('vma')
        if not vma:
            flags['NEEDS_TESTING_AEROBIC'] = True
            flags['aerobic_status'] = "Unknown"
        else:
            flags['NEEDS_TESTING_AEROBIC'] = False
            flags['aerobic_status'] = "Tested"

        # --- 2. SANITY CHECKS (Les Garde-fous) ---

        # VMA Humaine Max (~26km/h pour Kipchoge sur marathon, on est large mais safe)
        if vma and (isinstance(vma, (int, float))):
            if vma > 26.0:
                errors.append(ValidationError(
                    field="vma", 
                    message="VMA suspecte (> 26 km/h). √ätes-vous s√ªr ?", 
                    value=vma
                ))
            if vma < 3.0: # Marcher c'est 4-5km/h
                 errors.append(ValidationError(
                    field="vma", 
                    message="VMA trop faible (< 3 km/h).", 
                    value=vma
                ))

        # Poids de corps
        weight = phys.get('weight')
        if weight and (weight < 30 or weight > 250):
             errors.append(ValidationError(
                field="weight", 
                message="Poids hors normes physiologiques (30-250kg).", 
                value=weight
            ))

        # --- 3. COMPUTED FIELDS (Calculs Automatiques) ---

        # Relative Strength (Force Relative)
        if squat_1rm and weight and weight > 0:
            try:
                ratio = round(float(squat_1rm) / float(weight), 2)
                perf['relative_strength_squat'] = ratio
            except (ValueError, TypeError):
                pass # On ignore silencieusement si les types sont foireux (clean√©s par Pydantic ailleurs normalement)

        # Critical Swim Speed (CSS)
        # Formule : CSS = (400 - 200) / (T400 - T200)
        # T en secondes.
        t400 = perf.get('swim_400m_time_sec')
        t200 = perf.get('swim_200m_time_sec')

        if t400 and t200:
            try:
                t400_f = float(t400)
                t200_f = float(t200)
                
                delta_dist = 200.0 # 400m - 200m
                delta_time = t400_f - t200_f

                if delta_time <= 0:
                    errors.append(ValidationError(
                        field="swim_times", 
                        message="Le temps au 400m doit √™tre sup√©rieur au temps au 200m.", 
                        value=f"400:{t400}, 200:{t200}"
                    ))
                else:
                    css = round(delta_dist / delta_time, 2) # m/s

                    # Sanity Check CSS
                    if css < 0.5 or css > 2.5:
                        errors.append(ValidationError(
                            field="calculated_css", 
                            message=f"CSS calcul√© ({css} m/s) hors limites (0.5 - 2.5 m/s). V√©rifiez les temps.", 
                            value=css
                        ))
                    else:
                        perf['critical_swim_speed'] = css
                        
            except (ValueError, TypeError):
                # Si les inputs ne sont pas des nombres propres
                errors.append(ValidationError(
                    field="swim_times", 
                    message="Format des temps de natation invalide.", 
                    value=f"400:{t400}, 200:{t200}"
                ))

        # Si erreurs bloquantes, on rejette tout
        if errors:
            return ServiceResult.fail(errors)

        # Mise √† jour des donn√©es enrichies
        enriched_data['performance_baseline'] = perf
        enriched_data['memory_flags'] = flags
        
        return ServiceResult.ok(enriched_data)-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/services/schedule_validator.py
================================================================================
from typing import List, Dict, Optional
from dataclasses import dataclass
from app.models.domain import AthleteProfileDomain, TimeSlot
from app.models.enums import SlotStatus, LocationContext, CoachingMandate

@dataclass
class ConstraintWarning:
    """
    Avertissement g√©n√©r√© par le validateur pour l'interface utilisateur.
    """
    code: str
    message: str
    affected_days: List[str]

class ScheduleValidatorService:
    """
    Moteur de r√®gles physiologiques et logistiques.
    V√©rifie la coh√©rence du planning AVANT la g√©n√©ration du programme.
    """

    # Mapping temporel pour identifier les contigu√Øt√©s
    TIME_ORDER = {"Matin": 0, "Midi": 1, "Soir": 2}
    DAYS_ORDER = ["Lundi", "Mardi", "Mercredi", "Jeudi", "Vendredi", "Samedi", "Dimanche"]

    @staticmethod
    def validate_and_tag(profile: AthleteProfileDomain) -> List[ConstraintWarning]:
        """
        Applique les r√®gles strictes (Hard Rules).
        Modifie le profil IN-PLACE (ajout de tags) et retourne des warnings.
        """
        warnings: List[ConstraintWarning] = []
        
        # On trie la matrice pour faciliter l'analyse s√©quentielle
        # (M√™me si elle devrait l'√™tre, on s'assure de l'ordre Lundi Matin -> Dimanche Soir)
        sorted_slots = sorted(
            profile.time_matrix,
            key=lambda s: (
                ScheduleValidatorService.DAYS_ORDER.index(s.day_of_week),
                ScheduleValidatorService.TIME_ORDER[s.time_of_day]
            )
        )

        # --- R√àGLE 1 : CONFLIT mTOR/AMPK AIGU ---
        # Si slot EXTERNE INTENSE (PPS, RPE >= 7), le slot PR√âC√âDENT (<6h) est brid√©.
        
        for i, slot in enumerate(sorted_slots):
            if slot.status == SlotStatus.EXTERNAL_LOCKED and slot.external_load:
                # V√©rification des conditions d√©clenchantes
                is_pps = "PPS" in slot.external_load.type.upper() or "MATCH" in slot.external_load.type.upper()
                is_high_intensity = slot.external_load.estimated_rpe >= 7
                
                if (is_pps or is_high_intensity):
                    # Identifier le slot pr√©c√©dent
                    # Condition < 6h : On regarde uniquement le cr√©neau d'avant LE M√äME JOUR.
                    # (Soir J-1 -> Matin J est > 6h de sommeil en g√©n√©ral, donc hors scope conflit aigu imm√©diat)
                    if i > 0:
                        prev_slot = sorted_slots[i-1]
                        same_day = prev_slot.day_of_week == slot.day_of_week
                        
                        # Si c'est le m√™me jour et que le slot pr√©c√©dent est contigu
                        # Matin -> Midi OU Midi -> Soir
                        time_diff = ScheduleValidatorService.TIME_ORDER[slot.time_of_day] - ScheduleValidatorService.TIME_ORDER[prev_slot.time_of_day]
                        
                        if same_day and time_diff == 1:
                            # APPLICATION DE LA SANCTION PHYSIOLOGIQUE
                            if "RESTRICTED_LEG_VOLUME" not in prev_slot.tags:
                                prev_slot.tags.append("RESTRICTED_LEG_VOLUME")
                                warnings.append(ConstraintWarning(
                                    code="INTERFERENCE_ALERT",
                                    message=f"Interf√©rence d√©tect√©e le {slot.day_of_week}. Le volume jambes sera r√©duit avant votre s√©ance de {slot.external_load.type}.",
                                    affected_days=[slot.day_of_week]
                                ))

        # --- R√àGLE 2 : R√âCUP√âRATION SYST√àME NERVEUX (CNS) ---
        # Si PPG_ONLY + Charge Externe > 10h/semaine -> Cap Force
        
        if profile.mandate == CoachingMandate.PPG_ONLY:
            total_external_minutes = sum(
                s.external_load.duration_min 
                for s in sorted_slots 
                if s.status == SlotStatus.EXTERNAL_LOCKED and s.external_load
            )
            
            if total_external_minutes > 600: # 10 heures
                warnings.append(ConstraintWarning(
                    code="CNS_PROTECTION",
                    message="Charge externe √©lev√©e (>10h). Programme de force plafonn√© √† 2 s√©ances/semaine pour pr√©server le syst√®me nerveux.",
                    affected_days=[]
                ))
                # On taggue tous les slots disponibles pour informer le g√©n√©rateur
                for s in sorted_slots:
                    if s.status == SlotStatus.AVAILABLE:
                        s.tags.append("FORCE_VOLUME_CAP_2_SESSIONS")

        # --- R√àGLE 3 : LOGISTIQUE (MAT√âRIEL) ---
        # Si Location == HOME -> Pas de Deadlift (Lourd)
        
        home_days = []
        for slot in sorted_slots:
            if slot.location == LocationContext.HOME:
                if "NO_DEADLIFT" not in slot.tags:
                    slot.tags.append("NO_DEADLIFT")
                    # On √©vite les doublons de jours pour le warning
                    if slot.day_of_week not in home_days:
                        home_days.append(slot.day_of_week)
        
        if home_days:
            warnings.append(ConstraintWarning(
                code="LOGISTICS_LIMIT",
                message="S√©ances √† domicile d√©tect√©es : Les exercices n√©cessitant une barre olympique (ex: Deadlift) seront adapt√©s.",
                affected_days=home_days
            ))

        return warnings-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/validators/athlete_profile_validators.py
================================================================================
"""
Validateurs pour les profils athl√®tes
"""
import re
from datetime import datetime
from typing import Dict, Any, Optional

# Sport ‚Üî Position coh√©rence
VALID_SPORT_POSITIONS = {
    'Rugby': ['Pilier', 'Talonneur', '2√®me ligne', '3√®me ligne', 'Demi', 'Centre', 'Ailier', 'Arri√®re'],
    'Football': ['Gardien', 'D√©fenseur', 'Milieu', 'Attaquant'],
    'Basketball': ['Meneur', 'Arri√®re', 'Ailier', 'Ailier fort', 'Pivot'],
    'Natation': ['Nage libre', 'Dos', 'Brasse', 'Papillon', '4 nages'],
    'Athl√©tisme': ['Sprint', 'Demi-fond', 'Fond', 'Haies', 'Saut', 'Lancer'],
    'Musculation': ['Powerlifting', 'Weightlifting', 'Bodybuilding', 'CrossFit', 'G√©n√©ral'],
    'Cyclisme': ['Route', 'Piste', 'VTT', 'Cyclocross'],
    'Triathlon': ['Sprint', 'Olympique', 'Half-Ironman', 'Ironman'],
    'Escalade': ['Bloc', 'Difficult√©', 'Vitesse'],
    'Arts martiaux': ['Judo', 'BJJ', 'Boxe', 'Muay Thai', 'MMA']
}

# Note : VALID_COMPETITION_LEVELS a √©t√© supprim√© car le niveau est d√©sormais calcul√© par le backend.

def validate_athlete_profile(profile_data: Dict[str, Any]) -> bool:
    """
    Valide la coh√©rence globale d'un profil athl√®te
    """
    errors = []
    
    # Valider le contexte sportif
    if 'sport_context' in profile_data:
        errors.extend(validate_sport_context(profile_data['sport_context']))
    
    # Valider les m√©triques physiques
    if 'physical_metrics' in profile_data:
        errors.extend(validate_physical_metrics(profile_data['physical_metrics']))
    
    # Valider les objectifs
    if 'goals' in profile_data:
        errors.extend(validate_goals(profile_data['goals']))
    
    # Valider les informations de base
    if 'basic_info' in profile_data:
        errors.extend(validate_basic_info(profile_data['basic_info']))

    # Valider les performances (y compris les nouvelles m√©triques)
    if 'performance_baseline' in profile_data:
        errors.extend(validate_performance_baseline(profile_data['performance_baseline']))
    
    if errors:
        raise ValueError(" | ".join(errors))
    
    return True

def validate_performance_baseline(perf_data: Dict[str, Any]) -> list:
    """Valide les donn√©es de performance, maintenant en entiers (secondes/watts)."""
    errors = []
    
    # 1. Validation Running (Secondes)
    running_limits = {
        'running_time_5k': (700, 7200),       # ~11min √† 2h
        'running_time_10k': (1500, 14400),    # ~25min √† 4h
        'running_time_21k': (3400, 28800),    # ~56min √† 8h
        'running_max_sprint_time': (5, 60)    # Sprint court
    }

    for field, (min_s, max_s) in running_limits.items():
        val = perf_data.get(field)
        if val is not None:
            if not isinstance(val, int):
                errors.append(f"{field} format invalide (doit √™tre un entier ou HH:MM:SS valide).")
            elif val < min_s or val > max_s:
                 errors.append(f"{field} valeur {val}s hors normes physiologiques ({min_s}s - {max_s}s).")

    # 2. Validation Swimming (Secondes)
    swim_limits = {
        'swimming_time_200m': (90, 1800),    # ~1:30 √† 30min
        'swimming_time_400m': (200, 3600),   # ~3:20 √† 1h
    }

    for field, (min_s, max_s) in swim_limits.items():
        val = perf_data.get(field)
        if val is not None:
            if not isinstance(val, int):
                errors.append(f"{field} format invalide (doit √™tre un entier ou MM:SS valide).")
            elif val < min_s or val > max_s:
                 errors.append(f"{field} valeur {val}s hors normes physiologiques ({min_s}s - {max_s}s).")

    # 3. Validation Cycling (Watts)
    for field in ['cycling_max_power_15s', 'cycling_max_power_3min', 'cycling_max_power_20min', 'cycling_ftp']:
        val = perf_data.get(field)
        if val is not None:
            if not isinstance(val, int) or val < 0 or val > 3000:
                 errors.append(f"{field} doit √™tre un entier positif r√©aliste (Watts)")

    return errors

def validate_sport_context(sport_context: Dict[str, Any]) -> list:
    """Valide le contexte sportif"""
    errors = []
    
    primary_sport = sport_context.get('primary_sport')
    if not primary_sport:
        errors.append("Le sport principal est requis")
    
    playing_position = sport_context.get('playing_position')
    if playing_position and primary_sport in VALID_SPORT_POSITIONS:
        if playing_position not in VALID_SPORT_POSITIONS[primary_sport]:
            errors.append(f"Position '{playing_position}' invalide pour le sport '{primary_sport}'")
    
    training_history = sport_context.get('training_history_years')
    if training_history and (training_history < 0 or training_history > 50):
        errors.append(f"Ann√©es d'entra√Ænement invalides: {training_history}")
    
    # [MODIF V2] Suppression de la validation de 'competition_level' car le champ a √©t√© retir√©.
    # [MODIF V2] Validation de l'√©quipement (optionnel, assur√© par Pydantic Enum, mais on peut ajouter des r√®gles m√©tier ici si besoin)
    
    return errors

def validate_physical_metrics(metrics: Dict[str, Any]) -> list:
    """Valide les m√©triques physiques"""
    errors = []
    
    # Validation BMI
    if metrics.get('weight') and metrics.get('height'):
        weight = float(metrics['weight'])
        height = float(metrics['height']) / 100  # Convertir en m√®tres
        
        if height <= 0:
            errors.append("La taille doit √™tre positive")
        elif weight <= 0:
            errors.append("Le poids doit √™tre positif")
        else:
            bmi = weight / (height ** 2)
            if not (16 <= bmi <= 40):
                errors.append(f"BMI {bmi:.1f} hors des limites plausibles (16-40)")
    
    # Validation fr√©quence cardiaque
    resting_hr = metrics.get('resting_heart_rate')
    if resting_hr:
        hr = float(resting_hr)
        if not (30 <= hr <= 120):
            errors.append(f"Fr√©quence cardiaque au repos {hr} hors limites (30-120 bpm)")
    
    # Validation pourcentage de graisse
    body_fat = metrics.get('body_fat_estimate')
    if body_fat:
        bf = float(body_fat)
        if not (5 <= bf <= 50):
            errors.append(f"Pourcentage de graisse {bf}% hors limites (5-50%)")
    
    # Validation qualit√© de sommeil
    sleep_quality = metrics.get('sleep_quality_average')
    if sleep_quality:
        sq = float(sleep_quality)
        if not (1 <= sq <= 10):
            errors.append(f"Qualit√© de sommeil {sq} hors √©chelle (1-10)")
    
    return errors

def validate_goals(goals: Dict[str, Any]) -> list:
    """Valide les objectifs"""
    errors = []
    
    primary_goal = goals.get('primary_goal')
    if not primary_goal:
        errors.append("L'objectif principal est requis")
    
    target_date = goals.get('target_date')
    if target_date:
        try:
            target = datetime.strptime(target_date, '%Y-%m-%d')
            if target < datetime.now():
                errors.append("La date cible ne peut pas √™tre dans le pass√©")
        except ValueError:
            errors.append(f"Format de date invalide: {target_date}")
    
    # Valider les m√©triques cibles
    target_metrics = goals.get('target_metrics', {})
    for metric, value in target_metrics.items():
        if isinstance(value, (int, float)) and value <= 0:
            errors.append(f"M√©trique cible '{metric}' doit √™tre positive")
    
    return errors

def validate_basic_info(basic_info: Dict[str, Any]) -> list:
    """Valide les informations de base"""
    errors = []
    
    # Validation email
    email = basic_info.get('email')
    if email and not re.match(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$', email):
        errors.append("Format d'email invalide")
    
    # Validation date de naissance
    birth_date = basic_info.get('birth_date')
    if birth_date:
        try:
            birth = datetime.strptime(birth_date, '%Y-%m-%d')
            if birth > datetime.now():
                errors.append("La date de naissance ne peut pas √™tre dans le futur")
            
            # Calculer l'√¢ge
            age = datetime.now().year - birth.year
            if not (10 <= age <= 100):
                errors.append(f"√Çge {age} hors limites plausibles (10-100 ans)")
        except ValueError:
            errors.append(f"Format de date de naissance invalide: {birth_date}")
    
    # Validation √¢ge biologique
    bio_age = basic_info.get('biological_age')
    if bio_age and (bio_age < 10 or bio_age > 100):
        errors.append(f"√Çge biologique {bio_age} hors limites (10-100)")
    
    # Validation √¢ge d'entra√Ænement
    training_age = basic_info.get('training_age')
    if training_age and (training_age < 0 or training_age > 50):
        errors.append(f"√Çge d'entra√Ænement {training_age} hors limites (0-50)")
    
    # Validation sexe biologique
    biological_sex = basic_info.get('biological_sex')
    if biological_sex and biological_sex not in ['Homme', 'Femme', 'Autre']:
        errors.append(f"Sexe biologique invalide: {biological_sex}")
    
    # Validation main dominante
    dominant_hand = basic_info.get('dominant_hand')
    if dominant_hand and dominant_hand not in ['Droitier', 'Gaucher', 'Ambidextre']:
        errors.append(f"Main dominante invalide: {dominant_hand}")
    
    return errors

def validate_sport_position(sport: str, position: Optional[str]) -> bool:
    """Valide la coh√©rence sport/position"""
    if not position:
        return True
    
    if sport in VALID_SPORT_POSITIONS:
        return position in VALID_SPORT_POSITIONS[sport]
    
    return True

def validate_training_preferences(preferences: Dict[str, Any]) -> list:
    """Valide les pr√©f√©rences d'entra√Ænement"""
    errors = []
    
    max_duration = preferences.get('max_session_duration')
    if max_duration and (max_duration < 15 or max_duration > 240):
        errors.append(f"Dur√©e maximale de session {max_duration} hors limites (15-240 min)")
    
    feedback_style = preferences.get('feedback_style')
    if feedback_style and feedback_style not in ['Direct', 'Encourageant', 'Technique', 'Mixte']:
        errors.append(f"Style de feedback invalide: {feedback_style}")
    
    autonomy_preference = preferences.get('autonomy_preference')
    if autonomy_preference and autonomy_preference not in ['Faible', 'Moyenne', 'Forte']:
        errors.append(f"Pr√©f√©rence d'autonomie invalide: {autonomy_preference}")
    
    return errors

def validate_injury_prevention(injury_data: Dict[str, Any]) -> list:
    """Valide les donn√©es de pr√©vention des blessures"""
    errors = []
    
    medical_clearance = injury_data.get('medical_clearance')
    if medical_clearance is False:
        errors.append("Avis m√©dical requis pour l'entra√Ænement")
    
    return errors-e 

-e 
================================================================================
üìÑ FICHIER : backend/backend/app/models/core_models.py
================================================================================
from sqlalchemy import Column, Integer, String, Float, Date, ForeignKey, DateTime, Text, Boolean, JSON
from sqlalchemy.orm import relationship
from sqlalchemy.sql import func
from app.core.database import Base

class User(Base):
    __tablename__ = "users"
    id = Column(Integer, primary_key=True, index=True)
    username = Column(String, unique=True, index=True)
    email = Column(String, unique=True, index=True, nullable=True)
    hashed_password = Column(String)
    profile_data = Column(Text, nullable=True)
    strategy_data = Column(Text, nullable=True)
    weekly_plan_data = Column(Text, nullable=True)
    draft_workout_data = Column(Text, nullable=True)
    workouts = relationship("WorkoutSession", back_populates="owner")
    feed_items = relationship("FeedItem", back_populates="owner", cascade="all, delete-orphan")
    athlete_profile = relationship("AthleteProfile", back_populates="user", uselist=False, cascade="all, delete-orphan")

class AthleteProfile(Base):
    __tablename__ = "athlete_profiles"
    id = Column(Integer, primary_key=True, index=True)
    user_id = Column(Integer, ForeignKey("users.id"), unique=True)
    basic_info = Column(JSON, default={})
    physical_metrics = Column(JSON, default={})
    sport_context = Column(JSON, default={})
    performance_baseline = Column(JSON, default={})
    injury_prevention = Column(JSON, default={})
    training_preferences = Column(JSON, default={})
    goals = Column(JSON, default={})
    constraints = Column(JSON, default={})
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), onupdate=func.now())
    user = relationship("User", back_populates="athlete_profile")
    coach_memory = relationship("CoachMemory", back_populates="athlete_profile", uselist=False, cascade="all, delete-orphan")

class CoachMemory(Base):
    __tablename__ = "coach_memories"
    id = Column(Integer, primary_key=True, index=True)
    athlete_profile_id = Column(Integer, ForeignKey("athlete_profiles.id"), unique=True)
    metadata_info = Column(JSON, default={})
    current_context = Column(JSON, default={})
    response_patterns = Column(JSON, default={})
    performance_baselines = Column(JSON, default={})
    adaptation_signals = Column(JSON, default={})
    sport_specific_insights = Column(JSON, default={})
    training_history_summary = Column(JSON, default={})
    athlete_preferences = Column(JSON, default={})
    coach_notes = Column(JSON, default={})
    memory_flags = Column(JSON, default={})
    last_updated = Column(DateTime(timezone=True), server_default=func.now())
    athlete_profile = relationship("AthleteProfile", back_populates="coach_memory")

class WorkoutSession(Base):
    __tablename__ = "workout_sessions"
    id = Column(Integer, primary_key=True, index=True)
    user_id = Column(Integer, ForeignKey("users.id"))
    date = Column(Date, index=True)
    duration = Column(Float)
    rpe = Column(Float)
    energy_level = Column(Integer, default=5) 
    notes = Column(Text, nullable=True)      
    ai_analysis = Column(Text, nullable=True)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    owner = relationship("User", back_populates="workouts")
    sets = relationship("WorkoutSet", back_populates="session", cascade="all, delete-orphan")

class WorkoutSet(Base):
    __tablename__ = "workout_sets"
    id = Column(Integer, primary_key=True, index=True)
    session_id = Column(Integer, ForeignKey("workout_sessions.id"))
    exercise_name = Column(String, index=True)
    set_order = Column(Integer)
    weight = Column(Float, default=0.0)
    reps = Column(Float, default=0.0)
    rpe = Column(Float, default=0.0)
    rest_seconds = Column(Integer, default=0)
    metric_type = Column(String, nullable=False, default="LOAD_REPS") 
    session = relationship("WorkoutSession", back_populates="sets")

class FeedItem(Base):
    __tablename__ = "feed_items"
    id = Column(String, primary_key=True, index=True)
    user_id = Column(Integer, ForeignKey("users.id"))
    type = Column(String, index=True)
    title = Column(String)
    message = Column(String)
    action_payload = Column(Text, nullable=True)
    is_read = Column(Boolean, default=False)
    is_completed = Column(Boolean, default=False)
    priority = Column(Integer, default=1)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    owner = relationship("User", back_populates="feed_items")
-e 

-e 
================================================================================
üìÑ FICHIER : backend/backend/app/models/schemas.py
================================================================================
from pydantic import BaseModel, Field, field_validator, ConfigDict
from typing import List, Optional, Dict, Any, Union
from datetime import date, datetime
import json

# --- SUB-SCHEMAS ---

class BasicInfo(BaseModel):
    pseudo: Optional[str] = None
    email: Optional[str] = None
    birth_date: Optional[str] = None
    biological_sex: Optional[str] = "Homme"
    training_age: Optional[int] = 0

class PhysicalMetrics(BaseModel):
    height: Optional[float] = 0
    weight: Optional[float] = 0
    body_fat: Optional[float] = None
    resting_hr: Optional[int] = None
    sleep_quality_avg: Optional[int] = 5

class SportContext(BaseModel):
    # [FIX 422] On accepte tout ce qui vient du Frontend
    sport: Optional[str] = "Autre"
    position: Optional[str] = None
    level: Optional[str] = "Interm√©diaire"
    equipment: Optional[List[str]] = ["Standard"]

class TrainingPreferences(BaseModel):
    days_available: List[str] = []
    duration_min: int = 60
    preferred_split: str = "Upper/Lower"

# --- MAIN PROFILE ---

class AthleteProfileBase(BaseModel):
    basic_info: BasicInfo = Field(default_factory=BasicInfo)
    physical_metrics: PhysicalMetrics = Field(default_factory=PhysicalMetrics)
    sport_context: SportContext = Field(default_factory=SportContext)
    training_preferences: TrainingPreferences = Field(default_factory=TrainingPreferences)
    
    # Dictionnaires libres pour le reste
    goals: Dict[str, Any] = {}
    constraints: Dict[str, Any] = {}
    injury_prevention: Dict[str, Any] = {}
    performance_baseline: Dict[str, Any] = {}

class AthleteProfileCreate(AthleteProfileBase):
    pass

class AthleteProfileResponse(AthleteProfileBase):
    id: int
    user_id: int
    created_at: Optional[datetime] = None
    class Config:
        from_attributes = True

# --- OTHER SCHEMAS (REQUIRED FOR BUILD) ---

class CoachMemoryResponse(BaseModel):
    id: int
    readiness_score: int = Field(alias="current_context", default=50)
    current_phase: str = "G√©n√©ral"
    flags: Dict[str, bool] = {}
    insights: Dict[str, Any] = {}
    @field_validator('readiness_score', mode='before')
    def extract_readiness(cls, v):
        if isinstance(v, dict): return v.get('readiness_score', 50)
        return v
    class Config:
        from_attributes = True

class WorkoutSetBase(BaseModel):
    exercise_name: str
    set_order: int
    weight: Union[float, str] = 0.0
    reps: Union[float, str] = 0.0
    rpe: Optional[float] = 0.0
    rest_seconds: int = 0
    metric_type: str = "LOAD_REPS"
    @field_validator('weight', 'reps', mode='before')
    def parse_polymorphic_fields(cls, v):
        if isinstance(v, str):
            try: return float(v.replace(',', '.'))
            except: return 0.0
        return v

class WorkoutSetCreate(WorkoutSetBase): pass
class WorkoutSetResponse(WorkoutSetBase):
    id: int
    weight: float
    reps: float
    class Config: from_attributes = True

class WorkoutSessionCreate(BaseModel):
    date: date
    duration: float
    rpe: float
    energy_level: int = 5
    notes: Optional[str] = None
    sets: List[WorkoutSetCreate] = []
    ai_analysis: Optional[str] = None

class WorkoutSessionResponse(WorkoutSessionCreate):
    id: int
    ai_analysis: Optional[str] = None
    sets: List[WorkoutSetResponse] = []
    class Config: from_attributes = True

class GenerateWorkoutRequest(BaseModel):
    profile_data: Dict[str, Any]
    context: Dict[str, Any]

class AIExercise(BaseModel):
    name: str
    sets: int
    reps: Union[str, int]
    rest: int
    tips: str
    recording_mode: str = "LOAD_REPS"
    @field_validator('reps')
    def force_string_reps(cls, v): return str(v)

class AIWorkoutPlan(BaseModel):
    title: str
    coach_comment: str
    warmup: List[str]
    exercises: List[AIExercise]
    cooldown: List[str]

class UserCreate(BaseModel):
    username: str
    email: Optional[str] = None
    password: str

class UserResponse(BaseModel):
    id: int
    username: str
    email: Optional[str] = None
    profile_data: Optional[Dict[str, Any]] = None
    @field_validator('profile_data', mode='before')
    def parse_profile_data(cls, v):
        if v is None: return {}
        if isinstance(v, dict): return v
        try: return json.loads(v)
        except: return {}
    class Config: from_attributes = True

class Token(BaseModel):
    access_token: str
    token_type: str

class TokenData(BaseModel):
    username: Optional[str] = None

class FeedItemCreate(BaseModel):
    type: str
    title: str
    message: str
    priority: int = 1
    action_payload: Optional[Dict[str, Any]] = None

class FeedItemResponse(FeedItemCreate):
    id: str
    is_read: bool
    is_completed: bool
    created_at: datetime
    @field_validator('action_payload', mode='before')
    def parse_payload(cls, v):
        if isinstance(v, str) and v.strip():
            try: return json.loads(v)
            except: return None
        return v
    class Config: from_attributes = True

class OneRepMaxRequest(BaseModel):
    weight: float
    reps: int
class OneRepMaxResponse(BaseModel):
    estimated_1rm: float
    method_used: str
class ACWRRequest(BaseModel):
    history: List[Dict[str, Any]]
class ACWRResponse(BaseModel):
    ratio: float
    status: str
    color: str
    message: str
class ProfileAuditRequest(BaseModel):
    profile_data: Dict[str, Any]
class ProfileAuditResponse(BaseModel):
    markdown_report: str
class StrategyResponse(BaseModel):
    periodization_title: str
    phases: List[Any]
class WeeklyPlanResponse(BaseModel):
    schedule: List[Any]
    reasoning: str
class UserProfileUpdate(BaseModel):
    profile_data: Dict[str, Any]
-e 

-e 
================================================================================
üìÑ FICHIER : backend/backend/app/routers/profiles.py
================================================================================
from fastapi import APIRouter, Depends, HTTPException, status
from sqlalchemy.orm import Session
from app.core.database import get_db
from app.models import models_v2, schemas
from app.dependencies import get_current_user
from app.services.coach_logic import CoachLogic

router = APIRouter(
    prefix="/api/v1",
    tags=["Athlete Profile & Memory"]
)

@router.get("/profiles/me", response_model=schemas.AthleteProfileResponse)
async def get_my_profile(
    current_user: models_v2.User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    if not current_user.athlete_profile:
        profile = models_v2.AthleteProfile(user_id=current_user.id)
        db.add(profile)
        db.commit()
        db.refresh(profile)
        return profile
    return current_user.athlete_profile

@router.post("/profiles/complete", response_model=schemas.AthleteProfileResponse)
async def complete_profile(
    profile_data: schemas.AthleteProfileCreate,
    current_user: models_v2.User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    sport = profile_data.sport_context.sport
    pos = profile_data.sport_context.position
    # Validation basique
    if not CoachLogic.validate_sport_position(sport, pos):
        # On log mais on ne bloque pas forc√©ment en phase de dev
        print(f"Warning: Position {pos} mismatch for sport {sport}")

    db_profile = current_user.athlete_profile
    if not db_profile:
        db_profile = models_v2.AthleteProfile(user_id=current_user.id)
        db.add(db_profile)
    
    # 1. Update basic fields
    db_profile.basic_info = profile_data.basic_info.dict()
    db_profile.physical_metrics = profile_data.physical_metrics.dict()
    db_profile.sport_context = profile_data.sport_context.dict()
    db_profile.training_preferences = profile_data.training_preferences.dict()
    db_profile.goals = profile_data.goals
    db_profile.constraints = profile_data.constraints
    
    # 2. IMPORTANT : Update new fields (Labo & Sant√©)
    db_profile.performance_baseline = profile_data.performance_baseline
    db_profile.injury_prevention = profile_data.injury_prevention
    
    # 3. Initialize Memory if needed
    if not db_profile.coach_memory:
        memory = CoachLogic.initialize_memory(db_profile)
        db.add(memory)
    
    db.commit()
    db.refresh(db_profile)
    return db_profile

@router.get("/coach-memories/me", response_model=schemas.CoachMemoryResponse)
async def get_my_coach_memory(
    current_user: models_v2.User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    if not current_user.athlete_profile or not current_user.athlete_profile.coach_memory:
        raise HTTPException(status_code=404, detail="Profil introuvable.")
    return current_user.athlete_profile.coach_memory

@router.post("/coach-memories/recalculate")
async def force_recalculate(
    current_user: models_v2.User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    profile = current_user.athlete_profile
    if not profile or not profile.coach_memory:
        raise HTTPException(status_code=404, detail="Introuvable")
        
    CoachLogic.update_daily(profile.coach_memory, profile)
    db.commit()
    return {"status": "updated", "new_readiness": profile.coach_memory.current_context.get('readiness_score')}
-e 

-e 
================================================================================
üìÑ FICHIER : backend/backend/app/services/coach_logic.py
================================================================================
from datetime import date
from typing import Dict, Any
from app.models import models_v2

# Constantes de validation
VALID_SPORT_POSITIONS = {
    'Rugby': ['Pilier', 'Talonneur', '2√®me ligne', '3√®me ligne', 'Demi', 'Centre', 'Ailier', 'Arri√®re'],
    'Football': ['Gardien', 'D√©fenseur', 'Milieu', 'Attaquant'],
}

class CoachLogic:
    @staticmethod
    def validate_sport_position(sport: str, position: str) -> bool:
        if sport in VALID_SPORT_POSITIONS:
            if position and position not in VALID_SPORT_POSITIONS[sport]:
                return False
        return True

    @staticmethod
    def initialize_memory(profile: models_v2.AthleteProfile) -> models_v2.CoachMemory:
        """Cr√©e la structure initiale de la m√©moire du coach based sur le profil"""
        sport = profile.sport_context.get('sport', 'Autre')
        
        # Insights initiaux
        insights = {
            "primary_sport": sport,
            "specificity_index": "High" if sport in ['Rugby', 'Football'] else "Medium",
            "focus_areas": ["Strength", "Hypertrophy"] # D√©faut
        }
        
        # Contexte initial
        context = {
            "macrocycle_phase": "Adaptation Anatomique",
            "fatigue_state": "Fresh",
            "readiness_score": 100,
            "season_week": 1
        }
        
        # Drapeaux
        flags = {
            "needs_deload": False,
            "injury_risk": False,
            "adaptation_window_open": True
        }

        memory = models_v2.CoachMemory(
            athlete_profile_id=profile.id,
            sport_specific_insights=insights,
            current_context=context,
            memory_flags=flags,
            coach_notes={"initialization": f"Profil cr√©√© le {date.today()}"}
        )
        return memory

    @staticmethod
    def calculate_readiness(profile: models_v2.AthleteProfile) -> int:
        """Algorithme simple de readiness bas√© sur les m√©triques"""
        base_score = 80
        
        # Impact Sommeil
        sleep = profile.physical_metrics.get('sleep_quality_avg', 5)
        if sleep >= 8: base_score += 10
        elif sleep <= 4: base_score -= 20
        
        # Impact Stress
        stress = profile.constraints.get('work_stress_level', 5)
        if stress >= 8: base_score -= 15
        
        return max(0, min(100, base_score))

    @staticmethod
    def update_daily(memory: models_v2.CoachMemory, profile: models_v2.AthleteProfile):
        """Mise √† jour quotidienne (Batch Job simulation)"""
        # Recalcul Readiness
        new_readiness = CoachLogic.calculate_readiness(profile)
        
        # Update Context
        current_context = dict(memory.current_context or {})
        current_context['readiness_score'] = new_readiness
        
        if new_readiness < 40:
            current_context['fatigue_state'] = "High"
        elif new_readiness < 70:
            current_context['fatigue_state'] = "Moderate"
        else:
            current_context['fatigue_state'] = "Optimal"
            
        memory.current_context = current_context
        
        # Update Flags
        flags = dict(memory.memory_flags or {})
        flags['needs_deload'] = new_readiness < 30
        flags['adaptation_window_open'] = new_readiness > 70
        memory.memory_flags = flags
-e 

-e 
================================================================================
üìÑ FICHIER : backend/check_jwt_config.py
================================================================================
import os
import jwt
from datetime import datetime, timedelta
from dotenv import load_dotenv

load_dotenv()

SECRET_KEY = os.getenv("SECRET_KEY")
ALGORITHM = os.getenv("ALGORITHM", "HS256")
ACCESS_TOKEN_EXPIRE_MINUTES = int(os.getenv("ACCESS_TOKEN_EXPIRE_MINUTES", 30))

print("üîß Configuration JWT Actuelle:")
print(f"   SECRET_KEY: {'SET' if SECRET_KEY else 'NOT SET'}")
print(f"   ALGORITHM: {ALGORITHM}")
print(f"   ACCESS_TOKEN_EXPIRE_MINUTES: {ACCESS_TOKEN_EXPIRE_MINUTES} min")

# V√©rifier si on peut g√©n√©rer un token
if SECRET_KEY:
    print("\nüß™ Test de g√©n√©ration de token...")
    
    data = {"sub": "testuser", "exp": datetime.utcnow() + timedelta(minutes=30)}
    token = jwt.encode(data, SECRET_KEY, algorithm=ALGORITHM)
    
    print(f"   Token g√©n√©r√©: {token[:50]}...")
    
    # V√©rifier qu'on peut le d√©coder
    try:
        decoded = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        print(f"   ‚úÖ Token d√©cod√©: {decoded}")
    except Exception as e:
        print(f"   ‚ùå Erreur d√©codage: {e}")
else:
    print("\n‚ùå SECRET_KEY non d√©finie!")
    print("   D√©finissez-la dans .env:")
    print("   SECRET_KEY=votre_clef_secrete_tres_longue_ici")
-e 

-e 
================================================================================
üìÑ FICHIER : backend/exe.py
================================================================================
import sys
import os
from datetime import datetime, timedelta

# --- CONFIGURATION DU PATH ---
# Permet d'importer les modules 'app' m√™me si on lance le script depuis backend/
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)

try:
    # --- IMPORTS ---
    from app.core.database import Base, SQLALCHEMY_DATABASE_URL as DATABASE_URL
    # On importe TOUS les maillons de la cha√Æne
    from app.models.sql_models import CoachMemory, CoachEngram, User, AthleteProfile
    from app.models.enums import MemoryType, ImpactLevel, MemoryStatus

except ImportError as e:
    print(f"‚ùå Erreur d'import : {e}")
    sys.exit(1)

from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

# Configuration de la connexion DB
engine = create_engine(DATABASE_URL)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

def repair_and_seed(target_user_id: int):
    db = SessionLocal()
    try:
        print(f"üîß D√©marrage du protocole de r√©paration pour User ID: {target_user_id}...")

        # √âTAPE 1 : Validation de l'Utilisateur
        user = db.query(User).filter(User.id == target_user_id).first()
        if not user:
            print(f"‚ùå Erreur Fatale : L'utilisateur {target_user_id} n'existe pas dans la table 'users'.")
            return

        print(f"  - ‚úÖ Utilisateur {target_user_id} trouv√©.")

        # √âTAPE 2 : Validation ou Cr√©ation du Profil Athl√©tique
        profile = user.athlete_profile # Via la relation SQLAlchemy
        
        if not profile:
            print(f"  - ‚ö†Ô∏è Aucun profil athl√©tique trouv√© pour cet user.")
            print(f"  - üõ†Ô∏è CR√âATION D'UN PROFIL D'URGENCE...")
            
            # Cr√©ation d'un profil minimal pour satisfaire la Foreign Key
            profile = AthleteProfile(
                user_id=user.id,
                basic_info={"pseudo": user.username, "generated": True},
                physical_metrics={"weight": 80, "height": 180},
                performance_baseline={}
            )
            db.add(profile)
            db.commit()
            db.refresh(profile)
            print(f"  - ‚úÖ Profil cr√©√© avec succ√®s ! Nouvel ID du profil : {profile.id}")
        else:
            print(f"  - ‚úÖ Profil athl√©tique existant trouv√© (ID: {profile.id}).")

        # C'est cet ID qu'on doit utiliser pour lier la m√©moire
        real_profile_id = profile.id

        # √âTAPE 3 : Validation ou Cr√©ation de la M√©moire
        memory = db.query(CoachMemory).filter(CoachMemory.athlete_profile_id == real_profile_id).first()
        
        if not memory:
            print(f"  - Cr√©ation du conteneur CoachMemory pour le Profil {real_profile_id}...")
            memory = CoachMemory(
                athlete_profile_id=real_profile_id,
                coach_notes={"source": "repair_script"},
                memory_flags={},
                current_context={"readiness_score": 85}
            )
            db.add(memory)
            db.commit()
            db.refresh(memory)
        else:
            print(f"  - Conteneur CoachMemory trouv√© (ID: {memory.id})")

        # √âTAPE 4 : Injection des Engrammes (Souvenirs)
        # Note : On utilise 'ACTIVE' m√™me pour les √©v√©nements futurs (SCHEDULED n'existe pas dans l'Enum SQL)
        engrams_data = [
            {
                "type": MemoryType.INJURY_REPORT,
                "impact": ImpactLevel.SEVERE,
                "status": MemoryStatus.ACTIVE,
                "content": "Douleur patellaire gauche (4/10). Stop Squat profond.",
                "tags": ["knee", "squat"],
                "start_date": datetime.now() - timedelta(days=2),
                "end_date": None
            },
            {
                "type": MemoryType.LIFE_CONSTRAINT,
                "impact": ImpactLevel.MODERATE,
                # [CORRECTIF] Utilisation de ACTIVE car SCHEDULED n'est pas dans l'Enum
                "status": MemoryStatus.ACTIVE, 
                "content": "D√©placement Londres. Mat√©riel limit√©.",
                "tags": ["travel"],
                "start_date": datetime.now() + timedelta(days=5),
                "end_date": datetime.now() + timedelta(days=10)
            },
            {
                "type": MemoryType.STRATEGIC_OVERRIDE,
                "impact": ImpactLevel.MODERATE,
                "status": MemoryStatus.ACTIVE,
                "content": "Focus Hypertrophie Dos.",
                "tags": ["back", "hypertrophy"],
                "start_date": datetime.now() - timedelta(days=1),
                "end_date": datetime.now() + timedelta(days=30)
            }
        ]

        count = 0
        for data in engrams_data:
            # On v√©rifie si un engramme identique existe d√©j√† pour ne pas dupliquer
            exists = db.query(CoachEngram).filter(
                CoachEngram.memory_id == memory.id,
                CoachEngram.content == data["content"]
            ).first()
            
            if not exists:
                engram = CoachEngram(
                    memory_id=memory.id,
                    author="REPAIR_SCRIPT",
                    **data
                )
                db.add(engram)
                count += 1
        
        db.commit()
        print(f"‚úÖ SUCC√àS TOTAL : {count} engrammes inject√©s pour l'Utilisateur {target_user_id} (Profil {real_profile_id}).")

    except Exception as e:
        print(f"‚ùå Erreur Critique : {e}")
        import traceback
        traceback.print_exc()
        db.rollback()
    finally:
        db.close()

if __name__ == "__main__":
    # Remplacez 17 par l'ID de votre utilisateur si n√©cessaire,
    # ou laissez 17 si c'est celui que vous utilisez pour tester.
    repair_and_seed(target_user_id=1)-e 

-e 
================================================================================
üìÑ FICHIER : backend/generate_map.py
================================================================================
import sys
import os
import json
import inspect
import re

# --- CONFIGURATION DES CHEMINS ---
# On se place dans backend/tools/, on veut remonter √† la racine du projet
BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '../../'))
BACKEND_DIR = os.path.join(BASE_DIR, 'backend')
PUBSPEC_PATH = os.path.join(BASE_DIR, 'pubspec.yaml') # Hypoth√®se: pubspec √† la racine ou dans frontend/
REQUIREMENTS_PATH = os.path.join(BACKEND_DIR, 'requirements.txt')
MAP_FILE = os.path.join(BASE_DIR, 'TITANFLOW_MAP.json')

# Ajout du backend au path pour les imports SQLAlchemy
sys.path.append(BACKEND_DIR)

try:
    from app.models import sql_models
    from app.core.database import Base
except ImportError as e:
    print(f"‚ùå Erreur d'import Backend : {e}")
    print("Assurez-vous d'√™tre dans l'environnement virtuel (venv).")
    sys.exit(1)

def get_flutter_environment():
    """Lit le pubspec.yaml pour extraire les versions."""
    env = {"sdk": "Unknown", "packages": {}}
    
    if not os.path.exists(PUBSPEC_PATH):
        print(f"‚ö†Ô∏è  pubspec.yaml introuvable ici : {PUBSPEC_PATH}")
        return env

    with open(PUBSPEC_PATH, 'r', encoding='utf-8') as f:
        lines = f.readlines()
        
    in_dependencies = False
    for line in lines:
        line = line.strip()
        # SDK Version
        if line.startswith('sdk:'):
            env['sdk'] = line.split(':')[1].strip()
        
        # D√©tection bloc dependencies
        if line == 'dependencies:':
            in_dependencies = True
            continue
        if line == 'dev_dependencies:':
            in_dependencies = False
            continue
            
        # Extraction packages cl√©s (http, provider, etc.)
        if in_dependencies and ':' in line:
            parts = line.split(':')
            pkg_name = parts[0].strip()
            version = parts[1].strip()
            # On garde seulement les packages int√©ressants pour l'IA
            interesting_libs = ['http', 'shared_preferences', 'intl', 'flutter', 'provider', 'riverpod', 'bloc', 'go_router']
            if pkg_name in interesting_libs:
                env['packages'][pkg_name] = version
                
    return env

def get_backend_environment():
    """Lit le requirements.txt pour les versions Python."""
    env = {"python": sys.version.split()[0], "libs": {}}
    
    if not os.path.exists(REQUIREMENTS_PATH):
        return env

    with open(REQUIREMENTS_PATH, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if '==' in line:
                parts = line.split('==')
                env['libs'][parts[0]] = parts[1]
            elif '>=' in line:
                parts = line.split('>=')
                env['libs'][parts[0]] = f">={parts[1]}"
                
    return env

def generate_db_schema():
    """Scan les mod√®les SQLAlchemy."""
    schema = {}
    for name, obj in inspect.getmembers(sql_models):
        if inspect.isclass(obj) and issubclass(obj, Base) and obj != Base:
            table_name = obj.__tablename__
            columns = []
            
            for col in obj.__table__.columns:
                col_type = str(col.type)
                info = f"{col.name} ({col_type})"
                if col.primary_key: info += " [PK]"
                if col.foreign_keys: info += " [FK]"
                columns.append(info)
            
            schema[table_name] = {
                "description": f"Table SQL {table_name}",
                "columns": columns
            }
    return schema

def main():
    print("üöÄ D√©marrage de la cartographie TitanFlow...")
    
    # 1. Chargement existant
    data = {}
    if os.path.exists(MAP_FILE):
        try:
            with open(MAP_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
        except json.JSONDecodeError:
            print("‚ö†Ô∏è  Fichier JSON corrompu, cr√©ation d'un nouveau.")

    # 2. Mise √† jour Environment (Versions)
    print("üì¶ Analyse des d√©pendances (Flutter & Python)...")
    data["environment"] = {
        "frontend": get_flutter_environment(),
        "backend": get_backend_environment()
    }

    # 3. Mise √† jour DB Schema
    print("üóÑÔ∏è  Introspection de la Base de Donn√©es...")
    data["database_schema"] = generate_db_schema()

    # 4. Sauvegarde
    with open(MAP_FILE, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    
    print(f"‚úÖ Succ√®s ! Fichier mis √† jour : {MAP_FILE}")
    print("   -> L'IA aura maintenant une vision exacte de tes versions et de ta BDD.")

if __name__ == "__main__":
    main()-e 

-e 
================================================================================
üìÑ FICHIER : backend/implement.py
================================================================================
import os
import sys
import logging
from sqlalchemy import create_engine, text, inspect
from dotenv import load_dotenv
import json

# Configuration
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger("TitanUpgrader")

# --- CHEMIN DYNAMIQUE ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# 1. NOUVEAUX MOD√àLES SQL
SQL_MODELS_CONTENT = """from sqlalchemy import Column, Integer, String, Float, Date, ForeignKey, DateTime, Text, Boolean, JSON
from sqlalchemy.orm import relationship
from sqlalchemy.sql import func
from app.core.database import Base

class User(Base):
    __tablename__ = "users"

    id = Column(Integer, primary_key=True, index=True)
    username = Column(String, unique=True, index=True)
    email = Column(String, unique=True, index=True, nullable=True)
    hashed_password = Column(String)
    
    # Anciens champs (Legacy support)
    profile_data = Column(Text, nullable=True)
    strategy_data = Column(Text, nullable=True)
    weekly_plan_data = Column(Text, nullable=True)
    draft_workout_data = Column(Text, nullable=True)

    # Relations
    workouts = relationship("WorkoutSession", back_populates="owner")
    feed_items = relationship("FeedItem", back_populates="owner", cascade="all, delete-orphan")
    
    # [NOUVEAU] Relation vers le Profil Enrichi
    athlete_profile = relationship("AthleteProfile", back_populates="user", uselist=False, cascade="all, delete-orphan")

class AthleteProfile(Base):
    __tablename__ = "athlete_profiles"

    id = Column(Integer, primary_key=True, index=True)
    user_id = Column(Integer, ForeignKey("users.id"), unique=True)

    # Blocs de donn√©es JSONB
    basic_info = Column(JSON, default={})
    physical_metrics = Column(JSON, default={})
    sport_context = Column(JSON, default={})
    performance_baseline = Column(JSON, default={})
    injury_prevention = Column(JSON, default={})
    training_preferences = Column(JSON, default={})
    goals = Column(JSON, default={})
    constraints = Column(JSON, default={})

    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), onupdate=func.now())

    user = relationship("User", back_populates="athlete_profile")
    coach_memory = relationship("CoachMemory", back_populates="athlete_profile", uselist=False, cascade="all, delete-orphan")

class CoachMemory(Base):
    __tablename__ = "coach_memories"

    id = Column(Integer, primary_key=True, index=True)
    athlete_profile_id = Column(Integer, ForeignKey("athlete_profiles.id"), unique=True)

    # M√©moire contextuelle IA
    metadata_info = Column(JSON, default={})
    current_context = Column(JSON, default={})
    response_patterns = Column(JSON, default={})
    performance_baselines = Column(JSON, default={})
    adaptation_signals = Column(JSON, default={})
    sport_specific_insights = Column(JSON, default={})
    training_history_summary = Column(JSON, default={})
    athlete_preferences = Column(JSON, default={})
    coach_notes = Column(JSON, default={})
    memory_flags = Column(JSON, default={})

    last_updated = Column(DateTime(timezone=True), server_default=func.now())

    athlete_profile = relationship("AthleteProfile", back_populates="coach_memory")

# --- MOD√àLES EXISTANTS ---
class WorkoutSession(Base):
    __tablename__ = "workout_sessions"
    id = Column(Integer, primary_key=True, index=True)
    user_id = Column(Integer, ForeignKey("users.id"))
    date = Column(Date, index=True)
    duration = Column(Float)
    rpe = Column(Float)
    energy_level = Column(Integer, default=5) 
    notes = Column(Text, nullable=True)      
    ai_analysis = Column(Text, nullable=True)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    owner = relationship("User", back_populates="workouts")
    sets = relationship("WorkoutSet", back_populates="session", cascade="all, delete-orphan")

class WorkoutSet(Base):
    __tablename__ = "workout_sets"
    id = Column(Integer, primary_key=True, index=True)
    session_id = Column(Integer, ForeignKey("workout_sessions.id"))
    exercise_name = Column(String, index=True)
    set_order = Column(Integer)
    weight = Column(Float, default=0.0)
    reps = Column(Float, default=0.0)
    rpe = Column(Float, default=0.0)
    rest_seconds = Column(Integer, default=0)
    metric_type = Column(String, nullable=False, default="LOAD_REPS") 
    session = relationship("WorkoutSession", back_populates="sets")

class FeedItem(Base):
    __tablename__ = "feed_items"
    id = Column(String, primary_key=True, index=True)
    user_id = Column(Integer, ForeignKey("users.id"))
    type = Column(String, index=True)
    title = Column(String)
    message = Column(String)
    action_payload = Column(Text, nullable=True)
    is_read = Column(Boolean, default=False)
    is_completed = Column(Boolean, default=False)
    priority = Column(Integer, default=1)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    owner = relationship("User", back_populates="feed_items")
"""

# 2. SCHEMAS PYDANTIC
SCHEMAS_CONTENT = """from pydantic import BaseModel, Field, field_validator
from typing import List, Optional, Dict, Any, Union
from datetime import date, datetime
from enum import Enum
import json

# --- ENUMS & TYPES ---
class SportType(str, Enum):
    RUGBY = "Rugby"
    FOOTBALL = "Football"
    CROSSFIT = "CrossFit"
    HYBRID = "Hybrid"
    RUNNING = "Running"
    OTHER = "Autre"

# --- SUB-SCHEMAS FOR PROFILE ---
class BasicInfo(BaseModel):
    pseudo: Optional[str] = None
    email: Optional[str] = None
    birth_date: Optional[str] = None
    training_age: Optional[int] = 0

class PhysicalMetrics(BaseModel):
    height: float = 0
    weight: float = 0
    body_fat: Optional[float] = None
    resting_hr: Optional[int] = None
    sleep_quality_avg: Optional[int] = 5

class SportContext(BaseModel):
    sport: SportType = SportType.OTHER
    position: Optional[str] = None
    level: str = "Interm√©diaire"
    equipment: List[str] = ["Standard"]

class TrainingPreferences(BaseModel):
    days_available: List[str] = []
    duration_min: int = 60
    preferred_split: str = "Upper/Lower"

# --- MAIN PROFILE SCHEMAS ---
class AthleteProfileBase(BaseModel):
    basic_info: BasicInfo = Field(default_factory=BasicInfo)
    physical_metrics: PhysicalMetrics = Field(default_factory=PhysicalMetrics)
    sport_context: SportContext = Field(default_factory=SportContext)
    training_preferences: TrainingPreferences = Field(default_factory=TrainingPreferences)
    goals: Dict[str, Any] = {}
    constraints: Dict[str, Any] = {}
    injury_prevention: Dict[str, Any] = {}
    performance_baseline: Dict[str, Any] = {}

class AthleteProfileCreate(AthleteProfileBase):
    pass

class AthleteProfileResponse(AthleteProfileBase):
    id: int
    user_id: int
    created_at: datetime
    class Config:
        from_attributes = True

# --- MEMORY SCHEMAS ---
class CoachMemoryResponse(BaseModel):
    id: int
    readiness_score: int = Field(alias="current_context", default={}).get("readiness_score", 0)
    current_phase: str = "G√©n√©ral"
    flags: Dict[str, bool] = {}
    insights: Dict[str, Any] = {}
    
    @field_validator('readiness_score', mode='before')
    def extract_readiness(cls, v):
        if isinstance(v, dict): return v.get('readiness_score', 50)
        return v

    class Config:
        from_attributes = True

# --- LEGACY SCHEMAS ---
class WorkoutSetBase(BaseModel):
    exercise_name: str
    set_order: int
    weight: Union[float, str] = 0.0
    reps: Union[float, str] = 0.0
    rpe: Optional[float] = 0.0
    rest_seconds: int = 0
    metric_type: str = "LOAD_REPS"

    @field_validator('weight', 'reps', mode='before')
    def parse_polymorphic_fields(cls, v):
        if isinstance(v, str):
            v = v.strip().replace(',', '.')
            if ':' in v:
                parts = v.split(':')
                try:
                    seconds = 0.0
                    if len(parts) == 2:
                        seconds = float(parts[0]) * 60 + float(parts[1])
                    elif len(parts) == 3:
                        seconds = float(parts[0]) * 3600 + float(parts[1]) * 60 + float(parts[2])
                    return seconds
                except ValueError:
                    return 0.0
            try:
                return float(v)
            except ValueError:
                return 0.0
        return v

class WorkoutSetCreate(WorkoutSetBase):
    pass

class WorkoutSessionCreate(BaseModel):
    date: date
    duration: float
    rpe: float
    energy_level: int = 5
    notes: Optional[str] = None
    sets: List[WorkoutSetCreate] = []

class WorkoutSetResponse(WorkoutSetBase):
    id: int
    weight: float
    reps: float
    class Config:
        from_attributes = True

class WorkoutSessionResponse(WorkoutSessionCreate):
    id: int
    ai_analysis: Optional[str] = None
    sets: List[WorkoutSetResponse] = []
    class Config:
        from_attributes = True

class GenerateWorkoutRequest(BaseModel):
    profile_data: Dict[str, Any]
    context: Dict[str, Any]

class AIExercise(BaseModel):
    name: str
    sets: int
    reps: Union[str, int]
    rest: int
    tips: str
    recording_mode: str = "LOAD_REPS"
    @field_validator('reps')
    def force_string_reps(cls, v):
        return str(v)

class AIWorkoutPlan(BaseModel):
    title: str
    coach_comment: str
    warmup: List[str]
    exercises: List[AIExercise]
    cooldown: List[str]

class UserCreate(BaseModel):
    username: str
    email: Optional[str] = None
    password: str

class UserResponse(BaseModel):
    id: int
    username: str
    email: Optional[str] = None
    profile_data: Optional[str] = None 
    class Config:
        from_attributes = True

class Token(BaseModel):
    access_token: str
    token_type: str

class TokenData(BaseModel):
    username: Optional[str] = None

class FeedItemType(str, Enum):
    INFO = "INFO"
    ANALYSIS = "ANALYSIS"
    ACTION = "ACTION"
    ALERT = "ALERT"

class FeedItemCreate(BaseModel):
    type: FeedItemType
    title: str
    message: str
    priority: int = 1
    action_payload: Optional[Dict[str, Any]] = None

class FeedItemResponse(FeedItemCreate):
    id: str
    is_read: bool
    is_completed: bool
    created_at: datetime
    
    @field_validator('action_payload', mode='before')
    def parse_payload(cls, v):
        if isinstance(v, str) and v.strip():
            try: return json.loads(v)
            except: return None
        return v
    class Config:
        from_attributes = True

# --- PERFORMANCE ---
class OneRepMaxRequest(BaseModel):
    weight: float
    reps: int
class OneRepMaxResponse(BaseModel):
    estimated_1rm: float
    method_used: str
class ACWRRequest(BaseModel):
    history: List[Dict[str, Any]]
class ACWRResponse(BaseModel):
    ratio: float
    status: str
    color: str
    message: str
class ProfileAuditRequest(BaseModel):
    profile_data: Dict[str, Any]
class ProfileAuditResponse(BaseModel):
    markdown_report: str
class StrategyResponse(BaseModel):
    periodization_title: str
    phases: List[Any]
class WeeklyPlanResponse(BaseModel):
    schedule: List[Any]
    reasoning: str
class UserProfileUpdate(BaseModel):
    profile_data: Dict[str, Any]
"""

# 3. LOGIQUE M√âTIER
LOGIC_CONTENT = """from datetime import date
from typing import Dict, Any
from app.models import sql_models

VALID_SPORT_POSITIONS = {
    'Rugby': ['Pilier', 'Talonneur', '2√®me ligne', '3√®me ligne', 'Demi', 'Centre', 'Ailier', 'Arri√®re'],
    'Football': ['Gardien', 'D√©fenseur', 'Milieu', 'Attaquant'],
}

class CoachLogic:
    @staticmethod
    def validate_sport_position(sport: str, position: str) -> bool:
        if sport in VALID_SPORT_POSITIONS:
            if position and position not in VALID_SPORT_POSITIONS[sport]:
                return False
        return True

    @staticmethod
    def initialize_memory(profile: sql_models.AthleteProfile) -> sql_models.CoachMemory:
        sport = profile.sport_context.get('sport', 'Autre')
        insights = {
            "primary_sport": sport,
            "specificity_index": "High" if sport in ['Rugby', 'Football'] else "Medium",
            "focus_areas": ["Strength", "Hypertrophy"] 
        }
        context = {
            "macrocycle_phase": "Adaptation Anatomique",
            "fatigue_state": "Fresh",
            "readiness_score": 100,
            "season_week": 1
        }
        flags = {
            "needs_deload": False,
            "injury_risk": False,
            "adaptation_window_open": True
        }
        memory = sql_models.CoachMemory(
            athlete_profile_id=profile.id,
            sport_specific_insights=insights,
            current_context=context,
            memory_flags=flags,
            coach_notes={"initialization": f"Profil cr√©√© le {date.today()}"}
        )
        return memory

    @staticmethod
    def calculate_readiness(profile: sql_models.AthleteProfile) -> int:
        base_score = 80
        sleep = profile.physical_metrics.get('sleep_quality_avg', 5)
        if sleep >= 8: base_score += 10
        elif sleep <= 4: base_score -= 20
        stress = profile.constraints.get('work_stress_level', 5)
        if stress >= 8: base_score -= 15
        return max(0, min(100, base_score))

    @staticmethod
    def update_daily(memory: sql_models.CoachMemory, profile: sql_models.AthleteProfile):
        new_readiness = CoachLogic.calculate_readiness(profile)
        current_context = dict(memory.current_context or {})
        current_context['readiness_score'] = new_readiness
        
        if new_readiness < 40:
            current_context['fatigue_state'] = "High"
        elif new_readiness < 70:
            current_context['fatigue_state'] = "Moderate"
        else:
            current_context['fatigue_state'] = "Optimal"
            
        memory.current_context = current_context
        flags = dict(memory.memory_flags or {})
        flags['needs_deload'] = new_readiness < 30
        flags['adaptation_window_open'] = new_readiness > 70
        memory.memory_flags = flags
"""

# 4. NOUVEAU ROUTEUR
ROUTER_CONTENT = """from fastapi import APIRouter, Depends, HTTPException, status
from sqlalchemy.orm import Session
from app.core.database import get_db
from app.models import sql_models, schemas
from app.dependencies import get_current_user
from app.services.coach_logic import CoachLogic

router = APIRouter(
    prefix="/api/v1",
    tags=["Athlete Profile & Memory"]
)

@router.get("/profiles/me", response_model=schemas.AthleteProfileResponse)
async def get_my_profile(
    current_user: sql_models.User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    if not current_user.athlete_profile:
        profile = sql_models.AthleteProfile(user_id=current_user.id)
        db.add(profile)
        db.commit()
        db.refresh(profile)
        return profile
    return current_user.athlete_profile

@router.post("/profiles/complete", response_model=schemas.AthleteProfileResponse)
async def complete_profile(
    profile_data: schemas.AthleteProfileCreate,
    current_user: sql_models.User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    sport = profile_data.sport_context.sport
    pos = profile_data.sport_context.position
    if not CoachLogic.validate_sport_position(sport, pos):
        raise HTTPException(status_code=400, detail=f"Position {pos} invalide pour le sport {sport}")

    db_profile = current_user.athlete_profile
    if not db_profile:
        db_profile = sql_models.AthleteProfile(user_id=current_user.id)
        db.add(db_profile)
    
    db_profile.basic_info = profile_data.basic_info.dict()
    db_profile.physical_metrics = profile_data.physical_metrics.dict()
    db_profile.sport_context = profile_data.sport_context.dict()
    db_profile.training_preferences = profile_data.training_preferences.dict()
    db_profile.goals = profile_data.goals
    db_profile.constraints = profile_data.constraints
    
    if not db_profile.coach_memory:
        memory = CoachLogic.initialize_memory(db_profile)
        db.add(memory)
    
    db.commit()
    db.refresh(db_profile)
    return db_profile

@router.get("/coach-memories/me", response_model=schemas.CoachMemoryResponse)
async def get_my_coach_memory(
    current_user: sql_models.User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    if not current_user.athlete_profile or not current_user.athlete_profile.coach_memory:
        raise HTTPException(status_code=404, detail="Profil ou M√©moire introuvable. Compl√©tez votre profil.")
    return current_user.athlete_profile.coach_memory

@router.post("/coach-memories/recalculate")
async def force_recalculate(
    current_user: sql_models.User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    profile = current_user.athlete_profile
    if not profile or not profile.coach_memory:
        raise HTTPException(status_code=404, detail="Introuvable")
        
    CoachLogic.update_daily(profile.coach_memory, profile)
    db.commit()
    return {"status": "updated", "new_readiness": profile.coach_memory.current_context.get('readiness_score')}
"""

# --- FONCTIONS UTILITAIRES ---
def write_file(path, content):
    full_path = os.path.join(BASE_DIR, path)
    os.makedirs(os.path.dirname(full_path), exist_ok=True)
    with open(full_path, "w", encoding="utf-8") as f:
        f.write(content)
    logger.info(f"‚úÖ Fichier √©crit : {path}")

def update_main_py():
    main_path = os.path.join(BASE_DIR, "app/main.py")
    if not os.path.exists(main_path):
        logger.error(f"‚ùå Impossible de trouver {main_path}. V√©rifiez l'emplacement du script.")
        return

    with open(main_path, "r", encoding="utf-8") as f:
        content = f.read()
    
    if "app.routers import profiles" in content:
        logger.info("‚ÑπÔ∏è main.py d√©j√† √† jour.")
        return

    content = content.replace(
        "from app.routers import performance, safety, auth, workouts, coach, user, feed",
        "from app.routers import performance, safety, auth, workouts, coach, user, feed, profiles"
    )
    
    if "app.include_router(feed.router)" in content:
        content = content.replace(
            "app.include_router(feed.router)",
            "app.include_router(feed.router)\napp.include_router(profiles.router)"
        )
    
    with open(main_path, "w", encoding="utf-8") as f:
        f.write(content)
    logger.info("‚úÖ main.py mis √† jour avec le routeur profiles.")

def migrate_database():
    """Migration SQL + Donn√©es"""
    logger.info("üîÑ D√©marrage de la migration base de donn√©es...")
    load_dotenv(os.path.join(BASE_DIR, ".env"))
    
    db_url = os.getenv("DATABASE_URL", "sqlite:///./sql_app.db")
    if db_url.startswith("postgres://"):
        db_url = db_url.replace("postgres://", "postgresql://", 1)
        
    engine = create_engine(db_url)
    
    with engine.connect() as conn:
        trans = conn.begin()
        try:
            # 1. CR√âATION DES TABLES V2
            logger.info("üÜï Cr√©ation des tables V2 (si absentes)...")
            conn.execute(text("""
                CREATE TABLE IF NOT EXISTS athlete_profiles (
                    id SERIAL PRIMARY KEY,
                    user_id INTEGER UNIQUE REFERENCES users(id) ON DELETE CASCADE,
                    basic_info JSON DEFAULT '{}',
                    physical_metrics JSON DEFAULT '{}',
                    sport_context JSON DEFAULT '{}',
                    performance_baseline JSON DEFAULT '{}',
                    injury_prevention JSON DEFAULT '{}',
                    training_preferences JSON DEFAULT '{}',
                    goals JSON DEFAULT '{}',
                    constraints JSON DEFAULT '{}',
                    created_at TIMESTAMPTZ DEFAULT NOW(),
                    updated_at TIMESTAMPTZ DEFAULT NOW()
                );
            """))
            conn.execute(text("""
                CREATE TABLE IF NOT EXISTS coach_memories (
                    id SERIAL PRIMARY KEY,
                    athlete_profile_id INTEGER UNIQUE REFERENCES athlete_profiles(id) ON DELETE CASCADE,
                    metadata_info JSON DEFAULT '{}',
                    current_context JSON DEFAULT '{}',
                    response_patterns JSON DEFAULT '{}',
                    performance_baselines JSON DEFAULT '{}',
                    adaptation_signals JSON DEFAULT '{}',
                    sport_specific_insights JSON DEFAULT '{}',
                    training_history_summary JSON DEFAULT '{}',
                    athlete_preferences JSON DEFAULT '{}',
                    coach_notes JSON DEFAULT '{}',
                    memory_flags JSON DEFAULT '{}',
                    last_updated TIMESTAMPTZ DEFAULT NOW()
                );
            """))

            # 1.5. PATCH DES TABLES EXISTANTES (Correction de l'erreur email)
            # Cette √©tape ajoute les colonnes AVANT d'essayer de les lire pour la migration
            logger.info("üîß V√©rification et r√©paration du sch√©ma 'users'...")
            try:
                # Tentative d'ajout des colonnes si elles manquent
                conn.execute(text("ALTER TABLE users ADD COLUMN IF NOT EXISTS email VARCHAR UNIQUE;"))
                conn.execute(text("ALTER TABLE users ADD COLUMN IF NOT EXISTS profile_data TEXT;"))
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Note sur le patch de sch√©ma: {e}")
                # En cas d'erreur ici, on continue, l'√©tape de migration de donn√©es √©chouera plus tard si critique

            # 2. MIGRATION DES DONN√âES
            logger.info("üîÑ Migration des donn√©es utilisateurs existants...")
            
            # Maintenant que la colonne email existe forc√©ment, cette requ√™te ne plantera plus
            result = conn.execute(text("SELECT id, username, email, profile_data FROM users"))
            users = result.fetchall()
            
            migrated_count = 0
            for u in users:
                # V√©rifier si profil existe d√©j√†
                exists = conn.execute(text(f"SELECT 1 FROM athlete_profiles WHERE user_id = {u.id}")).scalar()
                if not exists:
                    # Parsing old data
                    old_data = {}
                    if u.profile_data:
                        try:
                            old_data = json.loads(u.profile_data)
                        except:
                            pass
                    
                    # Construction new data structure
                    basic_info = json.dumps({"pseudo": u.username, "email": u.email})
                    physical_metrics = json.dumps({"weight": old_data.get("weight", 0), "height": old_data.get("height", 0)})
                    sport_context = json.dumps({"sport": old_data.get("sport", "Autre"), "level": old_data.get("level", "Interm√©diaire")})
                    
                    # Insert AthleteProfile
                    conn.execute(text("""
                        INSERT INTO athlete_profiles (user_id, basic_info, physical_metrics, sport_context)
                        VALUES (:uid, :bi, :pm, :sc)
                    """), {"uid": u.id, "bi": basic_info, "pm": physical_metrics, "sc": sport_context})
                    
                    # Get Profile ID
                    pid_result = conn.execute(text(f"SELECT id FROM athlete_profiles WHERE user_id = {u.id}"))
                    pid = pid_result.scalar()
                    
                    # Insert Default CoachMemory
                    if pid:
                        conn.execute(text("""
                            INSERT INTO coach_memories (athlete_profile_id, current_context, memory_flags)
                            VALUES (:pid, '{"readiness_score": 80, "phase": "Integration"}', '{"migrated": true}')
                        """), {"pid": pid})
                    
                    migrated_count += 1
            
            trans.commit()
            logger.info(f"‚úÖ Migration termin√©e : {migrated_count} utilisateurs migr√©s vers v2.")
            
        except Exception as e:
            trans.rollback()
            logger.error(f"‚ùå Erreur migration DB: {e}")
            raise e

# --- EXECUTION PRINCIPALE ---

if __name__ == "__main__":
    print("üöÄ D√©marrage de la mise √† jour TitanFlow v2 (Fix Email)...")
    
    # 1. √âcriture des fichiers
    write_file("app/models/sql_models.py", SQL_MODELS_CONTENT)
    write_file("app/models/schemas.py", SCHEMAS_CONTENT)
    write_file("app/services/coach_logic.py", LOGIC_CONTENT)
    write_file("app/routers/profiles.py", ROUTER_CONTENT)
    
    # 2. Mise √† jour main.py
    update_main_py()
    
    # 3. Migration DB
    try:
        migrate_database()
    except Exception as e:
        logger.error(f"‚ùå √âchec critique: {e}")

    print("\n‚ú® Installation termin√©e ! Relancez le serveur Uvicorn.")-e 

-e 
================================================================================
üìÑ FICHIER : backend/init_db.py
================================================================================
from app.core.database import engine, Base
from app.models import sql_models

print("üèóÔ∏è  Force-Cr√©ation des tables en cours...")
try:
    Base.metadata.create_all(bind=engine)
    print("‚úÖ  Succ√®s ! Toutes les tables (users + workout_sessions) sont pr√™tes.")
except Exception as e:
    print(f"‚ùå  Erreur : {e}")-e 

-e 
================================================================================
üìÑ FICHIER : backend/migrate_db.py
================================================================================
#!/usr/bin/env python3
"""
SCRIPT DE MIGRATION S√âCURIS√â TITAN V2
Objectif : Ajouter le support JSON (profile_data) SANS casser l'existant.
"""

import sys
import os
from pathlib import Path
from sqlalchemy import create_engine, text, inspect
from dotenv import load_dotenv

# Ajouter le backend au path
sys.path.append(str(Path(__file__).parent))

load_dotenv()

def get_db_url():
    db_url = os.getenv("DATABASE_URL", "sqlite:///./sql_app.db")
    if db_url.startswith("postgres://"):
        db_url = db_url.replace("postgres://", "postgresql://", 1)
    return db_url

def run_migration():
    print("üöÄ D√âMARRAGE DE LA MIGRATION S√âCURIS√âE...")
    
    engine = create_engine(get_db_url())
    
    with engine.connect() as conn:
        trans = conn.begin()
        try:
            inspector = inspect(engine)
            existing_tables = inspector.get_table_names()
            
            # --- √âTAPE 1 : AJOUTER LA COLONNE JSON √Ä USERS (CRITIQUE POUR AUTH.PY) ---
            print("\n1Ô∏è‚É£  V√©rification de la table 'users'...")
            if 'users' in existing_tables:
                columns = [col['name'] for col in inspector.get_columns('users')]
                
                if 'profile_data' not in columns:
                    print("   ‚ûï Ajout de la colonne 'profile_data'...")
                    # Syntaxe compatible Postgres (JSONB) et SQLite (TEXT/JSON)
                    is_postgres = "postgresql" in str(engine.url)
                    col_type = "JSONB" if is_postgres else "JSON"
                    
                    # Fallback SQLite si besoin
                    if "sqlite" in str(engine.url): col_type = "TEXT" 

                    conn.execute(text(f"ALTER TABLE users ADD COLUMN profile_data {col_type} DEFAULT '{{}}'"))
                    print("   ‚úÖ Colonne ajout√©e avec succ√®s.")
                else:
                    print("   ‚úÖ Colonne 'profile_data' d√©j√† pr√©sente.")
            else:
                print("   ‚ö†Ô∏è Table 'users' introuvable (sera cr√©√©e au red√©marrage via init_db).")

            # --- √âTAPE 2 : PROTECTION DES TABLES EXISTANTES ---
            # On NE SUPPRIME PAS les tables tant que les mod√®les SQL les r√©f√©rencent encore.
            print("\n2Ô∏è‚É£  V√©rification des tables historiques (Mode Non-Destructif)...")
            tables_to_check = ['coach_memories', 'athlete_profiles']
            
            for table in tables_to_check:
                if table in existing_tables:
                    print(f"   üõ°Ô∏è  Table {table} pr√©serv√©e (Code SQL encore actif).")
                else:
                    print(f"   ‚ÑπÔ∏è  Table {table} absente (Sera recr√©√©e si n√©cessaire par SQLAlchemy).")

            # --- √âTAPE 3 : CR√âER FEED_ITEMS (SI MANQUANTE) ---
            print("\n3Ô∏è‚É£  V√©rification de 'feed_items'...")
            if 'feed_items' not in existing_tables:
                print("   ‚ûï Cr√©ation de 'feed_items'...")
                conn.execute(text("""
                    CREATE TABLE feed_items (
                        id VARCHAR PRIMARY KEY,
                        user_id INTEGER REFERENCES users(id) ON DELETE CASCADE,
                        type VARCHAR NOT NULL,
                        title VARCHAR NOT NULL,
                        message VARCHAR NOT NULL,
                        action_payload TEXT,
                        is_read BOOLEAN DEFAULT FALSE,
                        is_completed BOOLEAN DEFAULT FALSE,
                        priority INTEGER DEFAULT 1,
                        created_at TIMESTAMPTZ DEFAULT NOW()
                    );
                """))
                print("   ‚úÖ Table cr√©√©e.")
            else:
                print("   ‚úÖ Table 'feed_items' d√©j√† pr√©sente.")

            trans.commit()
            print("\nüéâ MIGRATION TERMIN√âE AVEC SUCC√àS !")
            print("   Votre base est pr√™te pour le profil JSON sans perte de donn√©es.")
            
        except Exception as e:
            trans.rollback()
            print(f"\n‚ùå ERREUR MIGRATION : {e}")
            raise e

if __name__ == "__main__":
    run_migration()-e 

-e 
================================================================================
üìÑ FICHIER : backend/synchro.py
================================================================================
import sys
import os
import json
import re
import datetime
from pathlib import Path

# ==============================================================================
# 1. SETUP
# ==============================================================================
print("üîç Initialisation du scanner de projet...")
SCRIPT_PATH = Path(__file__).resolve()
PROJECT_ROOT = None
search_dir = SCRIPT_PATH.parent
for _ in range(5):
    if (search_dir / "backend").exists() and (search_dir / "frontend").exists():
        PROJECT_ROOT = search_dir
        break
    if search_dir.parent == search_dir: break
    search_dir = search_dir.parent

if PROJECT_ROOT is None:
    print("‚ùå ERREUR : Racine du projet introuvable.")
    sys.exit(1)

BACKEND_DIR = PROJECT_ROOT / "backend"
FRONTEND_MODELS_DIR = PROJECT_ROOT / "frontend" / "lib" / "models" / "generated"
sys.path.append(str(BACKEND_DIR))

# ==============================================================================
# 2. IMPORTATION
# ==============================================================================
try:
    from app.models.schemas import (
        UserResponse, Token, AthleteProfileResponse,
        CoachMemoryResponse, CoachEngramResponse,
        WorkoutSessionResponse, WorkoutSetResponse,
        AIWorkoutPlan, AIExercise, WeeklyPlanResponse, StrategyResponse, ProfileAuditResponse,
        FeedItemResponse, OneRepMaxResponse, ACWRResponse,
        # ‚úÖ AJOUT DES SOUS-MOD√àLES MANQUANTS
        BasicInfo, PhysicalMetrics, SportContext, TrainingPreferences
    )
    from app.models.enums import (
        MemoryType, ImpactLevel, MemoryStatus, FeedItemType, SportType, EquipmentType
    )
    print("‚úÖ Mod√®les import√©s.")
except ImportError as e:
    print(f"‚ùå ERREUR IMPORT : {e}")
    sys.exit(1)

# Liste compl√®te des classes √† g√©n√©rer
MODELS = [
    UserResponse, Token, 
    AthleteProfileResponse, BasicInfo, PhysicalMetrics, SportContext, TrainingPreferences,
    CoachMemoryResponse, CoachEngramResponse,
    WorkoutSessionResponse, WorkoutSetResponse, 
    AIWorkoutPlan, AIExercise,
    WeeklyPlanResponse, StrategyResponse, ProfileAuditResponse, 
    FeedItemResponse, OneRepMaxResponse, ACWRResponse
]

ENUMS = [MemoryType, ImpactLevel, MemoryStatus, FeedItemType, SportType, EquipmentType]

# ==============================================================================
# 3. GENERATEUR DART INTELLIGENT
# ==============================================================================
TYPE_MAP = {
    'str': 'String', 'int': 'int', 'float': 'double', 'bool': 'bool', 
    'datetime': 'DateTime', 'date': 'DateTime', 'dict': 'Map<String, dynamic>', 
    'list': 'List', 'any': 'dynamic'
}

def to_camel_case(snake_str):
    components = snake_str.split('_')
    return components[0] + ''.join(x.title() for x in components[1:])

def get_dart_filename(class_name):
    # Nettoyage du nom pour le fichier (UserResponse -> user.dart)
    clean = class_name.replace("Response", "").replace("Schema", "")
    s1 = re.sub('(.)([A-Z][a-z]+)', r'\1_\2', clean)
    return re.sub('([a-z0-9])([A-Z])', r'\1_\2', s1).lower() + ".dart"

def get_clean_classname(class_name):
    return class_name.replace("Response", "").replace("Schema", "")

def generate_dart_code(model, all_models_names, all_enums_names):
    raw_name = model.__name__
    cls_name = get_clean_classname(raw_name)
    
    try: schema = model.model_json_schema()
    except: schema = model.schema()
    
    props = schema.get('properties', {})
    fields = []
    ctor_params = []
    from_json = []
    to_json = []
    imports_set = set() # Pour stocker les fichiers √† importer
    
    for fname, info in props.items():
        # --- 1. D√âTECTION DU TYPE ---
        py_type = info.get('type', 'any')
        dart_type = "dynamic"
        ref_obj = None
        
        # Extraction de la r√©f√©rence ($ref)
        if '$ref' in info: ref_obj = info['$ref'].split('/')[-1]
        elif 'allOf' in info and '$ref' in info['allOf'][0]: ref_obj = info['allOf'][0]['$ref'].split('/')[-1]
        elif 'anyOf' in info:
            for opt in info['anyOf']:
                if '$ref' in opt: ref_obj = opt['$ref'].split('/')[-1]
        
        is_complex = False
        is_enum = False
        is_list_complex = False
        
        if ref_obj:
            clean_ref = get_clean_classname(ref_obj)
            dart_type = clean_ref
            
            # Est-ce un Enum ou une autre Classe ?
            if ref_obj in all_enums_names or clean_ref in all_enums_names:
                is_enum = True
                imports_set.add("enums.dart")
            else:
                is_complex = True
                # ‚úÖ Ajout de l'import automatique
                target_file = get_dart_filename(ref_obj)
                if target_file != get_dart_filename(raw_name): # Pas d'auto-import
                    imports_set.add(target_file)
        else:
            dart_type = TYPE_MAP.get(py_type, 'dynamic')
            
        # Gestion des Listes
        if py_type == 'array':
            items = info.get('items', {})
            inner_ref = None
            
            if '$ref' in items: inner_ref = items['$ref'].split('/')[-1]
            elif 'anyOf' in items: # Cas List[Union[...]]
                 for opt in items['anyOf']:
                    if '$ref' in opt: inner_ref = opt['$ref'].split('/')[-1]

            if inner_ref:
                inner_clean = get_clean_classname(inner_ref)
                dart_type = f"List<{inner_clean}>"
                
                if inner_ref in all_enums_names or inner_clean in all_enums_names:
                    # List<Enum> (Rare mais possible)
                    imports_set.add("enums.dart") 
                else:
                    is_list_complex = True
                    target_file = get_dart_filename(inner_ref)
                    imports_set.add(target_file)
            else:
                inner = TYPE_MAP.get(items.get('type', 'any'), 'dynamic')
                dart_type = f"List<{inner}>"

        # --- 2. GESTION NULLABILIT√â & REQUIRED ---
        is_nullable = False
        if 'anyOf' in info:
            for t in info['anyOf']:
                if t.get('type') == 'null': is_nullable = True
        
        # dynamic ne prend jamais de ?
        if dart_type == 'dynamic': is_nullable = False 
        
        var_name = to_camel_case(fname)
        final_type = f"{dart_type}{'?' if is_nullable else ''}"
        
        fields.append(f"  final {final_type} {var_name};")
        
        # Constructeur
        # Si non-nullable, c'est required. Sinon c'est optionnel.
        req = "required " if not is_nullable else ""
        ctor_params.append(f"{req}this.{var_name}")
        
        # --- 3. FROM JSON (Parsing) ---
        acc = f"json['{fname}']"
        val = acc
        
        if is_enum:
            # Enum: fallback safe
            safe_default = f"{dart_type}.values.first"
            if is_nullable:
                val = f"{acc} != null ? {dart_type}.fromJson({acc}) : null"
            else:
                val = f"{acc} != null ? {dart_type}.fromJson({acc}) : {safe_default}"
                
        elif is_complex:
            # Objet Complexe
            if is_nullable:
                val = f"{acc} != null ? {dart_type}.fromJson({acc}) : null"
            else:
                # Force non-null (au pire crash ou map vide, mais respecte le type)
                val = f"{dart_type}.fromJson({acc} ?? {{}})"
                
        elif is_list_complex:
            # Liste d'Objets
            inner = dart_type.replace("List<", "").replace(">", "")
            val = f"({acc} as List?)?.map((e) => {inner}.fromJson(e)).toList()"
            if not is_nullable: val += " ?? []"
            
        elif 'DateTime' in dart_type:
            val = f"{acc} != null ? DateTime.tryParse({acc}.toString()) : null"
            if not is_nullable: val += " ?? DateTime.now()"
            
        elif 'double' in dart_type:
            val = f"({acc} as num?)?.toDouble()"
            if not is_nullable: val += " ?? 0.0"
            
        elif not is_nullable:
            # Primitifs non-nullables : valeurs par d√©faut
            if 'int' in dart_type: val += " ?? 0"
            elif 'String' in dart_type: val += " ?? ''"
            elif 'bool' in dart_type: val += " ?? false"
            elif 'List' in dart_type: val += " ?? []"
            elif 'Map' in dart_type: val += " ?? {}"
            
        from_json.append(f"      {var_name}: {val},")
        
        # --- 4. TO JSON (Export) ---
        exp = var_name
        # Op√©rateur null-aware si nullable
        op = "?." if is_nullable else "."
        
        if 'DateTime' in dart_type: exp = f"{var_name}{op}toIso8601String()"
        elif is_enum: exp = f"{var_name}{op}toJson()"
        elif is_list_complex: exp = f"{var_name}{op}map((e) => e.toJson()).toList()"
        elif is_complex: exp = f"{var_name}{op}toJson()"
            
        to_json.append(f"      '{fname}': {exp},")

    # G√©n√©ration des imports
    imports_code = "\n".join([f"import '{f}';" for f in sorted(imports_set)])
    
    return f"""// G√âN√âR√â AUTOMATIQUEMENT
// Timestamp : {datetime.datetime.now().isoformat()}

{imports_code}

class {cls_name} {{
{chr(10).join(fields)}

  {cls_name}({{
    {', '.join(ctor_params)}
  }});

  factory {cls_name}.fromJson(Map<String, dynamic> json) {{
    return {cls_name}(
{chr(10).join(from_json)}
    );
  }}

  Map<String, dynamic> toJson() {{
    return {{
{chr(10).join(to_json)}
    }};
  }}
}}
"""

def generate_enums(enums_list, path):
    content = f"// G√âN√âR√â AUTOMATIQUEMENT\n// Timestamp : {datetime.datetime.now().isoformat()}\n\n"
    for e in enums_list:
        name = e.__name__
        vals = [v.name for v in e]
        default = vals[-1] if "OTHER" in vals else vals[0]
        
        content += f"enum {name} {{\n  " + ",\n  ".join(vals) + ";\n\n"
        content += f"  String toJson() => name;\n"
        content += f"  static {name} fromJson(dynamic json) {{\n"
        content += f"    return {name}.values.firstWhere((e) => e.name == json.toString(), orElse: () => {default});\n"
        content += "  }\n}\n\n"
        
    with open(path, "w", encoding="utf-8") as f: f.write(content)

# ==============================================================================
# 4. EXECUTION
# ==============================================================================
if __name__ == "__main__":
    if not FRONTEND_MODELS_DIR.exists(): os.makedirs(FRONTEND_MODELS_DIR, exist_ok=True)
    
    # Pr√©pare les listes de noms pour la d√©tection
    model_names = [m.__name__ for m in MODELS]
    clean_model_names = [get_clean_classname(n) for n in model_names]
    enum_names = [e.__name__ for e in ENUMS]
    
    print("\nüöÄ G√©n√©ration des Mod√®les...")
    for m in MODELS:
        code = generate_dart_code(m, model_names + clean_model_names, enum_names)
        path = FRONTEND_MODELS_DIR / get_dart_filename(m.__name__)
        with open(path, "w") as f: f.write(code)
        print(f"  ‚ú® {get_clean_classname(m.__name__)}")

    print("\nüöÄ G√©n√©ration des Enums...")
    generate_enums(ENUMS, FRONTEND_MODELS_DIR / "enums.dart")
    print("  ‚ú® enums.dart")
    
    print("\n‚úÖ Termin√©.")-e 

-e 
================================================================================
üìÑ FICHIER : backend/test.py
================================================================================
import requests
import sys
import time
import json

# Configuration
BASE_URL = "http://localhost:8000"
# G√©n√©ration d'identifiants uniques pour le test
TIMESTAMP = int(time.time())
USERNAME = f"ci_bot_{TIMESTAMP}"
PASSWORD = "TestPassword123!"
EMAIL = f"ci_{TIMESTAMP}@test.com"

def run_test():
    print(f"üöÄ D√©marrage du test de validation FIX-500 sur {BASE_URL}")

    # 1. INSCRIPTION
    print("üîπ √âtape 1 : Inscription...")
    signup_payload = {"username": USERNAME, "email": EMAIL, "password": PASSWORD}
    try:
        r = requests.post(f"{BASE_URL}/auth/signup", json=signup_payload)
        if r.status_code not in [200, 201]:
            print(f"‚ùå √âchec Inscription: {r.text}")
            sys.exit(1)
    except Exception as e:
        print(f"‚ùå Le serveur semble √©teint : {e}")
        sys.exit(1)

    # 2. CONNEXION (TOKEN)
    print("üîπ √âtape 2 : Connexion...")
    login_data = {"username": USERNAME, "password": PASSWORD}
    r = requests.post(f"{BASE_URL}/auth/token", data=login_data)
    if r.status_code != 200:
        print(f"‚ùå √âchec Connexion: {r.text}")
        sys.exit(1)
    
    token = r.json().get("access_token")
    headers = {"Authorization": f"Bearer {token}", "Content-Type": "application/json"}

    # 3. TEST SAUVEGARDE PROFIL (LE COEUR DU BUG)
    print("üîπ √âtape 3 : Sauvegarde Profil (Test de r√©gression)...")
    
    # Payload complexe (Dictionnaire imbriqu√©) pour provoquer l'erreur 500
    # si le backend ne fait pas le json.dumps()
    profile_payload = {
        "profile_data": {
            "basic_info": {
                "pseudo": USERNAME,
                "email": EMAIL,
                "biography": "Test CI/CD avec caract√®res sp√©ciaux √©√†√π"
            },
            "sport_context": {
                "sport": "Crossfit",
                "stats": {"max_pullups": 20, "run_5k": "20:00"}
            },
            "goals": {
                "primary": "Survivre au d√©ploiement"
            }
        }
    }

    r = requests.post(f"{BASE_URL}/api/v1/profiles/complete", json=profile_payload, headers=headers)

    # 4. VERIFICATION
    if r.status_code == 200:
        print("‚úÖ SUCC√àS : Le profil a √©t√© sauvegard√© sans erreur 500.")
        print("   Le correctif `json.dumps` est actif.")
        
        # V√©rification optionnelle du retour
        data = r.json()
        if isinstance(data.get("profile_data"), dict):
             print("‚úÖ Le backend a bien retourn√© un JSON (Dict) propre.")
        else:
             print("‚ö†Ô∏è Warning: Le backend a retourn√© une String au lieu d'un Dict (Pydantic parsing warning).")
             
        sys.exit(0)
    elif r.status_code == 500:
        print("üî• √âCHEC CRITIQUE : Erreur 500 d√©tect√©e.")
        print("   Cause probable : Le dictionnaire Python est pass√© directement √† SQLAlchemy sans s√©rialisation.")
        print(f"   R√©ponse : {r.text}")
        sys.exit(1)
    else:
        print(f"‚ùå √âchec inattendu (Code {r.status_code}): {r.text}")
        sys.exit(1)

if __name__ == "__main__":
    run_test()-e 

-e 
================================================================================
üìÑ FICHIER : backend/visualisation_db.py
================================================================================
import streamlit as st
import pandas as pd
from sqlalchemy import create_engine, inspect, text

# --- 1. CONFIGURATION ---
# Votre URL est int√©gr√©e ici :
DB_URL = "postgresql://titanflow_prod_db_user:1VRDWljUne5YD0lczDfcY3gLglcgS3VU@dpg-d5ec3fruibrs738a76a0-a.frankfurt-postgres.render.com/titanflow_prod_db"

st.set_page_config(
    page_title="TitanFlow DB Admin", 
    layout="wide",  # Mode "Large" pour voir toutes les colonnes
    page_icon="üëÅÔ∏è"
)

# CSS pour maximiser l'espace
st.markdown("""
<style>
    .block-container { padding-top: 2rem; padding-bottom: 2rem; }
    div[data-testid="stDataFrame"] { width: 100%; }
</style>
""", unsafe_allow_html=True)

st.title("üëÅÔ∏è TitanFlow - Inspecteur de Base de Donn√©es")

# --- 2. CONNEXION ---
try:
    # Petit nettoyage au cas o√π (Render donne parfois postgres:// au lieu de postgresql://)
    if DB_URL.startswith("postgres://"):
        real_url = DB_URL.replace("postgres://", "postgresql://", 1)
    else:
        real_url = DB_URL

    engine = create_engine(real_url)
    inspector = inspect(engine)
    
    # Test de connexion et r√©cup√©ration des tables
    all_tables = inspector.get_table_names()

    if not all_tables:
        st.warning("‚ö†Ô∏è Connexion r√©ussie, mais aucune table trouv√©e dans la base.")
    else:
        # --- 3. SIDEBAR (Navigation) ---
        st.sidebar.header("üìÇ Tables disponibles")
        
        # Tri : on met les tables importantes en haut
        priority = ['users', 'athlete_profiles', 'workout_sessions', 'coach_memories']
        sorted_tables = sorted(all_tables, key=lambda x: (0 if x in priority else 1, x))
        
        selected_table = st.sidebar.radio("S√©lectionnez une table :", sorted_tables)

        st.divider()
        st.header(f"Table : `{selected_table}`")

        # --- 4. INSPECTION DE LA STRUCTURE (Colonnes) ---
        # C'est ici qu'on v√©rifie si les colonnes existent vraiment
        columns_info = inspector.get_columns(selected_table)
        col_names = [col['name'] for col in columns_info]
        
        st.info(f"üìä La table contient **{len(col_names)} colonnes**.")
        
        # Liste d√©roulante pour v√©rifier les noms exacts
        with st.expander("üîé Cliquez ici pour voir la liste exacte des colonnes (Sch√©ma)"):
            schema_df = pd.DataFrame([
                {"Nom": c['name'], "Type": str(c['type']), "Nullable": c['nullable']} 
                for c in columns_info
            ])
            st.table(schema_df)

        # --- 5. AFFICHAGE DES DONN√âES ---
        st.subheader("Donn√©es enregistr√©es")
        
        with engine.connect() as conn:
            # On r√©cup√®re tout le contenu
            query = text(f"SELECT * FROM {selected_table}")
            df = pd.read_sql(query, conn)

        if df.empty:
            st.warning("Cette table est vide (0 ligne).")
        else:
            # Affichage du tableau interactif
            st.dataframe(
                df, 
                use_container_width=True, 
                height=600  # Grande hauteur pour le confort
            )

except Exception as e:
    st.error("‚ùå Erreur de connexion")
    st.error(f"D√©tails : {e}")
    st.info("V√©rifiez votre connexion internet ou si l'URL a chang√©.")-e 

-e 
================================================================================
üìÑ FICHIER : frontend/app.py
================================================================================
import streamlit as st
import requests
import pandas as pd
from datetime import datetime

# Configuration de la page
st.set_page_config(page_title="TitanFlow Pro", page_icon="‚ö°", layout="wide")

# L'URL de ton API (Backend)
API_URL = "http://127.0.0.1:8000"

# --- GESTION DE LA SESSION (Token) ---
if "token" not in st.session_state:
    st.session_state.token = None

def login():
    st.sidebar.header("üîê Connexion")
    username = st.sidebar.text_input("Pseudo")
    password = st.sidebar.text_input("Mot de passe", type="password")
    
    if st.sidebar.button("Se connecter"):
        try:
            # Appel √† l'API pour r√©cup√©rer le token
            response = requests.post(
                f"{API_URL}/auth/token",
                data={"username": username, "password": password}
            )
            if response.status_code == 200:
                st.session_state.token = response.json()["access_token"]
                st.sidebar.success("Connect√© !")
                st.rerun()
            else:
                st.sidebar.error("Erreur de connexion")
        except Exception as e:
            st.sidebar.error(f"API introuvable : {e}")

def logout():
    if st.sidebar.button("Se d√©connecter"):
        st.session_state.token = None
        st.rerun()

# --- INTERFACE PRINCIPALE ---
st.title("‚ö° TitanFlow : Monitoring Athl√©tique")

# V√©rification de l'√©tat de l'API
try:
    health = requests.get(f"{API_URL}/health").json()
    st.success(f"Backend connect√© v{health['version']}")
except:
    st.error("üö® Le Backend semble √©teint. V√©rifie que le Terminal 1 tourne bien !")

# Gestion Login/Logout
if not st.session_state.token:
    st.info("Veuillez vous connecter dans la barre lat√©rale pour acc√©der aux donn√©es.")
    login()
else:
    logout()
    st.write("---")
    
    # Onglets de l'application
    tab1, tab2 = st.tabs(["üèãÔ∏è‚Äç‚ôÇÔ∏è Historique", "‚ûï Nouvelle S√©ance"])
    
    # --- ONGLET 1 : HISTORIQUE ---
    with tab1:
        st.subheader("Vos s√©ances enregistr√©es")
        headers = {"Authorization": f"Bearer {st.session_state.token}"}
        
        try:
            res = requests.get(f"{API_URL}/workouts/", headers=headers)
            if res.status_code == 200:
                workouts = res.json()
                if workouts:
                    df = pd.DataFrame(workouts)
                    st.dataframe(df, use_container_width=True)
                else:
                    st.info("Aucune s√©ance trouv√©e.")
            else:
                st.error("Erreur chargement donn√©es")
        except Exception as e:
            st.error(f"Erreur : {e}")

    # --- ONGLET 2 : AJOUTER S√âANCE ---
    with tab2:
        st.subheader("Enregistrer un entra√Ænement")
        with st.form("new_workout"):
            col1, col2 = st.columns(2)
            date = col1.date_input("Date")
            duration = col2.number_input("Dur√©e (min)", min_value=0, value=60)
            rpe = st.slider("Intensit√© (RPE)", 0, 10, 5)
            
            submitted = st.form_submit_button("Sauvegarder")
            
            if submitted:
                # Pr√©paration du JSON
                payload = {
                    "date": str(date),
                    "duration": duration,
                    "rpe": rpe
                }
                # Envoi √† l'API
                headers = {"Authorization": f"Bearer {st.session_state.token}"}
                res = requests.post(f"{API_URL}/workouts/", json=payload, headers=headers)
                
                if res.status_code == 200:
                    st.success("S√©ance enregistr√©e ! üéâ")
                    # Petit hack pour rafra√Æchir l'historique
                    st.rerun()
                else:
                    st.error(f"Erreur : {res.text}")
-e 

-e 
================================================================================
üìÑ FICHIER : frontend/lib/models/generated/acwr.dart
================================================================================
// G√âN√âR√â AUTOMATIQUEMENT
// Timestamp : 2026-01-16T20:28:24.624022



class ACWR {
  final dynamic ratio;
  final dynamic status;
  final dynamic color;
  final dynamic message;

  ACWR({
    required this.ratio, required this.status, required this.color, required this.message
  });

  factory ACWR.fromJson(Map<String, dynamic> json) {
    return ACWR(
      ratio: json['ratio'],
      status: json['status'],
      color: json['color'],
      message: json['message'],
    );
  }

  Map<String, dynamic> toJson() {
    return {
      'ratio': ratio,
      'status': status,
      'color': color,
      'message': message,
    };
  }
}
-e 

-e 
================================================================================
üìÑ FICHIER : frontend/lib/models/generated/ai_exercise.dart
================================================================================
// G√âN√âR√â AUTOMATIQUEMENT
// Timestamp : 2026-01-16T20:28:24.620342



class AIExercise {
  final dynamic name;
  final dynamic sets;
  final dynamic reps;
  final dynamic rest;
  final dynamic tips;
  final dynamic recordingMode;

  AIExercise({
    required this.name, required this.sets, required this.reps, required this.rest, required this.tips, required this.recordingMode
  });

  factory AIExercise.fromJson(Map<String, dynamic> json) {
    return AIExercise(
      name: json['name'],
      sets: json['sets'],
      reps: json['reps'],
      rest: json['rest'],
      tips: json['tips'],
      recordingMode: json['recording_mode'],
    );
  }

  Map<String, dynamic> toJson() {
    return {
      'name': name,
      'sets': sets,
      'reps': reps,
      'rest': rest,
      'tips': tips,
      'recording_mode': recordingMode,
    };
  }
}
-e 

-e 
================================================================================
üìÑ FICHIER : frontend/lib/models/generated/ai_workout_plan.dart
================================================================================
// G√âN√âR√â AUTOMATIQUEMENT
// Timestamp : 2026-01-16T20:28:24.618409

import 'ai_exercise.dart';

class AIWorkoutPlan {
  final dynamic title;
  final dynamic coachComment;
  final List<dynamic> warmup;
  final List<AIExercise> exercises;
  final List<dynamic> cooldown;

  AIWorkoutPlan({
    required this.title, required this.coachComment, required this.warmup, required this.exercises, required this.cooldown
  });

  factory AIWorkoutPlan.fromJson(Map<String, dynamic> json) {
    return AIWorkoutPlan(
      title: json['title'],
      coachComment: json['coach_comment'],
      warmup: json['warmup'] ?? [],
      exercises: (json['exercises'] as List?)?.map((e) => AIExercise.fromJson(e)).toList() ?? [],
      cooldown: json['cooldown'] ?? [],
    );
  }

  Map<String, dynamic> toJson() {
    return {
      'title': title,
      'coach_comment': coachComment,
      'warmup': warmup,
      'exercises': exercises.map((e) => e.toJson()).toList(),
      'cooldown': cooldown,
    };
  }
}
-e 

-e 
================================================================================
üìÑ FICHIER : frontend/lib/models/generated/athlete_profile.dart
================================================================================
// G√âN√âR√â AUTOMATIQUEMENT
// Timestamp : 2026-01-16T20:28:24.604398

import 'basic_info.dart';
import 'physical_metrics.dart';
import 'sport_context.dart';
import 'training_preferences.dart';

class AthleteProfile {
  final BasicInfo basicInfo;
  final PhysicalMetrics physicalMetrics;
  final SportContext sportContext;
  final TrainingPreferences trainingPreferences;
  final dynamic goals;
  final dynamic constraints;
  final dynamic injuryPrevention;
  final dynamic performanceBaseline;
  final dynamic id;
  final dynamic userId;
  final dynamic createdAt;
  final dynamic updatedAt;

  AthleteProfile({
    required this.basicInfo, required this.physicalMetrics, required this.sportContext, required this.trainingPreferences, required this.goals, required this.constraints, required this.injuryPrevention, required this.performanceBaseline, required this.id, required this.userId, required this.createdAt, required this.updatedAt
  });

  factory AthleteProfile.fromJson(Map<String, dynamic> json) {
    return AthleteProfile(
      basicInfo: BasicInfo.fromJson(json['basic_info'] ?? {}),
      physicalMetrics: PhysicalMetrics.fromJson(json['physical_metrics'] ?? {}),
      sportContext: SportContext.fromJson(json['sport_context'] ?? {}),
      trainingPreferences: TrainingPreferences.fromJson(json['training_preferences'] ?? {}),
      goals: json['goals'],
      constraints: json['constraints'],
      injuryPrevention: json['injury_prevention'],
      performanceBaseline: json['performance_baseline'],
      id: json['id'],
      userId: json['user_id'],
      createdAt: json['created_at'],
      updatedAt: json['updated_at'],
    );
  }

  Map<String, dynamic> toJson() {
    return {
      'basic_info': basicInfo.toJson(),
      'physical_metrics': physicalMetrics.toJson(),
      'sport_context': sportContext.toJson(),
      'training_preferences': trainingPreferences.toJson(),
      'goals': goals,
      'constraints': constraints,
      'injury_prevention': injuryPrevention,
      'performance_baseline': performanceBaseline,
      'id': id,
      'user_id': userId,
      'created_at': createdAt,
      'updated_at': updatedAt,
    };
  }
}
-e 

-e 
================================================================================
üìÑ FICHIER : frontend/lib/models/generated/basic_info.dart
================================================================================
// G√âN√âR√â AUTOMATIQUEMENT
// Timestamp : 2026-01-16T20:28:24.605472



class BasicInfo {
  final dynamic pseudo;
  final dynamic email;
  final dynamic birthDate;
  final dynamic trainingAge;
  final dynamic biologicalSex;

  BasicInfo({
    required this.pseudo, required this.email, required this.birthDate, required this.trainingAge, required this.biologicalSex
  });

  factory BasicInfo.fromJson(Map<String, dynamic> json) {
    return BasicInfo(
      pseudo: json['pseudo'],
      email: json['email'],
      birthDate: json['birth_date'],
      trainingAge: json['training_age'],
      biologicalSex: json['biological_sex'],
    );
  }

  Map<String, dynamic> toJson() {
    return {
      'pseudo': pseudo,
      'email': email,
      'birth_date': birthDate,
      'training_age': trainingAge,
      'biological_sex': biologicalSex,
    };
  }
}
-e 

-e 
================================================================================
üìÑ FICHIER : frontend/lib/models/generated/coach_engram.dart
================================================================================
// G√âN√âR√â AUTOMATIQUEMENT
// Timestamp : 2026-01-16T20:28:24.613778

import 'enums.dart';

class CoachEngram {
  final MemoryType type;
  final ImpactLevel impact;
  final MemoryStatus status;
  final dynamic content;
  final List<dynamic> tags;
  final dynamic endDate;
  final dynamic id;
  final dynamic memoryId;
  final dynamic author;
  final dynamic createdAt;

  CoachEngram({
    required this.type, required this.impact, required this.status, required this.content, required this.tags, required this.endDate, required this.id, required this.memoryId, required this.author, required this.createdAt
  });

  factory CoachEngram.fromJson(Map<String, dynamic> json) {
    return CoachEngram(
      type: json['type'] != null ? MemoryType.fromJson(json['type']) : MemoryType.values.first,
      impact: json['impact'] != null ? ImpactLevel.fromJson(json['impact']) : ImpactLevel.values.first,
      status: json['status'] != null ? MemoryStatus.fromJson(json['status']) : MemoryStatus.values.first,
      content: json['content'],
      tags: json['tags'] ?? [],
      endDate: json['end_date'],
      id: json['id'],
      memoryId: json['memory_id'],
      author: json['author'],
      createdAt: json['created_at'],
    );
  }

  Map<String, dynamic> toJson() {
    return {
      'type': type.toJson(),
      'impact': impact.toJson(),
      'status': status.toJson(),
      'content': content,
      'tags': tags,
      'end_date': endDate,
      'id': id,
      'memory_id': memoryId,
      'author': author,
      'created_at': createdAt,
    };
  }
}
-e 

-e 
================================================================================
üìÑ FICHIER : frontend/lib/models/generated/coach_memory.dart
================================================================================
// G√âN√âR√â AUTOMATIQUEMENT
// Timestamp : 2026-01-16T20:28:24.611966

import 'coach_engram.dart';

class CoachMemory {
  final dynamic id;
  final dynamic currentContext;
  final dynamic currentPhase;
  final dynamic flags;
  final dynamic insights;
  final List<CoachEngram> engrams;

  CoachMemory({
    required this.id, required this.currentContext, required this.currentPhase, required this.flags, required this.insights, required this.engrams
  });

  factory CoachMemory.fromJson(Map<String, dynamic> json) {
    return CoachMemory(
      id: json['id'],
      currentContext: json['current_context'],
      currentPhase: json['current_phase'],
      flags: json['flags'],
      insights: json['insights'],
      engrams: (json['engrams'] as List?)?.map((e) => CoachEngram.fromJson(e)).toList() ?? [],
    );
  }

  Map<String, dynamic> toJson() {
    return {
      'id': id,
      'current_context': currentContext,
      'current_phase': currentPhase,
      'flags': flags,
      'insights': insights,
      'engrams': engrams.map((e) => e.toJson()).toList(),
    };
  }
}
-e 

-e 
================================================================================
üìÑ FICHIER : frontend/lib/models/generated/enums.dart
================================================================================
// G√âN√âR√â AUTOMATIQUEMENT
// Timestamp : 2026-01-16T20:28:24.624205

enum MemoryType {
  INJURY,
  CONSTRAINT,
  PREFERENCE,
  STRATEGY,
  OTHER;

  String toJson() => name;
  static MemoryType fromJson(dynamic json) {
    return MemoryType.values.firstWhere((e) => e.name == json.toString(), orElse: () => OTHER);
  }
}

enum ImpactLevel {
  HIGH,
  MEDIUM,
  LOW,
  INFO;

  String toJson() => name;
  static ImpactLevel fromJson(dynamic json) {
    return ImpactLevel.values.firstWhere((e) => e.name == json.toString(), orElse: () => HIGH);
  }
}

enum MemoryStatus {
  ACTIVE,
  RESOLVED,
  ARCHIVED,
  FORGOTTEN;

  String toJson() => name;
  static MemoryStatus fromJson(dynamic json) {
    return MemoryStatus.values.firstWhere((e) => e.name == json.toString(), orElse: () => ACTIVE);
  }
}

enum FeedItemType {
  INFO,
  ANALYSIS,
  ACTION,
  ALERT,
  WORKOUT_LOG,
  COACH_INSIGHT,
  PERSONAL_RECORD,
  SYSTEM_ALERT,
  DAILY_TIP;

  String toJson() => name;
  static FeedItemType fromJson(dynamic json) {
    return FeedItemType.values.firstWhere((e) => e.name == json.toString(), orElse: () => INFO);
  }
}

enum SportType {
  RUGBY,
  FOOTBALL,
  CROSSFIT,
  HYBRID,
  RUNNING,
  OTHER,
  BODYBUILDING,
  CYCLING,
  TRIATHLON,
  POWERLIFTING,
  SWIMMING,
  COMBAT_SPORTS;

  String toJson() => name;
  static SportType fromJson(dynamic json) {
    return SportType.values.firstWhere((e) => e.name == json.toString(), orElse: () => COMBAT_SPORTS);
  }
}

enum EquipmentType {
  PERFORMANCE_LAB,
  COMMERCIAL_GYM,
  HOME_GYM_BARBELL,
  HOME_GYM_DUMBBELL,
  CALISTHENICS_KIT,
  BODYWEIGHT_ZERO,
  ENDURANCE_SUITE,
  STANDARD,
  HOME_GYM_FULL,
  CROSSFIT_BOX,
  DUMBBELLS,
  BARBELL,
  KETTLEBELLS,
  PULL_UP_BAR,
  BENCH,
  DIP_STATION,
  BANDS,
  RINGS_TRX,
  JUMP_ROPE,
  WEIGHT_VEST,
  BIKE,
  HOME_TRAINER,
  ROWER,
  TREADMILL,
  POOL;

  String toJson() => name;
  static EquipmentType fromJson(dynamic json) {
    return EquipmentType.values.firstWhere((e) => e.name == json.toString(), orElse: () => PERFORMANCE_LAB);
  }
}

-e 

-e 
================================================================================
üìÑ FICHIER : frontend/lib/models/generated/feed_item.dart
================================================================================
// G√âN√âR√â AUTOMATIQUEMENT
// Timestamp : 2026-01-16T20:28:24.622965

import 'enums.dart';

class FeedItem {
  final FeedItemType type;
  final dynamic title;
  final dynamic message;
  final dynamic priority;
  final dynamic actionPayload;
  final dynamic id;
  final dynamic isRead;
  final dynamic isCompleted;
  final dynamic createdAt;

  FeedItem({
    required this.type, required this.title, required this.message, required this.priority, required this.actionPayload, required this.id, required this.isRead, required this.isCompleted, required this.createdAt
  });

  factory FeedItem.fromJson(Map<String, dynamic> json) {
    return FeedItem(
      type: json['type'] != null ? FeedItemType.fromJson(json['type']) : FeedItemType.values.first,
      title: json['title'],
      message: json['message'],
      priority: json['priority'],
      actionPayload: json['action_payload'],
      id: json['id'],
      isRead: json['is_read'],
      isCompleted: json['is_completed'],
      createdAt: json['created_at'],
    );
  }

  Map<String, dynamic> toJson() {
    return {
      'type': type.toJson(),
      'title': title,
      'message': message,
      'priority': priority,
      'action_payload': actionPayload,
      'id': id,
      'is_read': isRead,
      'is_completed': isCompleted,
      'created_at': createdAt,
    };
  }
}
-e 

-e 
================================================================================
üìÑ FICHIER : frontend/lib/models/generated/one_rep_max.dart
================================================================================
// G√âN√âR√â AUTOMATIQUEMENT
// Timestamp : 2026-01-16T20:28:24.623484



class OneRepMax {
  final dynamic estimated1Rm;
  final dynamic methodUsed;

  OneRepMax({
    required this.estimated1Rm, required this.methodUsed
  });

  factory OneRepMax.fromJson(Map<String, dynamic> json) {
    return OneRepMax(
      estimated1Rm: json['estimated_1rm'],
      methodUsed: json['method_used'],
    );
  }

  Map<String, dynamic> toJson() {
    return {
      'estimated_1rm': estimated1Rm,
      'method_used': methodUsed,
    };
  }
}
-e 

-e 
================================================================================
üìÑ FICHIER : frontend/lib/models/generated/physical_metrics.dart
================================================================================
// G√âN√âR√â AUTOMATIQUEMENT
// Timestamp : 2026-01-16T20:28:24.606446



class PhysicalMetrics {
  final dynamic height;
  final dynamic weight;
  final dynamic bodyFat;
  final dynamic restingHr;
  final dynamic sleepQualityAvg;

  PhysicalMetrics({
    required this.height, required this.weight, required this.bodyFat, required this.restingHr, required this.sleepQualityAvg
  });

  factory PhysicalMetrics.fromJson(Map<String, dynamic> json) {
    return PhysicalMetrics(
      height: json['height'],
      weight: json['weight'],
      bodyFat: json['body_fat'],
      restingHr: json['resting_hr'],
      sleepQualityAvg: json['sleep_quality_avg'],
    );
  }

  Map<String, dynamic> toJson() {
    return {
      'height': height,
      'weight': weight,
      'body_fat': bodyFat,
      'resting_hr': restingHr,
      'sleep_quality_avg': sleepQualityAvg,
    };
  }
}
-e 

-e 
================================================================================
üìÑ FICHIER : frontend/lib/models/generated/profile_audit.dart
================================================================================
// G√âN√âR√â AUTOMATIQUEMENT
// Timestamp : 2026-01-16T20:28:24.621806



class ProfileAudit {
  final dynamic markdownReport;

  ProfileAudit({
    required this.markdownReport
  });

  factory ProfileAudit.fromJson(Map<String, dynamic> json) {
    return ProfileAudit(
      markdownReport: json['markdown_report'],
    );
  }

  Map<String, dynamic> toJson() {
    return {
      'markdown_report': markdownReport,
    };
  }
}
-e 

-e 
================================================================================
üìÑ FICHIER : frontend/lib/models/generated/sport_context.dart
================================================================================
// G√âN√âR√â AUTOMATIQUEMENT
// Timestamp : 2026-01-16T20:28:24.607969

import 'enums.dart';

class SportContext {
  final SportType sport;
  final dynamic position;
  final dynamic level;
  final List<EquipmentType> equipment;

  SportContext({
    required this.sport, required this.position, required this.level, required this.equipment
  });

  factory SportContext.fromJson(Map<String, dynamic> json) {
    return SportContext(
      sport: json['sport'] != null ? SportType.fromJson(json['sport']) : SportType.values.first,
      position: json['position'],
      level: json['level'],
      equipment: json['equipment'] ?? [],
    );
  }

  Map<String, dynamic> toJson() {
    return {
      'sport': sport.toJson(),
      'position': position,
      'level': level,
      'equipment': equipment,
    };
  }
}
-e 

-e 
================================================================================
üìÑ FICHIER : frontend/lib/models/generated/strategy.dart
================================================================================
// G√âN√âR√â AUTOMATIQUEMENT
// Timestamp : 2026-01-16T20:28:24.621349



class Strategy {
  final dynamic periodizationTitle;
  final List<dynamic> phases;

  Strategy({
    required this.periodizationTitle, required this.phases
  });

  factory Strategy.fromJson(Map<String, dynamic> json) {
    return Strategy(
      periodizationTitle: json['periodization_title'],
      phases: json['phases'] ?? [],
    );
  }

  Map<String, dynamic> toJson() {
    return {
      'periodization_title': periodizationTitle,
      'phases': phases,
    };
  }
}
-e 

-e 
================================================================================
üìÑ FICHIER : frontend/lib/models/generated/token.dart
================================================================================
// G√âN√âR√â AUTOMATIQUEMENT
// Timestamp : 2026-01-16T20:28:24.600521



class Token {
  final dynamic accessToken;
  final dynamic tokenType;

  Token({
    required this.accessToken, required this.tokenType
  });

  factory Token.fromJson(Map<String, dynamic> json) {
    return Token(
      accessToken: json['access_token'],
      tokenType: json['token_type'],
    );
  }

  Map<String, dynamic> toJson() {
    return {
      'access_token': accessToken,
      'token_type': tokenType,
    };
  }
}
-e 

-e 
================================================================================
üìÑ FICHIER : frontend/lib/models/generated/training_preferences.dart
================================================================================
// G√âN√âR√â AUTOMATIQUEMENT
// Timestamp : 2026-01-16T20:28:24.608890



class TrainingPreferences {
  final List<dynamic> daysAvailable;
  final dynamic durationMin;
  final dynamic preferredSplit;

  TrainingPreferences({
    required this.daysAvailable, required this.durationMin, required this.preferredSplit
  });

  factory TrainingPreferences.fromJson(Map<String, dynamic> json) {
    return TrainingPreferences(
      daysAvailable: json['days_available'] ?? [],
      durationMin: json['duration_min'],
      preferredSplit: json['preferred_split'],
    );
  }

  Map<String, dynamic> toJson() {
    return {
      'days_available': daysAvailable,
      'duration_min': durationMin,
      'preferred_split': preferredSplit,
    };
  }
}
-e 

-e 
================================================================================
üìÑ FICHIER : frontend/lib/models/generated/user.dart
================================================================================
// G√âN√âR√â AUTOMATIQUEMENT
// Timestamp : 2026-01-16T20:28:24.599658



class User {
  final dynamic id;
  final dynamic username;
  final dynamic email;
  final dynamic profileData;

  User({
    required this.id, required this.username, required this.email, required this.profileData
  });

  factory User.fromJson(Map<String, dynamic> json) {
    return User(
      id: json['id'],
      username: json['username'],
      email: json['email'],
      profileData: json['profile_data'],
    );
  }

  Map<String, dynamic> toJson() {
    return {
      'id': id,
      'username': username,
      'email': email,
      'profile_data': profileData,
    };
  }
}
-e 

-e 
================================================================================
üìÑ FICHIER : frontend/lib/models/generated/weekly_plan.dart
================================================================================
// G√âN√âR√â AUTOMATIQUEMENT
// Timestamp : 2026-01-16T20:28:24.620855



class WeeklyPlan {
  final List<dynamic> schedule;
  final dynamic reasoning;

  WeeklyPlan({
    required this.schedule, required this.reasoning
  });

  factory WeeklyPlan.fromJson(Map<String, dynamic> json) {
    return WeeklyPlan(
      schedule: json['schedule'] ?? [],
      reasoning: json['reasoning'],
    );
  }

  Map<String, dynamic> toJson() {
    return {
      'schedule': schedule,
      'reasoning': reasoning,
    };
  }
}
-e 

-e 
================================================================================
üìÑ FICHIER : frontend/lib/models/generated/workout_session.dart
================================================================================
// G√âN√âR√â AUTOMATIQUEMENT
// Timestamp : 2026-01-16T20:28:24.615950

import 'workout_set.dart';

class WorkoutSession {
  final dynamic date;
  final dynamic duration;
  final dynamic rpe;
  final dynamic energyLevel;
  final dynamic notes;
  final dynamic aiAnalysis;
  final List<WorkoutSet> sets;
  final dynamic id;

  WorkoutSession({
    required this.date, required this.duration, required this.rpe, required this.energyLevel, required this.notes, required this.aiAnalysis, required this.sets, required this.id
  });

  factory WorkoutSession.fromJson(Map<String, dynamic> json) {
    return WorkoutSession(
      date: json['date'],
      duration: json['duration'],
      rpe: json['rpe'],
      energyLevel: json['energy_level'],
      notes: json['notes'],
      aiAnalysis: json['ai_analysis'],
      sets: (json['sets'] as List?)?.map((e) => WorkoutSet.fromJson(e)).toList() ?? [],
      id: json['id'],
    );
  }

  Map<String, dynamic> toJson() {
    return {
      'date': date,
      'duration': duration,
      'rpe': rpe,
      'energy_level': energyLevel,
      'notes': notes,
      'ai_analysis': aiAnalysis,
      'sets': sets.map((e) => e.toJson()).toList(),
      'id': id,
    };
  }
}
-e 

-e 
================================================================================
üìÑ FICHIER : frontend/lib/models/generated/workout_set.dart
================================================================================
// G√âN√âR√â AUTOMATIQUEMENT
// Timestamp : 2026-01-16T20:28:24.616957



class WorkoutSet {
  final dynamic exerciseName;
  final dynamic setOrder;
  final dynamic weight;
  final dynamic reps;
  final dynamic rpe;
  final dynamic restSeconds;
  final dynamic metricType;
  final dynamic id;

  WorkoutSet({
    required this.exerciseName, required this.setOrder, required this.weight, required this.reps, required this.rpe, required this.restSeconds, required this.metricType, required this.id
  });

  factory WorkoutSet.fromJson(Map<String, dynamic> json) {
    return WorkoutSet(
      exerciseName: json['exercise_name'],
      setOrder: json['set_order'],
      weight: json['weight'],
      reps: json['reps'],
      rpe: json['rpe'],
      restSeconds: json['rest_seconds'],
      metricType: json['metric_type'],
      id: json['id'],
    );
  }

  Map<String, dynamic> toJson() {
    return {
      'exercise_name': exerciseName,
      'set_order': setOrder,
      'weight': weight,
      'reps': reps,
      'rpe': rpe,
      'rest_seconds': restSeconds,
      'metric_type': metricType,
      'id': id,
    };
  }
}
-e 


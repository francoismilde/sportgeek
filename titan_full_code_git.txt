-e 
================================================================================
üìÑ FICHIER : README.md
================================================================================

-e 

-e 
================================================================================
üìÑ FICHIER : backend/add_safe_cache.py
================================================================================
import re

with open('app/routers/coach.py', 'r') as f:
    content = f.read()

# Trouver generate_workout et ajouter un d√©corateur safe
func_pattern = r'async def generate_workout\([\s\S]*?\):'
match = re.search(func_pattern, content)

if match:
    func_start = match.start()
    
    # Ajouter le d√©corateur safe (ignore current_user)
    decorated_func = '@cached_response_fixed(ttl_hours=6, ignore_args=["current_user"])\n' + match.group(0)
    
    new_content = content[:func_start] + decorated_func + content[func_start + len(match.group(0)):]
    
    with open('app/routers/coach.py', 'w') as f:
        f.write(new_content)
    
    print("‚úÖ D√©corateur safe ajout√© √† generate_workout")
else:
    print("‚ö†Ô∏è  Impossible de trouver generate_workout")
-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/__init__.py
================================================================================
-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/core/__init__.py
================================================================================
-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/core/cache.py
================================================================================
"""
Cache intelligent pour les appels Gemini IA.
√âconomise les co√ªts API et am√©liore la r√©activit√©.
"""
import json
import hashlib
from datetime import datetime, timedelta
from typing import Any, Optional
from functools import wraps
import logging

logger = logging.getLogger(__name__)

class IntelligentCache:
    """Cache m√©moire avec expiration et invalidation intelligente."""
    
    def __init__(self, default_ttl_hours: int = 24):
        self._cache = {}
        self.default_ttl = default_ttl_hours
    
    def _generate_key(self, *args, **kwargs) -> str:
        """G√©n√®re une cl√© unique √† partir des param√®tres."""
        data = json.dumps({
            'args': args,
            'kwargs': kwargs
        }, sort_keys=True)
        return hashlib.md5(data.encode()).hexdigest()
    
    def get(self, key: str) -> Optional[Any]:
        """R√©cup√®re un √©l√©ment du cache s'il est valide."""
        if key in self._cache:
            entry = self._cache[key]
            if datetime.now() < entry['expires_at']:
                logger.debug(f"üì¶ Cache hit: {key}")
                return entry['data']
            else:
                del self._cache[key]
                logger.debug(f"üßπ Cache expired: {key}")
        return None
    
    def set(self, key: str, data: Any, ttl_hours: Optional[int] = None):
        """Stocke un √©l√©ment dans le cache."""
        ttl = ttl_hours if ttl_hours is not None else self.default_ttl
        self._cache[key] = {
            'data': data,
            'expires_at': datetime.now() + timedelta(hours=ttl),
            'created_at': datetime.now()
        }
        logger.debug(f"üíæ Cache stored: {key} (TTL: {ttl}h)")
    
    def clear_old_entries(self):
        """Nettoie les entr√©es expir√©es."""
        now = datetime.now()
        expired_keys = [
            k for k, v in self._cache.items() 
            if now >= v['expires_at']
        ]
        for k in expired_keys:
            del self._cache[k]
        if expired_keys:
            logger.info(f"üßπ Cache cleanup: {len(expired_keys)} entr√©es expir√©es")

# Instance globale
ai_cache = IntelligentCache(default_ttl_hours=6)  # 6h pour les plans IA

def cached_response(ttl_hours: int = 6):
    """D√©corateur pour mettre en cache les r√©ponses IA."""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # G√©n√©rer une cl√© unique
            cache_key = f"{func.__name__}:{ai_cache._generate_key(*args, **kwargs)}"
            
            # V√©rifier le cache
            cached = ai_cache.get(cache_key)
            if cached is not None:
                return cached
            
            # Ex√©cuter la fonction
            result = await func(*args, **kwargs)
            
            # Mettre en cache
            if result is not None:
                ai_cache.set(cache_key, result, ttl_hours)
            
            return result
        return wrapper
    return decorator
-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/core/cache_fixed.py
================================================================================
"""
Version corrig√©e du d√©corateur de cache qui ignore les objets non s√©rialisables
"""

import json
import hashlib
from datetime import datetime, timedelta
from typing import Any, Optional, Callable
from functools import wraps
import logging

logger = logging.getLogger(__name__)

class FixedIntelligentCache:
    """Cache m√©moire avec gestion des objets non s√©rialisables."""
    
    def __init__(self, default_ttl_hours: int = 24):
        self._cache = {}
        self.default_ttl = default_ttl_hours
    
    def _safe_serialize(self, obj: Any) -> Any:
        """S√©rialise en toute s√©curit√©, convertissant les objets SQLAlchemy en dict."""
        if hasattr(obj, '__dict__'):
            # Si c'est un mod√®le SQLAlchemy, on prend son ID
            if hasattr(obj, 'id'):
                return f"{obj.__class__.__name__}:{obj.id}"
            # Sinon, on convertit en dict sans les relations
            return {k: self._safe_serialize(v) for k, v in obj.__dict__.items() 
                    if not k.startswith('_')}
        elif isinstance(obj, (list, tuple)):
            return [self._safe_serialize(item) for item in obj]
        elif isinstance(obj, dict):
            return {k: self._safe_serialize(v) for k, v in obj.items()}
        else:
            return obj
    
    def _generate_key(self, *args, **kwargs) -> str:
        """G√©n√®re une cl√© unique en s√©rialisant en toute s√©curit√©."""
        safe_args = self._safe_serialize(args)
        safe_kwargs = self._safe_serialize(kwargs)
        
        data = json.dumps({
            'args': safe_args,
            'kwargs': safe_kwargs
        }, sort_keys=True, default=str)
        return hashlib.md5(data.encode()).hexdigest()
    
    def get(self, key: str) -> Optional[Any]:
        """R√©cup√®re un √©l√©ment du cache s'il est valide."""
        if key in self._cache:
            entry = self._cache[key]
            if datetime.now() < entry['expires_at']:
                logger.debug(f"üì¶ Cache hit: {key}")
                return entry['data']
            else:
                del self._cache[key]
                logger.debug(f"üßπ Cache expired: {key}")
        return None
    
    def set(self, key: str, data: Any, ttl_hours: Optional[int] = None):
        """Stocke un √©l√©ment dans le cache."""
        ttl = ttl_hours if ttl_hours is not None else self.default_ttl
        self._cache[key] = {
            'data': data,
            'expires_at': datetime.now() + timedelta(hours=ttl),
            'created_at': datetime.now()
        }
        logger.debug(f"üíæ Cache stored: {key} (TTL: {ttl}h)")
    
    def clear_old_entries(self):
        """Nettoie les entr√©es expir√©es."""
        now = datetime.now()
        expired_keys = [
            k for k, v in self._cache.items() 
            if now >= v['expires_at']
        ]
        for k in expired_keys:
            del self._cache[k]
        if expired_keys:
            logger.info(f"üßπ Cache cleanup: {len(expired_keys)} entr√©es expir√©es")

# Instance globale
ai_cache_fixed = FixedIntelligentCache(default_ttl_hours=6)

def cached_response_fixed(ttl_hours: int = 6, ignore_args: list = None):
    """
    D√©corateur corrig√© pour mettre en cache les r√©ponses IA.
    ignore_args: liste des noms d'arguments √† ignorer (ex: ['current_user'])
    """
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # Cr√©er une copie des kwargs pour la g√©n√©ration de cl√©
            cache_kwargs = kwargs.copy()
            
            # Ignorer les arguments sp√©cifi√©s
            if ignore_args:
                for arg_name in ignore_args:
                    cache_kwargs.pop(arg_name, None)
            
            # G√©n√©rer une cl√© unique (sans les arguments ignor√©s)
            cache_key = f"{func.__name__}:{ai_cache_fixed._generate_key(*args, **cache_kwargs)}"
            
            # V√©rifier le cache
            cached = ai_cache_fixed.get(cache_key)
            if cached is not None:
                return cached
            
            # Ex√©cuter la fonction
            result = await func(*args, **kwargs)
            
            # Mettre en cache
            if result is not None:
                ai_cache_fixed.set(cache_key, result, ttl_hours)
            
            return result
        return wrapper
    return decorator
-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/core/database.py
================================================================================
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
import os
from dotenv import load_dotenv

load_dotenv()

# 1. On r√©cup√®re l'URL
SQLALCHEMY_DATABASE_URL = os.getenv("DATABASE_URL")

# 2. S√©curit√© : si pas d'URL, on met du SQLite temporaire
if not SQLALCHEMY_DATABASE_URL:
    print("‚ö†Ô∏è DATABASE_URL absente. Mode SQLite temporaire.")
    SQLALCHEMY_DATABASE_URL = "sqlite:///./sql_app.db"

# 3. Correctif pour l'URL (postgres -> postgresql)
if SQLALCHEMY_DATABASE_URL.startswith("postgres://"):
    SQLALCHEMY_DATABASE_URL = SQLALCHEMY_DATABASE_URL.replace("postgres://", "postgresql://", 1)

# 4. Cr√©ation du moteur (Version Simplifi√©e pour √©viter les Timeouts)
connect_args = {}
if "sqlite" in SQLALCHEMY_DATABASE_URL:
    connect_args = {"check_same_thread": False}

engine = create_engine(
    SQLALCHEMY_DATABASE_URL,
    connect_args=connect_args
    # On a retir√© pool_pre_ping pour tester la connexion brute
)

SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()

def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/core/security.py
================================================================================
from datetime import datetime, timedelta
from typing import Optional
from jose import jwt
from passlib.context import CryptContext
import os
from dotenv import load_dotenv

load_dotenv()

# Configuration
SECRET_KEY = os.getenv("SECRET_KEY", "fallback_secret_key_if_env_missing")
ALGORITHM = os.getenv("ALGORITHM", "HS256")
ACCESS_TOKEN_EXPIRE_MINUTES = int(os.getenv("ACCESS_TOKEN_EXPIRE_MINUTES", 30))

pwd_context = CryptContext(
    schemes=["bcrypt"], 
    deprecated="auto",
    bcrypt__ident="2b"
)

def get_password_hash(password: str) -> str:
    """Transforme un mot de passe en clair en hash s√©curis√©."""
    return pwd_context.hash(password)

def verify_password(plain_password: str, hashed_password: str) -> bool:
    """V√©rifie si le mot de passe correspond au hash."""
    return pwd_context.verify(plain_password, hashed_password)

def create_access_token(data: dict, expires_delta: Optional[timedelta] = None):
    """G√©n√®re un Token JWT sign√©."""
    to_encode = data.copy()
    
    if expires_delta:
        expire = datetime.utcnow() + expires_delta
    else:
        expire = datetime.utcnow() + timedelta(minutes=15)
        
    to_encode.update({"exp": expire})
    
    encoded_jwt = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)
    return encoded_jwt-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/dependencies.py
================================================================================
from fastapi import Depends, HTTPException, status
from fastapi.security import OAuth2PasswordBearer
from jose import JWTError, jwt
from sqlalchemy.orm import Session
from app.core.database import get_db
from app.core import security
from app.models import sql_models, schemas

# C'est ici qu'on dit √† FastAPI o√π aller chercher le token si on ne l'a pas
oauth2_scheme = OAuth2PasswordBearer(tokenUrl="auth/token")

async def get_current_user(token: str = Depends(oauth2_scheme), db: Session = Depends(get_db)):
    """
    Cette fonction est un 'Dependency'. 
    Elle sera appel√©e avant chaque route prot√©g√©e.
    1. Elle r√©cup√®re le token.
    2. Elle le d√©code.
    3. Elle v√©rifie si l'utilisateur existe en BDD.
    """
    credentials_exception = HTTPException(
        status_code=status.HTTP_401_UNAUTHORIZED,
        detail="Impossible de valider les identifiants",
        headers={"WWW-Authenticate": "Bearer"},
    )
    
    try:
        # D√©codage du token
        payload = jwt.decode(token, security.SECRET_KEY, algorithms=[security.ALGORITHM])
        username: str = payload.get("sub")
        if username is None:
            raise credentials_exception
    except JWTError:
        raise credentials_exception
        
    # Recherche de l'utilisateur en BDD
    user = db.query(sql_models.User).filter(sql_models.User.username == username).first()
    if user is None:
        raise credentials_exception
        
    return user-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/domain/__init__.py
================================================================================
-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/domain/bioenergetics.py
================================================================================
import math
from typing import Dict, Any, List

class BioenergeticService:
    """
    Service de calcul physiologique (Bio-Twin v1).
    Estime la d√©pense √©nerg√©tique et les besoins nutritionnels post-effort.
    Ne d√©pend PAS de l'IA, mais de formules m√©taboliques.
    """

    @staticmethod
    def calculate_needs(profile_data: Dict[str, Any], workout_sets: List[Any], duration_min: float, rpe: float) -> Dict[str, Any]:
        """
        Calcule les KPIs physiologiques de la s√©ance.
        """
        # 1. Extraction du profil (valeurs par d√©faut de s√©curit√©)
        weight = float(profile_data.get('weight', 70.0))
        if weight <= 0: weight = 70.0
        
        gender = profile_data.get('gender', 'Homme')
        
        # 2. Estimation de la D√©pense √ânerg√©tique (Kcal)
        # M√©thode A : Pr√©cise si Watts disponibles
        total_work_kj = 0.0
        has_power_data = False
        
        for s in workout_sets:
            if s.metric_type == 'POWER_TIME':
                # Watts * secondes / 1000 = kJ
                # weight = watts, reps = duration(s) dans ce mode (selon schemas.py)
                # Mais attention, le frontend envoie parfois des minutes converties.
                # Dans sql_models/schemas, on a standardis√© : weight=Watts, reps=Secondes (via validateur polymorphique)
                watts = s.weight
                seconds = s.reps 
                total_work_kj += (watts * seconds) / 1000.0
                has_power_data = True
        
        kcal_burn = 0.0
        
        if has_power_data:
            # Rendement m√©canique humain ~20-25% => x4 √† x5 pour passer de kJ m√©canique √† kcal m√©tabolique
            # Formule standard: kJ * 1.1 est une approx basse, kJ / 4.18 * 4 (rendement) est mieux.
            # Simplification robuste : kJ m√©canique * 1.0 = Kcal m√©tabolique (approx tr√®s courante en cyclisme)
            kcal_burn = total_work_kj * 1.0 
            # Si on ajoute le m√©tabolisme de base pendant la dur√©e... Restons sur l'activit√© pure.
        else:
            # M√©thode B : Estimation METs (Metabolic Equivalent of Task)
            # RPE 1-3 (Repos/Recup) : 3 METs
            # RPE 4-6 (Endurance) : 6 METs
            # RPE 7-8 (Seuil) : 9 METs
            # RPE 9-10 (Max) : 12 METs
            mets = 3.0
            if rpe > 8: mets = 11.0
            elif rpe > 6: mets = 9.0
            elif rpe > 4: mets = 6.0
            else: mets = 3.5
            
            # Formule : Kcal = METs * Poids(kg) * Dur√©e(h)
            duration_hours = duration_min / 60.0
            kcal_burn = mets * weight * duration_hours

        # 3. Partition Macro-nutritionnelle (Fili√®res √©nerg√©tiques)
        # Ratio Glucides/Lipides d√©pend de l'intensit√© relative
        # RPE √©lev√© -> Glycolytique -> Besoin Glucides
        carbs_ratio = 0.5 # 50% par d√©faut
        
        if rpe >= 8: carbs_ratio = 0.8  # 80% glucides
        elif rpe >= 6: carbs_ratio = 0.6 # 60% glucides
        elif rpe <= 4: carbs_ratio = 0.3 # 30% glucides (LIPOX max)
        
        kcal_carbs = kcal_burn * carbs_ratio
        carbs_g = kcal_carbs / 4.0 # 4 kcal/g
        
        # 4. Prot√©ines (R√©paration tissulaire)
        # Base : 0.3g / kg de poids de corps apr√®s une s√©ance standard
        # Boost si s√©ance de force (RPE > 7)
        protein_factor = 0.25
        if rpe > 7: protein_factor = 0.35
        
        protein_g = weight * protein_factor
        
        # 5. Hydratation (Estimation sudation standard)
        # ~10ml / min / kg est trop. 
        # Standard : 0.5L √† 1L par heure selon intensit√©.
        sweat_rate_ml_h = 500
        if rpe > 7: sweat_rate_ml_h = 800
        water_ml = (duration_min / 60.0) * sweat_rate_ml_h

        return {
            "kcal_total": int(kcal_burn),
            "carbs_g": int(carbs_g),
            "protein_g": int(protein_g),
            "water_ml": int(water_ml),
            "source": "Wattmeter" if has_power_data else "METs Estimator"
        }-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/domain/calculations.py
================================================================================
import math
from abc import ABC, abstractmethod

# --- STRATEGY PATTERN : MOTEUR 1RM ---

class OneRepMaxStrategy(ABC):
    """Interface abstraite pour les strat√©gies de calcul du 1RM."""
    @abstractmethod
    def calculate(self, weight: float, reps: int) -> float:
        pass

    @property
    @abstractmethod
    def name(self) -> str:
        pass

class EpleyStrategy(OneRepMaxStrategy):
    """Formule d'Epley : W * (1 + r/30)"""
    def calculate(self, weight: float, reps: int) -> float:
        return weight * (1 + reps / 30.0)

    @property
    def name(self) -> str:
        return "Epley"

class BrzyckiStrategy(OneRepMaxStrategy):
    """Formule de Brzycki : W * (36 / (37 - r))"""
    def calculate(self, weight: float, reps: int) -> float:
        if reps >= 37: return 0.0
        return weight * (36.0 / (37.0 - reps))

    @property
    def name(self) -> str:
        return "Brzycki"

class WathanStrategy(OneRepMaxStrategy):
    """Formule de Wathan : Exponentielle"""
    def calculate(self, weight: float, reps: int) -> float:
        denominator = 48.8 + (53.8 * math.exp(-0.075 * reps))
        if denominator == 0: return 0.0
        return (100.0 * weight) / denominator

    @property
    def name(self) -> str:
        return "Wathan"

class OneRepMaxCalculator:
    """Factory : S√©lectionne la bonne strat√©gie selon le nombre de r√©p√©titions."""
    @staticmethod
    def get_strategy(reps: int) -> OneRepMaxStrategy:
        if reps <= 5:
            return EpleyStrategy()
        elif reps <= 10:
            return BrzyckiStrategy()
        else:
            return WathanStrategy()

def calculate_1rm(weight: float, reps: int) -> dict:
    """
    Fonction principale expos√©e au reste de l'app.
    """
    if weight <= 0 or reps <= 0:
        return {"1rm": 0.0, "method": "N/A"}
    
    if reps == 1:
        return {"1rm": weight, "method": "Actual Lift"}
        
    if reps > 30:
        return {"1rm": 0.0, "method": "Out of Range (>30)"}

    strategy = OneRepMaxCalculator.get_strategy(reps)
    one_rm_val = strategy.calculate(weight, reps)
    
    # Arrondi au 0.5kg le plus proche
    final_val = round(one_rm_val * 2) / 2

    return {
        "1rm": final_val,
        "method": strategy.name
    }-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/domain/safety.py
================================================================================
import pandas as pd
from datetime import date, timedelta
import re

def _safe_float(val):
    """Helper pour convertir n'importe quoi en float."""
    if val is None: return 0.0
    try:
        clean = str(val).replace(',', '.').strip()
        match = re.search(r"[-+]?\d*\.\d+|\d+", clean)
        return float(match.group()) if match else 0.0
    except:
        return 0.0

def calculate_acwr(history_logs: list) -> dict:
    """
    Calcule le Ratio Aigu/Chronique (ACWR).
    Entr√©e : Liste de dictionnaires (date, duration, rpe).
    Sortie : Dict avec ratio, status, charges.
    """
    default_res = {
        "ratio": 0.0,
        "status": "Inactif",
        "color": "gray",
        "acute_load": 0,
        "chronic_load": 0,
        "message": "Pas assez de donn√©es."
    }
    
    if not history_logs:
        return default_res

    try:
        # 1. Cr√©ation DataFrame
        df = pd.DataFrame(history_logs)
        
        # Conversion et tri des dates
        df['date_dt'] = pd.to_datetime(df['date'], errors='coerce').dt.floor('D')
        df = df.dropna(subset=['date_dt']).sort_values('date_dt')
        
        if df.empty:
            return default_res

        # 2. Calcul de la charge (Load = Dur√©e * RPE)
        # On s√©curise les valeurs
        df['duration'] = df['duration'].apply(_safe_float)
        df['rpe'] = df['rpe'].apply(_safe_float)
        df['load'] = df['duration'] * df['rpe']
        
        # Agr√©gation par jour (si plusieurs s√©ances le m√™me jour)
        daily_loads = df.groupby('date_dt')['load'].sum()
        
        # 3. Timeline Continue (J-27 √† Aujourd'hui)
        end_date = pd.Timestamp.now().floor('D')
        idx_ref = pd.date_range(end=end_date, periods=28, freq='D')
        
        # On remplit les trous avec 0
        timeline = daily_loads.reindex(idx_ref, fill_value=0)
        
        # 4. Calculs Fen√™tres Glissantes
        acute_avg = timeline.tail(7).mean()   # Fatigue (7j)
        chronic_avg = timeline.mean()         # Forme (28j)
        
        # 5. Ratio
        ratio = 0.0
        if chronic_avg > 0:
            ratio = acute_avg / chronic_avg
        elif acute_avg > 0:
            ratio = 2.0 # Reprise brutale
            
        # 6. Diagnostic
        ratio = round(ratio, 2)
        status, color, msg = "Inactif", "gray", "Reprends progressivement."
        
        if ratio > 0:
            if ratio <= 0.80:
                status, color, msg = "Sous-entra√Ænement", "blue", "Charge faible."
            elif 0.80 < ratio <= 1.30:
                status, color, msg = "Optimal", "green", "Zone de progression."
            elif 1.30 < ratio <= 1.50:
                status, color, msg = "Surcharge", "orange", "Attention fatigue."
            else:
                status, color, msg = "DANGER", "red", "Pic de charge critique (>1.5)."

        return {
            "ratio": ratio,
            "status": status,
            "color": color,
            "acute_load": int(acute_avg),
            "chronic_load": int(chronic_avg),
            "message": msg
        }
        
    except Exception as e:
        print(f"Erreur ACWR: {e}")
        return default_res-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/jobs/daily_coach_memory_update.py
================================================================================
"""
Job quotidien pour mettre √† jour les m√©moires du coach
Ex√©cut√© automatiquement √† 02:00 chaque jour
"""
import logging
import asyncio
from datetime import datetime, timedelta
from sqlalchemy.orm import Session
from sqlalchemy import and_

from app.core.database import SessionLocal
from app.models import sql_models
from app.services.coach_memory.service import CoachMemoryService

logger = logging.getLogger(__name__)

async def daily_coach_memory_update():
    """
    Met √† jour toutes les m√©moires du coach quotidiennement
    """
    logger.info("üöÄ D√©marrage du job quotidien de mise √† jour des m√©moires du coach")
    
    db = SessionLocal()
    try:
        # R√©cup√©rer toutes les m√©moires actives
        coach_memories = db.query(sql_models.CoachMemory).all()
        
        logger.info(f"üìä {len(coach_memories)} m√©moires √† mettre √† jour")
        
        updated_count = 0
        error_count = 0
        
        for memory in coach_memories:
            try:
                # R√©cup√©rer le profil associ√©
                athlete_profile = db.query(sql_models.AthleteProfile).filter(
                    sql_models.AthleteProfile.id == memory.athlete_profile_id
                ).first()
                
                if not athlete_profile:
                    logger.warning(f"Profil non trouv√© pour la m√©moire {memory.id}")
                    continue
                
                # Mettre √† jour le contexte avec des valeurs par d√©faut
                default_checkin = {
                    "sleep_quality": 7,
                    "sleep_duration": 7.5,
                    "perceived_stress": 5,
                    "muscle_soreness": 3,
                    "energy_level": 7
                }
                
                # Mettre √† jour le contexte
                CoachMemoryService.update_daily_context(memory, default_checkin, db)
                
                # Mettre √† jour les m√©tadonn√©es
                metadata = json.loads(memory.metadata) if memory.metadata else {}
                metadata['last_daily_update'] = datetime.utcnow().isoformat()
                metadata['total_updates'] = metadata.get('total_updates', 0) + 1
                memory.metadata = json.dumps(metadata)
                
                updated_count += 1
                
                # Log tous les 10 profils
                if updated_count % 10 == 0:
                    logger.info(f"‚úÖ {updated_count} m√©moires mises √† jour")
                
            except Exception as e:
                error_count += 1
                logger.error(f"‚ùå Erreur mise √† jour m√©moire {memory.id}: {str(e)}")
                continue
        
        db.commit()
        
        logger.info(f"üéâ Job termin√©: {updated_count} mises √† jour, {error_count} erreurs")
        
        # G√©n√©rer un rapport
        report = {
            "timestamp": datetime.now().isoformat(),
            "total_memories": len(coach_memories),
            "updated": updated_count,
            "errors": error_count,
            "success_rate": (updated_count / len(coach_memories) * 100) if coach_memories else 100
        }
        
        logger.info(f"üìà Rapport: {report}")
        
        return report
        
    except Exception as e:
        logger.error(f"üí• Erreur critique dans le job quotidien: {str(e)}")
        db.rollback()
        raise
    finally:
        db.close()

async def update_memory_flags_batch():
    """
    Met √† jour les flags de m√©moire en batch
    """
    logger.info("üöÄ Mise √† jour batch des flags de m√©moire")
    
    db = SessionLocal()
    try:
        # R√©cup√©rer les m√©moires avec contexte r√©cent
        one_day_ago = datetime.utcnow() - timedelta(days=1)
        
        coach_memories = db.query(sql_models.CoachMemory).filter(
            sql_models.CoachMemory.last_updated >= one_day_ago
        ).all()
        
        for memory in coach_memories:
            try:
                context = json.loads(memory.current_context) if memory.current_context else {}
                readiness = context.get('readiness_score', 70)
                
                memory_flags = json.loads(memory.memory_flags) if memory.memory_flags else {}
                
                # Mettre √† jour les flags bas√©s sur le contexte
                memory_flags['needs_deload'] = readiness < 40
                memory_flags['adaptation_window_open'] = readiness > 70
                memory_flags['pr_potential'] = readiness > 80 and context.get('fatigue_state') == 'fresh'
                
                memory.memory_flags = json.dumps(memory_flags)
                
            except Exception as e:
                logger.error(f"‚ùå Erreur mise √† jour flags m√©moire {memory.id}: {str(e)}")
                continue
        
        db.commit()
        logger.info(f"‚úÖ Flags mis √† jour pour {len(coach_memories)} m√©moires")
        
    except Exception as e:
        logger.error(f"üí• Erreur batch flags: {str(e)}")
        db.rollback()
    finally:
        db.close()

async def cleanup_old_data():
    """
    Nettoie les donn√©es anciennes
    """
    logger.info("üßπ Nettoyage des donn√©es anciennes")
    
    db = SessionLocal()
    try:
        # Supprimer les profils incomplets de plus de 30 jours
        thirty_days_ago = datetime.utcnow() - timedelta(days=30)
        
        incomplete_profiles = db.query(sql_models.AthleteProfile).filter(
            and_(
                sql_models.AthleteProfile.is_complete == False,
                sql_models.AthleteProfile.created_at < thirty_days_ago
            )
        ).all()
        
        deleted_count = 0
        for profile in incomplete_profiles:
            try:
                db.delete(profile)
                deleted_count += 1
            except Exception as e:
                logger.error(f"‚ùå Erreur suppression profil {profile.id}: {str(e)}")
        
        db.commit()
        logger.info(f"üóëÔ∏è  {deleted_count} profils incomplets supprim√©s")
        
    except Exception as e:
        logger.error(f"üí• Erreur nettoyage: {str(e)}")
        db.rollback()
    finally:
        db.close()

if __name__ == "__main__":
    # Pour ex√©cution manuelle
    import asyncio
    import json
    
    async def main():
        logger.info("üß™ Ex√©cution manuelle du job quotidien")
        
        # Ex√©cuter les t√¢ches
        report = await daily_coach_memory_update()
        await update_memory_flags_batch()
        await cleanup_old_data()
        
        print(json.dumps(report, indent=2))
    
    asyncio.run(main())
-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/main.py
================================================================================
from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
import logging
from sqlalchemy import text 

# --- IMPORTS DES ROUTEURS ---
# [MODIFICATION] Ajout de 'feed'
from .routers import performance, safety, auth, workouts, coach, user, feed, profiles, athlete_profiles, coach_memories
from app.core.database import engine, Base

# Configuration des logs
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --- DATABASE INIT ---
try:
    logger.info("Tentative de cr√©ation des tables SQL...")
    Base.metadata.create_all(bind=engine)
    logger.info("Tables v√©rifi√©es/cr√©√©es.")
except Exception as e:
    logger.error(f"ERREUR CRITIQUE D√âMARRAGE DB : {e}")

app = FastAPI(
    title="TitanFlow API",
    description="API Backend pour l'application TitanFlow",
    version="2.0.0", # Neural Feed v2 - AthleteProfile version="1.9.4", # Bump Neural Feed CoachMemory
    docs_url="/docs",
    redoc_url="/redoc"
)

# Configuration CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- GLOBAL EXCEPTION HANDLER (ANTI-CRASH) ---
@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    logger.error(f"üî• CRASH GLOBAL NON G√âR√â : {str(exc)}")
    return JSONResponse(
        status_code=500,
        content={"detail": f"Erreur serveur interne (TitanFlow Panic): {str(exc)}"},
        headers={
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Methods": "*",
            "Access-Control-Allow-Headers": "*",
        },
    )

# --- ROUTEURS ---
app.include_router(auth.router)
app.include_router(workouts.router)
app.include_router(performance.router)
app.include_router(safety.router)
app.include_router(coach.router)
app.include_router(user.router)
# [MODIFICATION] Activation du Feed
app.include_router(feed.router)
app.include_router(profiles.router)
app.include_router(athlete_profiles.router)
app.include_router(coach_memories.router)

# --- ROUTE SP√âCIALE DE R√âPARATION (SELF-REPAIR V4) ---
@app.get("/fix_db", tags=["System"])
def fix_database_schema():
    """
    üõ†Ô∏è ROUTE D'URGENCE V4 : Ajoute TOUTES les colonnes manquantes.
    Inclus maintenant draft_workout_data pour r√©parer le crash Login.
    """
    try:
        with engine.connect() as connection:
            trans = connection.begin()
            
            # 1. Table WORKOUT_SESSIONS (S√©ances)
            connection.execute(text("ALTER TABLE workout_sessions ADD COLUMN IF NOT EXISTS energy_level INTEGER DEFAULT 5;"))
            connection.execute(text("ALTER TABLE workout_sessions ADD COLUMN IF NOT EXISTS notes TEXT;"))
            connection.execute(text("ALTER TABLE workout_sessions ADD COLUMN IF NOT EXISTS created_at TIMESTAMPTZ DEFAULT NOW();"))
            
            # 2. Table WORKOUT_SETS (S√©ries)
            connection.execute(text("ALTER TABLE workout_sets ADD COLUMN IF NOT EXISTS metric_type VARCHAR DEFAULT 'LOAD_REPS';"))
            connection.execute(text("ALTER TABLE workout_sets ADD COLUMN IF NOT EXISTS rest_seconds INTEGER DEFAULT 0;"))
            
            # 3. Table USERS (Profil)
            connection.execute(text("ALTER TABLE users ADD COLUMN IF NOT EXISTS profile_data TEXT;"))
            
            # M√©moire IA
            connection.execute(text("ALTER TABLE users ADD COLUMN IF NOT EXISTS strategy_data TEXT;"))
            connection.execute(text("ALTER TABLE users ADD COLUMN IF NOT EXISTS weekly_plan_data TEXT;"))
            
            # [FIX CRITIQUE] Ajout du brouillon
            connection.execute(text("ALTER TABLE users ADD COLUMN IF NOT EXISTS draft_workout_data TEXT;"))
            
            trans.commit()
            return {
                "status": "SUCCESS", 
                "message": "‚úÖ Base de donn√©es r√©par√©e : Colonne 'draft_workout_data' ajout√©e !"
            }
            
    except Exception as e:
        return {"status": "ERROR", "message": f"‚ùå Erreur lors de la r√©paration : {str(e)}"}


@app.get("/health", tags=["Health Check"])
async def health_check():
    return {
        "status": "active",
        "version": "1.9.4",
        "service": "TitanFlow Backend",
        "database": "connected"
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("app.main:app", host="0.0.0.0", port=8000, reload=True)-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/models/__init__.py
================================================================================
-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/models/schemas.py
================================================================================
from pydantic import BaseModel, Field, field_validator
from typing import List, Optional, Dict, Any, Union
from datetime import date, datetime
from enum import Enum
import json

# --- ENUMS & TYPES ---
class SportType(str, Enum):
    RUGBY = "Rugby"
    FOOTBALL = "Football"
    CROSSFIT = "CrossFit"
    HYBRID = "Hybrid"
    RUNNING = "Running"
    OTHER = "Autre"

# --- SUB-SCHEMAS FOR PROFILE ---
class BasicInfo(BaseModel):
    pseudo: Optional[str] = None
    email: Optional[str] = None
    birth_date: Optional[str] = None
    training_age: Optional[int] = 0

class PhysicalMetrics(BaseModel):
    height: float = 0
    weight: float = 0
    body_fat: Optional[float] = None
    resting_hr: Optional[int] = None
    sleep_quality_avg: Optional[int] = 5

class SportContext(BaseModel):
    sport: SportType = SportType.OTHER
    position: Optional[str] = None
    level: str = "Interm√©diaire"
    equipment: List[str] = ["Standard"]

class TrainingPreferences(BaseModel):
    days_available: List[str] = []
    duration_min: int = 60
    preferred_split: str = "Upper/Lower"

# --- MAIN PROFILE SCHEMAS ---
class AthleteProfileBase(BaseModel):
    basic_info: BasicInfo = Field(default_factory=BasicInfo)
    physical_metrics: PhysicalMetrics = Field(default_factory=PhysicalMetrics)
    sport_context: SportContext = Field(default_factory=SportContext)
    training_preferences: TrainingPreferences = Field(default_factory=TrainingPreferences)
    goals: Dict[str, Any] = {}
    constraints: Dict[str, Any] = {}
    injury_prevention: Dict[str, Any] = {}
    performance_baseline: Dict[str, Any] = {}

class AthleteProfileCreate(AthleteProfileBase):
    pass

class AthleteProfileResponse(AthleteProfileBase):
    id: int
    user_id: int
    created_at: datetime
    class Config:
        from_attributes = True

# --- MEMORY SCHEMAS ---
class CoachMemoryResponse(BaseModel):
    id: int
    readiness_score: int = Field(alias="current_context", default=50)
    current_phase: str = "G√©n√©ral"
    flags: Dict[str, bool] = {}
    insights: Dict[str, Any] = {}
    
    @field_validator('readiness_score', mode='before')
    def extract_readiness(cls, v):
        if isinstance(v, dict): return v.get('readiness_score', 50)
        return v

    class Config:
        from_attributes = True

# --- LEGACY SCHEMAS ---
class WorkoutSetBase(BaseModel):
    exercise_name: str
    set_order: int
    weight: Union[float, str] = 0.0
    reps: Union[float, str] = 0.0
    rpe: Optional[float] = 0.0
    rest_seconds: int = 0
    metric_type: str = "LOAD_REPS"

    @field_validator('weight', 'reps', mode='before')
    def parse_polymorphic_fields(cls, v):
        if isinstance(v, str):
            v = v.strip().replace(',', '.')
            if ':' in v:
                parts = v.split(':')
                try:
                    seconds = 0.0
                    if len(parts) == 2:
                        seconds = float(parts[0]) * 60 + float(parts[1])
                    elif len(parts) == 3:
                        seconds = float(parts[0]) * 3600 + float(parts[1]) * 60 + float(parts[2])
                    return seconds
                except ValueError:
                    return 0.0
            try:
                return float(v)
            except ValueError:
                return 0.0
        return v

class WorkoutSetCreate(WorkoutSetBase):
    pass

class WorkoutSessionCreate(BaseModel):
    date: date
    duration: float
    rpe: float
    energy_level: int = 5
    notes: Optional[str] = None
    sets: List[WorkoutSetCreate] = []

class WorkoutSetResponse(WorkoutSetBase):
    id: int
    weight: float
    reps: float
    class Config:
        from_attributes = True

class WorkoutSessionResponse(WorkoutSessionCreate):
    id: int
    ai_analysis: Optional[str] = None
    sets: List[WorkoutSetResponse] = []
    class Config:
        from_attributes = True

class GenerateWorkoutRequest(BaseModel):
    profile_data: Dict[str, Any]
    context: Dict[str, Any]

class AIExercise(BaseModel):
    name: str
    sets: int
    reps: Union[str, int]
    rest: int
    tips: str
    recording_mode: str = "LOAD_REPS"
    @field_validator('reps')
    def force_string_reps(cls, v):
        return str(v)

class AIWorkoutPlan(BaseModel):
    title: str
    coach_comment: str
    warmup: List[str]
    exercises: List[AIExercise]
    cooldown: List[str]

class UserCreate(BaseModel):
    username: str
    email: Optional[str] = None
    password: str

class UserResponse(BaseModel):
    id: int
    username: str
    email: Optional[str] = None
    profile_data: Optional[str] = None 
    class Config:
        from_attributes = True

class Token(BaseModel):
    access_token: str
    token_type: str

class TokenData(BaseModel):
    username: Optional[str] = None

class FeedItemType(str, Enum):
    INFO = "INFO"
    ANALYSIS = "ANALYSIS"
    ACTION = "ACTION"
    ALERT = "ALERT"

class FeedItemCreate(BaseModel):
    type: FeedItemType
    title: str
    message: str
    priority: int = 1
    action_payload: Optional[Dict[str, Any]] = None

class FeedItemResponse(FeedItemCreate):
    id: str
    is_read: bool
    is_completed: bool
    created_at: datetime
    
    @field_validator('action_payload', mode='before')
    def parse_payload(cls, v):
        if isinstance(v, str) and v.strip():
            try: return json.loads(v)
            except: return None
        return v
    class Config:
        from_attributes = True

# --- PERFORMANCE ---
class OneRepMaxRequest(BaseModel):
    weight: float
    reps: int
class OneRepMaxResponse(BaseModel):
    estimated_1rm: float
    method_used: str
class ACWRRequest(BaseModel):
    history: List[Dict[str, Any]]
class ACWRResponse(BaseModel):
    ratio: float
    status: str
    color: str
    message: str
class ProfileAuditRequest(BaseModel):
    profile_data: Dict[str, Any]
class ProfileAuditResponse(BaseModel):
    markdown_report: str
class StrategyResponse(BaseModel):
    periodization_title: str
    phases: List[Any]
class WeeklyPlanResponse(BaseModel):
    schedule: List[Any]
    reasoning: str
class UserProfileUpdate(BaseModel):
    profile_data: Dict[str, Any]


# --- AUTO-INJECTED UPDATE SCHEMAS ---

class AthleteProfileUpdate(AthleteProfileBase):
    pass

class ProfileSectionUpdate(BaseModel):
    section_data: Dict[str, Any]

class DailyMetrics(BaseModel):
    date: str
    weight: Optional[float] = None
    sleep_quality: Optional[int] = None
    resting_heart_rate: Optional[int] = None
    hrv: Optional[int] = None
    energy_level: Optional[int] = None
    muscle_soreness: Optional[int] = None
    perceived_stress: Optional[int] = None
    sleep_duration: Optional[float] = None

class GoalProgressUpdate(BaseModel):
    progress_value: int
    progress_note: Optional[str] = None
    achieved: bool = False
-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/models/sql_models.py
================================================================================
from sqlalchemy import Column, Integer, String, Float, Date, ForeignKey, DateTime, Text, Boolean, JSON
from sqlalchemy.orm import relationship
from sqlalchemy.sql import func
from app.core.database import Base

class User(Base):
    __tablename__ = "users"

    id = Column(Integer, primary_key=True, index=True)
    username = Column(String, unique=True, index=True)
    email = Column(String, unique=True, index=True, nullable=True)
    hashed_password = Column(String)
    
    # Anciens champs (Legacy support)
    profile_data = Column(Text, nullable=True)
    strategy_data = Column(Text, nullable=True)
    weekly_plan_data = Column(Text, nullable=True)
    draft_workout_data = Column(Text, nullable=True)

    # Relations
    workouts = relationship("WorkoutSession", back_populates="owner")
    feed_items = relationship("FeedItem", back_populates="owner", cascade="all, delete-orphan")
    
    # [NOUVEAU] Relation vers le Profil Enrichi
    athlete_profile = relationship("AthleteProfile", back_populates="user", uselist=False, cascade="all, delete-orphan")

class AthleteProfile(Base):
    __tablename__ = "athlete_profiles"

    id = Column(Integer, primary_key=True, index=True)
    user_id = Column(Integer, ForeignKey("users.id"), unique=True)

    # Blocs de donn√©es JSONB
    basic_info = Column(JSON, default={})
    physical_metrics = Column(JSON, default={})
    sport_context = Column(JSON, default={})
    performance_baseline = Column(JSON, default={})
    injury_prevention = Column(JSON, default={})
    training_preferences = Column(JSON, default={})
    goals = Column(JSON, default={})
    constraints = Column(JSON, default={})

    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), onupdate=func.now())

    user = relationship("User", back_populates="athlete_profile")
    coach_memory = relationship("CoachMemory", back_populates="athlete_profile", uselist=False, cascade="all, delete-orphan")

class CoachMemory(Base):
    __tablename__ = "coach_memories"

    id = Column(Integer, primary_key=True, index=True)
    athlete_profile_id = Column(Integer, ForeignKey("athlete_profiles.id"), unique=True)

    # M√©moire contextuelle IA
    metadata_info = Column(JSON, default={})
    current_context = Column(JSON, default={})
    response_patterns = Column(JSON, default={})
    performance_baselines = Column(JSON, default={})
    adaptation_signals = Column(JSON, default={})
    sport_specific_insights = Column(JSON, default={})
    training_history_summary = Column(JSON, default={})
    athlete_preferences = Column(JSON, default={})
    coach_notes = Column(JSON, default={})
    memory_flags = Column(JSON, default={})

    last_updated = Column(DateTime(timezone=True), server_default=func.now())

    athlete_profile = relationship("AthleteProfile", back_populates="coach_memory")

# --- MOD√àLES EXISTANTS ---
class WorkoutSession(Base):
    __tablename__ = "workout_sessions"
    id = Column(Integer, primary_key=True, index=True)
    user_id = Column(Integer, ForeignKey("users.id"))
    date = Column(Date, index=True)
    duration = Column(Float)
    rpe = Column(Float)
    energy_level = Column(Integer, default=5) 
    notes = Column(Text, nullable=True)      
    ai_analysis = Column(Text, nullable=True)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    owner = relationship("User", back_populates="workouts")
    sets = relationship("WorkoutSet", back_populates="session", cascade="all, delete-orphan")

class WorkoutSet(Base):
    __tablename__ = "workout_sets"
    id = Column(Integer, primary_key=True, index=True)
    session_id = Column(Integer, ForeignKey("workout_sessions.id"))
    exercise_name = Column(String, index=True)
    set_order = Column(Integer)
    weight = Column(Float, default=0.0)
    reps = Column(Float, default=0.0)
    rpe = Column(Float, default=0.0)
    rest_seconds = Column(Integer, default=0)
    metric_type = Column(String, nullable=False, default="LOAD_REPS") 
    session = relationship("WorkoutSession", back_populates="sets")

class FeedItem(Base):
    __tablename__ = "feed_items"
    id = Column(String, primary_key=True, index=True)
    user_id = Column(Integer, ForeignKey("users.id"))
    type = Column(String, index=True)
    title = Column(String)
    message = Column(String)
    action_payload = Column(Text, nullable=True)
    is_read = Column(Boolean, default=False)
    is_completed = Column(Boolean, default=False)
    priority = Column(Integer, default=1)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    owner = relationship("User", back_populates="feed_items")
-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/routers/__init__.py
================================================================================
-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/routers/athlete_profiles.py
================================================================================
"""
Routeur pour la gestion des profils athl√®tes enrichis
"""
import json
from typing import List, Dict, Any
from fastapi import APIRouter, Depends, HTTPException, status
from sqlalchemy.orm import Session
from sqlalchemy.exc import IntegrityError

from app.core.database import get_db
from app.dependencies import get_current_user
from app.models import sql_models, schemas
from app.services.coach_memory.service import initialize_coach_memory
from app.validators.athlete_profile_validators import validate_athlete_profile

router = APIRouter(
    prefix="/api/v1/profiles",
    tags=["Athlete Profiles v2"]
)

@router.post("/complete", response_model=schemas.AthleteProfileResponse, status_code=status.HTTP_201_CREATED)
async def create_complete_profile(
    profile_data: Dict[str, Any],
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    Cr√©e un profil athl√®te complet via le wizard
    """
    # V√©rifier si l'utilisateur a d√©j√† un profil
    existing_profile = db.query(sql_models.AthleteProfile).filter(
        sql_models.AthleteProfile.user_id == current_user.id
    ).first()
    
    if existing_profile:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Un profil existe d√©j√† pour cet utilisateur"
        )
    
    # Valider les donn√©es du profil
    try:
        validate_athlete_profile(profile_data)
    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )
    
    # Cr√©er le profil
    athlete_profile = sql_models.AthleteProfile(
        user_id=current_user.id,
        basic_info=json.dumps(profile_data.get('basic_info', {})),
        physical_metrics=json.dumps(profile_data.get('physical_metrics', {})),
        sport_context=json.dumps(profile_data.get('sport_context', {})),
        performance_baseline=json.dumps(profile_data.get('performance_baseline', {})),
        injury_prevention=json.dumps(profile_data.get('injury_prevention', {})),
        training_preferences=json.dumps(profile_data.get('training_preferences', {})),
        goals=json.dumps(profile_data.get('goals', {})),
        constraints=json.dumps(profile_data.get('constraints', {}))
    )
    
    try:
        db.add(athlete_profile)
        db.commit()
        db.refresh(athlete_profile)
        
        # Calculer le pourcentage de compl√©tion
        athlete_profile.completion_percentage = athlete_profile.calculate_completion()
        athlete_profile.is_complete = athlete_profile.completion_percentage >= 80
        db.commit()
        
        # Initialiser la m√©moire du coach
        initialize_coach_memory(athlete_profile, db)
        
        return athlete_profile
        
    except IntegrityError:
        db.rollback()
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Erreur d'int√©grit√© des donn√©es"
        )

@router.get("/{profile_id}", response_model=schemas.AthleteProfileResponse)
async def get_profile(
    profile_id: int,
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    R√©cup√®re un profil athl√®te par ID
    """
    profile = db.query(sql_models.AthleteProfile).filter(
        sql_models.AthleteProfile.id == profile_id,
        sql_models.AthleteProfile.user_id == current_user.id
    ).first()
    
    if not profile:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Profil non trouv√©"
        )
    
    return profile

@router.put("/{profile_id}", response_model=schemas.AthleteProfileResponse)
async def update_profile(
    profile_id: int,
    profile_update: schemas.AthleteProfileUpdate,
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    Met √† jour compl√®tement un profil
    """
    profile = db.query(sql_models.AthleteProfile).filter(
        sql_models.AthleteProfile.id == profile_id,
        sql_models.AthleteProfile.user_id == current_user.id
    ).first()
    
    if not profile:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Profil non trouv√©"
        )
    
    # Mettre √† jour chaque section
    update_dict = profile_update.dict(exclude_unset=True)
    for section, data in update_dict.items():
        if data is not None:
            setattr(profile, section, json.dumps(data))
    
    # Recalculer la compl√©tion
    profile.completion_percentage = profile.calculate_completion()
    profile.is_complete = profile.completion_percentage >= 80
    
    db.commit()
    db.refresh(profile)
    
    return profile

@router.patch("/{profile_id}/section/{section_name}")
async def update_profile_section(
    profile_id: int,
    section_name: str,
    section_update: schemas.ProfileSectionUpdate,
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    Met √† jour une section sp√©cifique du profil
    """
    profile = db.query(sql_models.AthleteProfile).filter(
        sql_models.AthleteProfile.id == profile_id,
        sql_models.AthleteProfile.user_id == current_user.id
    ).first()
    
    if not profile:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Profil non trouv√©"
        )
    
    # V√©rifier que la section existe
    valid_sections = [
        'basic_info', 'physical_metrics', 'sport_context',
        'performance_baseline', 'injury_prevention',
        'training_preferences', 'goals', 'constraints'
    ]
    
    if section_name not in valid_sections:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Section invalide. Options: {', '.join(valid_sections)}"
        )
    
    # Mettre √† jour la section
    setattr(profile, section_name, json.dumps(section_update.section_data))
    
    # Recalculer la compl√©tion
    profile.completion_percentage = profile.calculate_completion()
    profile.is_complete = profile.completion_percentage >= 80
    
    db.commit()
    
    return {
        "message": "Section mise √† jour avec succ√®s",
        "completion_percentage": profile.completion_percentage,
        "is_complete": profile.is_complete
    }

@router.post("/{profile_id}/metrics")
async def add_daily_metrics(
    profile_id: int,
    metrics: schemas.DailyMetrics,
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    Ajoute des m√©triques quotidiennes au profil
    """
    profile = db.query(sql_models.AthleteProfile).filter(
        sql_models.AthleteProfile.id == profile_id,
        sql_models.AthleteProfile.user_id == current_user.id
    ).first()
    
    if not profile:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Profil non trouv√©"
        )
    
    # Ici, on pourrait stocker les m√©triques dans une table s√©par√©e
    # Pour l'instant, on les ajoute aux m√©triques physiques
    physical_metrics = json.loads(profile.physical_metrics) if profile.physical_metrics else {}
    
    if 'daily_metrics' not in physical_metrics:
        physical_metrics['daily_metrics'] = []
    
    physical_metrics['daily_metrics'].append(metrics.dict())
    
    # Garder seulement les 30 derniers jours
    if len(physical_metrics['daily_metrics']) > 30:
        physical_metrics['daily_metrics'] = physical_metrics['daily_metrics'][-30:]
    
    # Mettre √† jour les m√©triques agr√©g√©es
    if metrics.resting_heart_rate:
        physical_metrics['resting_heart_rate'] = metrics.resting_heart_rate
        physical_metrics['last_updated'] = metrics.date
    
    profile.physical_metrics = json.dumps(physical_metrics)
    db.commit()
    
    return {"message": "M√©triques quotidiennes enregistr√©es"}

@router.post("/{profile_id}/goals", status_code=status.HTTP_201_CREATED)
async def add_goal(
    profile_id: int,
    goal_data: Dict[str, Any],
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    Ajoute un nouvel objectif au profil
    """
    profile = db.query(sql_models.AthleteProfile).filter(
        sql_models.AthleteProfile.id == profile_id,
        sql_models.AthleteProfile.user_id == current_user.id
    ).first()
    
    if not profile:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Profil non trouv√©"
        )
    
    goals = json.loads(profile.goals) if profile.goals else {"secondary_goals": [], "milestones": []}
    
    if goal_data.get('is_primary', False):
        goals['primary_goal'] = goal_data.get('description', '')
        goals['target_date'] = goal_data.get('target_date')
        goals['target_metrics'] = goal_data.get('target_metrics', {})
    else:
        if 'secondary_goals' not in goals:
            goals['secondary_goals'] = []
        goals['secondary_goals'].append(goal_data.get('description', ''))
    
    profile.goals = json.dumps(goals)
    db.commit()
    
    return {"message": "Objectif ajout√© avec succ√®s"}

@router.put("/{profile_id}/goals/{goal_id}/progress")
async def update_goal_progress(
    profile_id: int,
    goal_id: str,  # Pour les objectifs principaux: "primary", pour secondaires: index
    progress: schemas.GoalProgressUpdate,
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    Met √† jour la progression d'un objectif
    """
    profile = db.query(sql_models.AthleteProfile).filter(
        sql_models.AthleteProfile.id == profile_id,
        sql_models.AthleteProfile.user_id == current_user.id
    ).first()
    
    if not profile:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Profil non trouv√©"
        )
    
    goals = json.loads(profile.goals) if profile.goals else {}
    
    if goal_id == "primary":
        if 'milestones' not in goals:
            goals['milestones'] = []
        
        goals['milestones'].append({
            "date": progress.progress_note.split(" - ")[0] if progress.progress_note else "",
            "description": progress.progress_note or f"Progression: {progress.progress_value}%",
            "progress": progress.progress_value,
            "achieved": progress.achieved
        })
    else:
        # Pour les objectifs secondaires, on pourrait avoir une structure diff√©rente
        pass
    
    profile.goals = json.dumps(goals)
    db.commit()
    
    return {"message": "Progression mise √† jour"}

@router.post("/{profile_id}/import")
async def import_external_data(
    profile_id: int,
    import_data: Dict[str, Any],
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    Importe des donn√©es depuis des sources externes (Strava, Garmin, etc.)
    """
    profile = db.query(sql_models.AthleteProfile).filter(
        sql_models.AthleteProfile.id == profile_id,
        sql_models.AthleteProfile.user_id == current_user.id
    ).first()
    
    if not profile:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Profil non trouv√©"
        )
    
    source = import_data.get('source', '').lower()
    data = import_data.get('data', {})
    
    if source == 'strava':
        # Importer les donn√©es Strava
        if 'weight' in data:
            physical_metrics = json.loads(profile.physical_metrics) if profile.physical_metrics else {}
            physical_metrics['weight'] = data['weight']
            profile.physical_metrics = json.dumps(physical_metrics)
    
    elif source == 'garmin':
        # Importer les donn√©es Garmin
        if 'resting_heart_rate' in data:
            physical_metrics = json.loads(profile.physical_metrics) if profile.physical_metrics else {}
            physical_metrics['resting_heart_rate'] = data['resting_heart_rate']
            profile.physical_metrics = json.dumps(physical_metrics)
    
    elif source == 'whoop':
        # Importer les donn√©es Whoop
        if 'recovery' in data:
            physical_metrics = json.loads(profile.physical_metrics) if profile.physical_metrics else {}
            physical_metrics['hrv_baseline'] = data.get('hrv', physical_metrics.get('hrv_baseline'))
            profile.physical_metrics = json.dumps(physical_metrics)
    
    db.commit()
    
    return {
        "message": f"Donn√©es import√©es depuis {source}",
        "imported_fields": list(data.keys())
    }

@router.get("/{profile_id}/completion")
async def get_profile_completion(
    profile_id: int,
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    R√©cup√®re le statut de compl√©tion du profil
    """
    profile = db.query(sql_models.AthleteProfile).filter(
        sql_models.AthleteProfile.id == profile_id,
        sql_models.AthleteProfile.user_id == current_user.id
    ).first()
    
    if not profile:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Profil non trouv√©"
        )
    
    # Calculer les sections manquantes
    sections = {
        'basic_info': profile.basic_info,
        'physical_metrics': profile.physical_metrics,
        'sport_context': profile.sport_context,
        'performance_baseline': profile.performance_baseline,
        'injury_prevention': profile.injury_prevention,
        'training_preferences': profile.training_preferences,
        'goals': profile.goals,
        'constraints': profile.constraints
    }
    
    missing_sections = []
    for name, value in sections.items():
        if not value or value == '{}' or value == 'null':
            missing_sections.append(name)
    
    return {
        "completion_percentage": profile.completion_percentage,
        "is_complete": profile.is_complete,
        "missing_sections": missing_sections,
        "total_sections": 8,
        "completed_sections": 8 - len(missing_sections)
    }
-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/routers/auth.py
================================================================================
from datetime import timedelta
from fastapi import APIRouter, Depends, HTTPException, status
from fastapi.security import OAuth2PasswordRequestForm
from sqlalchemy.orm import Session
from app.core.database import get_db
from app.models import sql_models, schemas
from app.core import security

router = APIRouter(
    prefix="/auth",
    tags=["Authentication"]
)

@router.post("/signup", response_model=schemas.UserResponse, status_code=status.HTTP_201_CREATED)
async def create_user(user: schemas.UserCreate, db: Session = Depends(get_db)):
    """Inscription d'un nouvel utilisateur."""
    # 1. V√©rifier si le pseudo existe d√©j√†
    db_user = db.query(sql_models.User).filter(sql_models.User.username == user.username).first()
    if db_user:
        raise HTTPException(status_code=400, detail="Ce pseudo est d√©j√† pris.")
    
    # 2. Hasher le mot de passe
    hashed_pwd = security.get_password_hash(user.password)
    
    # 3. Cr√©er l'utilisateur avec l'email
    new_user = sql_models.User(
        username=user.username,
        email=user.email,  # <--- On passe l'email ici
        hashed_password=hashed_pwd
    )
    
    db.add(new_user)
    db.commit()
    db.refresh(new_user)
    
    return new_user

@router.post("/token", response_model=schemas.Token)
async def login_for_access_token(form_data: OAuth2PasswordRequestForm = Depends(), db: Session = Depends(get_db)):
    """Login : V√©rifie pseudo/mot de passe et renvoie un Token JWT."""
    user = db.query(sql_models.User).filter(sql_models.User.username == form_data.username).first()
    
    if not user or not security.verify_password(form_data.password, user.hashed_password):
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Pseudo ou mot de passe incorrect",
            headers={"WWW-Authenticate": "Bearer"},
        )
    
    access_token_expires = timedelta(minutes=security.ACCESS_TOKEN_EXPIRE_MINUTES)
    access_token = security.create_access_token(
        data={"sub": user.username}, expires_delta=access_token_expires
    )
    
    return {"access_token": access_token, "token_type": "bearer"}-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/routers/coach.py
================================================================================
import os
import json
import re
import google.generativeai as genai
from fastapi import APIRouter, HTTPException, Depends
from sqlalchemy.orm import Session
from app.core.database import get_db
from app.dependencies import get_current_user
from app.models import sql_models, schemas
from app.models.schemas import (
    ProfileAuditRequest, ProfileAuditResponse, 
    StrategyResponse, WeeklyPlanResponse,
    GenerateWorkoutRequest, AIWorkoutPlan
)
from dotenv import load_dotenv
from datetime import date

load_dotenv()

router = APIRouter(
    prefix="/coach",
    tags=["AI Coach"]
)

# Configuration unique de l'IA
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# --- UTILITAIRES ---

def clean_ai_json(text: str) -> str:
    """
    Nettoie la r√©ponse de l'IA pour extraire uniquement le bloc JSON valide.
    G√®re les cas o√π l'IA ajoute des balises markdown ```json ... ```.
    """
    try:
        # On cherche le contenu entre ```json et ``` ou juste ``` et ```
        pattern = r"```(?:json)?\s*([\s\S]*?)\s*```"
        match = re.search(pattern, text)
        if match:
            return match.group(1).strip()
        return text.strip()
    except Exception:
        return text

# --- PROMPTS ---

def get_profile_analysis_prompt(profile_data):
    """G√©n√®re le prompt pour l'audit du profil."""
    profile_str = json.dumps(profile_data, ensure_ascii=False, indent=2)
    return f"""
    R√îLE : Tu es le Lead Sport Scientist d'une f√©d√©ration olympique (TitanFlow).
    TACHE : Auditer le profil d'un athl√®te et D√âFINIR LA LIGNE DIRECTRICE.
    
    DONN√âES BRUTES ATHL√àTE (JSON) :
    {profile_str}

    CONSIGNES D'ANALYSE :
    1. V√©rifie la coh√©rence "Niveau vs Performances".
    2. V√©rifie la coh√©rence "Objectif vs Logistique (Dispo)".
    3. Identifie les risques de blessures ou les incoh√©rences majeures.
    
    FORMAT DE SORTIE :
    R√©ponds UNIQUEMENT en Markdown bien format√©.
    Utilise des emojis. Sois direct, bienveillant mais exigeant.
    """

def get_periodization_prompt(profile_data):
    """G√©n√®re le prompt pour la strat√©gie de p√©riodisation (JSON)."""
    today_str = date.today().strftime("%Y-%m-%d")
    profile_str = json.dumps(profile_data, ensure_ascii=False, indent=2)
    cycle_goal = profile_data.get('goal', 'Performance G√©n√©rale')
    target_date_str = profile_data.get('target_date', '2025-12-31')

    return f"""
    R√îLE : Directeur de Performance Sportive (Haut Niveau).
    CONTEXTE : Cr√©er une P√âRIODISATION MACRO (Les Grandes Phases) pour un athl√®te.

    1. DONN√âES ATHL√àTE :
    {profile_str}

    2. PARAM√àTRES DU CYCLE :
    - Objectif : {cycle_goal}
    - Date actuelle : {today_str}
    - Deadline : {target_date_str}

    CONSIGNES DE P√âRIODISATION :
    - Divise la p√©riode en BLOCS (PHASES) de 3 √† 8 semaines.
    - G√©n√®re entre 3 et 6 phases majeures.

    STRUCTURE DE SORTIE (JSON STRICT) :
    {{
        "periodization_title": "Nom scientifique",
        "periodization_logic": "Justification courte.",
        "progression_model": "Ex: RPE Progression.",
        "recommended_frequency": 4, 
        "phases": [
            {{
                "phase_name": "Phase 1 : [Nom]",
                "focus": "Objectif physiologique",
                "intensity_metric": "RPE 7-8", 
                "volume_strategy": "Ex: Volume √âlev√©",
                "start": "YYYY-MM-DD",
                "end": "YYYY-MM-DD"
            }}
        ]
    }}
    """

def get_weekly_planning_prompt(profile_data):
    """G√©n√®re le prompt complexe pour la semaine type."""
    
    user_sport = profile_data.get('sport', 'Musculation')
    avail = profile_data.get('availability', [])
    
    slots_context = []
    for slot in avail:
        if slot.get('isActive', False): # Adaptation au format Flutter (isActive vs Active)
             slots_context.append({
                "Jour": slot.get('day'),
                "Moment": slot.get('moment'),
                "Dispo_Max": f"{slot.get('duration')} min",
                "Type_Cible": slot.get('type')
            })
    
    avail_json = json.dumps(slots_context, ensure_ascii=False, indent=2)

    return f"""
    R√îLE : Entra√Æneur Expert en {user_sport}.
    MISSION : G√©n√©rer la SEMAINE TYPE (Lundi-Dimanche) pour cet athl√®te.

    CONTEXTE ATHL√àTE :
    - Sport : {user_sport}
    - Niveau : {profile_data.get('level')}
    - Objectif : {profile_data.get('goal')}

    === CONTRAINTES STRICTES (MATRICE DE DISPONIBILIT√â) ===
    Tu DOIS respecter ces cr√©neaux √† la lettre. Si un jour n'est pas list√© ci-dessous, c'est REPOS.
    {avail_json}

    R√àGLES D'ALLOCATION :
    1. Pour chaque cr√©neau disponible, assigne une s√©ance pr√©cise.
    2. Respecte le "Type_Cible" impos√© par l'utilisateur :
       - "PPS" = Sport Sp√©cifique (Terrain, Piste, Bassin).
       - "PPG" = Renforcement / Muscu.
       - "Libre" = Choisis le mieux adapt√© pour l'√©quilibre.
    3. Si pas de cr√©neau dispo un jour -> "Type": "Repos", "Focus": "R√©cup√©ration".
    4. "RPE Cible" doit √™tre un ENTIER (ex: 0 pour Repos, 7 pour une s√©ance). Ne jamais mettre null.

    FORMAT DE SORTIE (JSON OBJET) :
    {{
        "schedule": [
            {{ "Jour": "Lundi", "Cr√©neau": "Soir", "Type": "Sp√©cifique (PPS)", "Focus": "...", "RPE Cible": 7 }},
            ... (14 entr√©es pour couvrir la semaine)
        ],
        "reasoning": "Explication courte de la logique de la semaine."
    }}
    """

def get_workout_generation_prompt(profile_data, context):
    """
    G√©n√®re une s√©ance d√©taill√©e avec gestion stricte des MODES D'ENREGISTREMENT.
    """
    sport = profile_data.get('sport', 'Musculation')
    user_level = profile_data.get('level', 'Interm√©diaire')
    
    duration = context.get('duration', 60)
    energy = context.get('energy', 5)
    focus = context.get('focus', 'Full Body')
    equipment = context.get('equipment', 'Standard')

    return f"""
    R√îLE : Coach Sportif d'√âlite (SmartCoach).
    MISSION : Concevoir une s√©ance sur-mesure (JSON).

    ATHL√àTE :
    - Sport : {sport} ({user_level})
    - Blessures : {profile_data.get('injuries', 'Aucune')}
    
    CONTEXTE DU JOUR :
    - Dur√©e Max : {duration} min
    - √ânergie : {energy}/10
    - Focus demand√© : {focus}
    - Mat√©riel : {equipment}

    INSTRUCTIONS TECHNIQUES CRITIQUES :
    1. Adapte le volume (S√©ries/Reps) √† l'√©nergie du jour.
    2. Pour CHAQUE exercice, tu DOIS choisir le 'recording_mode' adapt√© √† la nature de l'effort :
       - "LOAD_REPS" : Pour la musculation classique (Halt√®res, Barres, Machines). Champs : Poids/Reps.
       - "BODYWEIGHT_REPS" : Pour le poids du corps (Pompes, Tractions). Champs : Lest/Reps.
       - "ISOMETRIC_TIME" : Pour le statique (Gainage, Chaise). Champs : Lest/Temps(s).
       - "PACE_DISTANCE" : Pour le Cardio/Running/Natation. Champs : Allure/Distance(m).
       - "POWER_TIME" : Pour le V√©lo/Ergo. Champs : Watts/Temps(s).
    
    3. Le champ 'reps' peut √™tre une string (ex: "10-12" ou "AMRAP") ou un nombre.
    4. Le champ 'rest' est en secondes.

    STRUCTURE DE SORTIE (JSON STRICT) :
    {{
        "title": "Nom de la s√©ance",
        "coach_comment": "Phrase de motivation ou conseil technique.",
        "warmup": ["Exo 1", "Exo 2"],
        "exercises": [
            {{
                "name": "Squat",
                "sets": 4,
                "reps": "8-10",
                "rest": 90,
                "tips": "Dos droit, descendre sous la parall√®le.",
                "recording_mode": "LOAD_REPS"
            }}
        ],
        "cooldown": ["Etirement 1"]
    }}
    """

# --- ROUTES ---

@router.post("/audit", response_model=ProfileAuditResponse)
async def audit_profile(
    payload: ProfileAuditRequest,
    current_user: sql_models.User = Depends(get_current_user)
):
    """Audit du profil athl√®te par l'IA."""
    if not GEMINI_API_KEY:
        raise HTTPException(status_code=500, detail="Cl√© API Gemini manquante.")
    
    try:
        genai.configure(api_key=GEMINI_API_KEY)
        model = genai.GenerativeModel('gemini-2.0-flash')
        response = model.generate_content(get_profile_analysis_prompt(payload.profile_data))
        
        # Stocker l'audit localement
        from app.core.database import get_db
        from sqlalchemy.orm import Session
        db = next(get_db())
        current_user.profile_data = json.dumps(payload.profile_data)
        db.commit()
        
        return {"markdown_report": response.text}
    except Exception as e:
        print(f"‚ùå Erreur audit: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# --- STRAT√âGIE (Lecture & √âcriture Persistante) ---

@router.get("/strategy", response_model=StrategyResponse)
async def get_strategy(
    current_user: sql_models.User = Depends(get_current_user)
):
    """R√©cup√®re la strat√©gie sauvegard√©e (si elle existe)."""
    if not current_user.strategy_data:
        raise HTTPException(status_code=404, detail="Aucune strat√©gie trouv√©e.")
    try:
        data = json.loads(current_user.strategy_data)
        return data
    except Exception as e:
        print(f"‚ùå Erreur lecture strat√©gie: {e}")
        raise HTTPException(status_code=500, detail="Erreur lecture strat√©gie.")

@router.post("/strategy", response_model=StrategyResponse)
async def generate_strategy(
    payload: ProfileAuditRequest,
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """G√©n√®re ET sauvegarde la strat√©gie."""
    if not GEMINI_API_KEY:
        raise HTTPException(status_code=500, detail="Cl√© API Gemini manquante.")
    
    try:
        genai.configure(api_key=GEMINI_API_KEY)
        model = genai.GenerativeModel('gemini-2.0-flash', generation_config={"response_mime_type": "application/json"})
        response = model.generate_content(get_periodization_prompt(payload.profile_data))
        
        # Nettoyage et Validation JSON
        clean_text = clean_ai_json(response.text)
        strategy_data = json.loads(clean_text)
        
        # Sauvegarde en BDD
        current_user.strategy_data = json.dumps(strategy_data)
        db.commit()
        db.refresh(current_user)
        
        return strategy_data
    except Exception as e:
        print(f"‚ùå Erreur Strategy Gen: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# --- PLANNING SEMAINE (Lecture & √âcriture Persistante) ---

@router.get("/week", response_model=WeeklyPlanResponse)
async def get_week(
    current_user: sql_models.User = Depends(get_current_user)
):
    """R√©cup√®re la semaine type sauvegard√©e."""
    if not current_user.weekly_plan_data:
         raise HTTPException(status_code=404, detail="Aucune semaine trouv√©e.")
    try:
        data = json.loads(current_user.weekly_plan_data)
        return data
    except Exception as e:
        print(f"‚ùå Erreur lecture semaine: {e}")
        raise HTTPException(status_code=500, detail="Erreur lecture semaine.")

@router.post("/week", response_model=WeeklyPlanResponse)
async def generate_week(
    payload: ProfileAuditRequest,
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """G√©n√®re ET sauvegarde la semaine type."""
    if not GEMINI_API_KEY:
        raise HTTPException(status_code=500, detail="Cl√© API Gemini manquante.")
    
    try:
        genai.configure(api_key=GEMINI_API_KEY)
        model = genai.GenerativeModel('gemini-2.0-flash', generation_config={"response_mime_type": "application/json"})
        
        prompt = get_weekly_planning_prompt(payload.profile_data)
        response = model.generate_content(prompt)
        
        # Nettoyage et Parsing
        clean_text = clean_ai_json(response.text)
        result = json.loads(clean_text)
        
        if "schedule" not in result and isinstance(result, list):
            result = {"schedule": result, "reasoning": "G√©n√©r√© automatiquement."}
        
        # Sauvegarde en BDD
        current_user.weekly_plan_data = json.dumps(result)
        db.commit()
        db.refresh(current_user)
            
        return result
    except Exception as e:
        print(f"‚ùå Erreur Week Gen: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# --- GESTION DES S√âANCES & BROUILLONS ---

@router.get("/workout/draft", response_model=AIWorkoutPlan)
async def get_draft_workout(
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    R√©cup√®re le brouillon de s√©ance en cours (si existant).
    Utile pour reprendre une session apr√®s un crash.
    """
    if not current_user.draft_workout_data:
        raise HTTPException(status_code=404, detail="Aucun brouillon trouv√©.")
    
    try:
        return json.loads(current_user.draft_workout_data)
    except Exception as e:
        print(f"‚ùå Erreur lecture brouillon: {e}")
        raise HTTPException(status_code=500, detail="Erreur lecture brouillon.")

@router.delete("/workout/draft")
async def discard_draft_workout(
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    Supprime explicitement le brouillon (Abandon).
    """
    try:
        current_user.draft_workout_data = None
        db.commit()
        return {"status": "success", "message": "Brouillon supprim√©."}
    except Exception as e:
        print(f"‚ùå Erreur suppression brouillon: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/workout", response_model=AIWorkoutPlan)
async def generate_workout(
    payload: GenerateWorkoutRequest,
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    G√©n√®re une s√©ance d√©taill√©e ET la sauvegarde en brouillon.
    """
    if not GEMINI_API_KEY:
        raise HTTPException(status_code=500, detail="Cl√© API Gemini manquante.")
    
    try:
        genai.configure(api_key=GEMINI_API_KEY)
        model = genai.GenerativeModel('gemini-2.0-flash', generation_config={"response_mime_type": "application/json"})
        
        prompt = get_workout_generation_prompt(payload.profile_data, payload.context)
        response = model.generate_content(prompt)
        
        # Nettoyage et Parsing
        clean_text = clean_ai_json(response.text)
        parsed_response = json.loads(clean_text)
        
        # Validation de la structure
        if isinstance(parsed_response, list):
            if parsed_response:
                parsed_response = parsed_response[0]
            else:
                raise ValueError("L'IA a renvoy√© une liste vide.")
        
        # Validation des exercices
        if "exercises" not in parsed_response:
            parsed_response["exercises"] = []
        
        # S'assurer que chaque exercice a un recording_mode
        for exercise in parsed_response["exercises"]:
            if "recording_mode" not in exercise:
                exercise["recording_mode"] = "LOAD_REPS"
        
        # Sauvegarde automatique du brouillon
        current_user.draft_workout_data = json.dumps(parsed_response)
        db.commit()
        db.refresh(current_user)

        return parsed_response
    except json.JSONDecodeError as e:
        print(f"‚ùå Erreur JSON IA: {e}")
        print(f"Texte brut re√ßu: {clean_text[:500]}...")
        raise HTTPException(
            status_code=500, 
            detail="L'IA a renvoy√© une r√©ponse invalide. Veuillez r√©essayer."
        )
    except Exception as e:
        print(f"‚ùå Erreur Workout Gen: {e}")
        raise HTTPException(
            status_code=500, 
            detail=f"Erreur lors de la g√©n√©ration: {str(e)}"
        )-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/routers/coach_memories.py
================================================================================
"""
Routeur pour la gestion de la m√©moire du coach
"""
import json
from datetime import datetime, timedelta
from typing import Dict, Any, List
from fastapi import APIRouter, Depends, HTTPException, status, BackgroundTasks
from sqlalchemy.orm import Session

from app.core.database import get_db
from app.dependencies import get_current_user
from app.models import sql_models, schemas
from app.services.coach_memory.service import (
    initialize_coach_memory,
    process_workout_session,
    update_daily_context,
    generate_insights,
    recalculate_memory
)

router = APIRouter(
    prefix="/api/v1/coach-memories",
    tags=["Coach Memory v2"]
)

@router.get("/athlete/{athlete_id}", response_model=schemas.CoachMemoryResponse)
async def get_coach_memory_by_athlete(
    athlete_id: int,
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    R√©cup√®re la m√©moire du coach pour un athl√®te sp√©cifique
    """
    # V√©rifier que l'athl√®te appartient √† l'utilisateur
    athlete_profile = db.query(sql_models.AthleteProfile).filter(
        sql_models.AthleteProfile.id == athlete_id,
        sql_models.AthleteProfile.user_id == current_user.id
    ).first()
    
    if not athlete_profile:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Profil athl√®te non trouv√©"
        )
    
    # R√©cup√©rer ou cr√©er la m√©moire du coach
    coach_memory = db.query(sql_models.CoachMemory).filter(
        sql_models.CoachMemory.athlete_profile_id == athlete_id
    ).first()
    
    if not coach_memory:
        # Initialiser la m√©moire si elle n'existe pas
        coach_memory = initialize_coach_memory(athlete_profile, db)
    
    return coach_memory

@router.get("/{memory_id}/context")
async def get_memory_context(
    memory_id: int,
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    R√©cup√®re uniquement le contexte actuel de la m√©moire
    """
    coach_memory = db.query(sql_models.CoachMemory).filter(
        sql_models.CoachMemory.id == memory_id
    ).first()
    
    if not coach_memory:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="M√©moire du coach non trouv√©e"
        )
    
    # V√©rifier les permissions
    athlete_profile = db.query(sql_models.AthleteProfile).filter(
        sql_models.AthleteProfile.id == coach_memory.athlete_profile_id,
        sql_models.AthleteProfile.user_id == current_user.id
    ).first()
    
    if not athlete_profile:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Acc√®s non autoris√©"
        )
    
    context = json.loads(coach_memory.current_context) if coach_memory.current_context else {}
    
    return {
        "context": context,
        "last_updated": coach_memory.last_updated,
        "readiness_score": context.get('readiness_score', 0),
        "fatigue_state": context.get('fatigue_state', 'unknown')
    }

@router.get("/{memory_id}/insights")
async def get_memory_insights(
    memory_id: int,
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    R√©cup√®re les insights g√©n√©r√©s par la m√©moire
    """
    coach_memory = db.query(sql_models.CoachMemory).filter(
        sql_models.CoachMemory.id == memory_id
    ).first()
    
    if not coach_memory:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="M√©moire du coach non trouv√©e"
        )
    
    # V√©rifier les permissions
    athlete_profile = db.query(sql_models.AthleteProfile).filter(
        sql_models.AthleteProfile.id == coach_memory.athlete_profile_id,
        sql_models.AthleteProfile.user_id == current_user.id
    ).first()
    
    if not athlete_profile:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Acc√®s non autoris√©"
        )
    
    # G√©n√©rer des insights si n√©cessaire
    insights = generate_insights(coach_memory, athlete_profile, db)
    
    return {
        "sport_specific_insights": json.loads(coach_memory.sport_specific_insights) if coach_memory.sport_specific_insights else {},
        "performance_baselines": json.loads(coach_memory.performance_baselines) if coach_memory.performance_baselines else {},
        "adaptation_signals": json.loads(coach_memory.adaptation_signals) if coach_memory.adaptation_signals else {},
        "memory_flags": json.loads(coach_memory.memory_flags) if coach_memory.memory_flags else {},
        "generated_insights": insights
    }

@router.post("/{memory_id}/process-session")
async def process_session(
    memory_id: int,
    session_data: Dict[str, Any],
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    Traite une s√©ance d'entra√Ænement et met √† jour la m√©moire
    """
    coach_memory = db.query(sql_models.CoachMemory).filter(
        sql_models.CoachMemory.id == memory_id
    ).first()
    
    if not coach_memory:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="M√©moire du coach non trouv√©e"
        )
    
    # V√©rifier les permissions
    athlete_profile = db.query(sql_models.AthleteProfile).filter(
        sql_models.AthleteProfile.id == coach_memory.athlete_profile_id,
        sql_models.AthleteProfile.user_id == current_user.id
    ).first()
    
    if not athlete_profile:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Acc√®s non autoris√©"
        )
    
    # Traiter la s√©ance en arri√®re-plan
    background_tasks.add_task(
        process_workout_session,
        coach_memory,
        athlete_profile,
        session_data,
        db
    )
    
    return {
        "message": "S√©ance en cours de traitement",
        "session_id": session_data.get('id'),
        "processing": True
    }

@router.post("/{memory_id}/daily-checkin")
async def daily_checkin(
    memory_id: int,
    checkin_data: Dict[str, Any],
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    Effectue un check-in quotidien et met √† jour le contexte
    """
    coach_memory = db.query(sql_models.CoachMemory).filter(
        sql_models.CoachMemory.id == memory_id
    ).first()
    
    if not coach_memory:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="M√©moire du coach non trouv√©e"
        )
    
    # V√©rifier les permissions
    athlete_profile = db.query(sql_models.AthleteProfile).filter(
        sql_models.AthleteProfile.id == coach_memory.athlete_profile_id,
        sql_models.AthleteProfile.user_id == current_user.id
    ).first()
    
    if not athlete_profile:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Acc√®s non autoris√©"
        )
    
    # Mettre √† jour le contexte quotidien
    updated_context = update_daily_context(coach_memory, checkin_data, db)
    
    return {
        "message": "Check-in quotidien enregistr√©",
        "updated_context": updated_context,
        "readiness_score": updated_context.get('readiness_score', 0),
        "next_recommendations": updated_context.get('recommendations', [])
    }

@router.post("/{memory_id}/notes")
async def add_coach_note(
    memory_id: int,
    note_data: Dict[str, Any],
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    Ajoute une note du coach √† la m√©moire
    """
    coach_memory = db.query(sql_models.CoachMemory).filter(
        sql_models.CoachMemory.id == memory_id
    ).first()
    
    if not coach_memory:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="M√©moire du coach non trouv√©e"
        )
    
    # V√©rifier les permissions
    athlete_profile = db.query(sql_models.AthleteProfile).filter(
        sql_models.AthleteProfile.id == coach_memory.athlete_profile_id,
        sql_models.AthleteProfile.user_id == current_user.id
    ).first()
    
    if not athlete_profile:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Acc√®s non autoris√©"
        )
    
    # Ajouter la note
    coach_notes = json.loads(coach_memory.coach_notes) if coach_memory.coach_notes else {}
    
    note_id = f"note_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    coach_notes[note_id] = {
        "timestamp": datetime.now().isoformat(),
        "content": note_data.get('content', ''),
        "type": note_data.get('type', 'observation'),
        "priority": note_data.get('priority', 1),
        "tags": note_data.get('tags', [])
    }
    
    coach_memory.coach_notes = json.dumps(coach_notes)
    db.commit()
    
    return {
        "message": "Note ajout√©e",
        "note_id": note_id,
        "timestamp": datetime.now().isoformat()
    }

@router.post("/{memory_id}/recalculate")
async def force_recalculate_memory(
    memory_id: int,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    Force le recalcul complet de la m√©moire
    """
    coach_memory = db.query(sql_models.CoachMemory).filter(
        sql_models.CoachMemory.id == memory_id
    ).first()
    
    if not coach_memory:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="M√©moire du coach non trouv√©e"
        )
    
    # V√©rifier les permissions
    athlete_profile = db.query(sql_models.AthleteProfile).filter(
        sql_models.AthleteProfile.id == coach_memory.athlete_profile_id,
        sql_models.AthleteProfile.user_id == current_user.id
    ).first()
    
    if not athlete_profile:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Acc√®s non autoris√©"
        )
    
    # Recalculer en arri√®re-plan
    background_tasks.add_task(
        recalculate_memory,
        coach_memory,
        athlete_profile,
        db
    )
    
    return {
        "message": "Recalcul de la m√©moire d√©marr√©",
        "memory_id": memory_id,
        "recalculating": True
    }

@router.get("/{memory_id}/flags")
async def get_memory_flags(
    memory_id: int,
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    R√©cup√®re les flags (indicateurs) de la m√©moire
    """
    coach_memory = db.query(sql_models.CoachMemory).filter(
        sql_models.CoachMemory.id == memory_id
    ).first()
    
    if not coach_memory:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="M√©moire du coach non trouv√©e"
        )
    
    # V√©rifier les permissions
    athlete_profile = db.query(sql_models.AthleteProfile).filter(
        sql_models.AthleteProfile.id == coach_memory.athlete_profile_id,
        sql_models.AthleteProfile.user_id == current_user.id
    ).first()
    
    if not athlete_profile:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Acc√®s non autoris√©"
        )
    
    flags = json.loads(coach_memory.memory_flags) if coach_memory.memory_flags else {}
    
    # Filtrer les flags actifs
    active_flags = {k: v for k, v in flags.items() if v is True}
    warning_flags = []
    
    # D√©terminer les priorit√©s
    if active_flags.get('needs_deload'):
        warning_flags.append({"flag": "needs_deload", "priority": "high", "message": "Besoin de d√©charge d√©tect√©"})
    if active_flags.get('approaching_overtraining'):
        warning_flags.append({"flag": "approaching_overtraining", "priority": "high", "message": "Risque de surentra√Ænement"})
    if active_flags.get('detraining_risk'):
        warning_flags.append({"flag": "detraining_risk", "priority": "medium", "message": "Risque de d√©sentra√Ænement"})
    if active_flags.get('adaptation_window_open'):
        warning_flags.append({"flag": "adaptation_window_open", "priority": "low", "message": "Fen√™tre d'adaptation ouverte"})
    
    return {
        "active_flags": active_flags,
        "warnings": warning_flags,
        "total_flags": len(flags),
        "active_count": len(active_flags)
    }

@router.get("/{memory_id}/recommendations")
async def get_recommendations(
    memory_id: int,
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    G√©n√®re des recommandations bas√©es sur la m√©moire
    """
    coach_memory = db.query(sql_models.CoachMemory).filter(
        sql_models.CoachMemory.id == memory_id
    ).first()
    
    if not coach_memory:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="M√©moire du coach non trouv√©e"
        )
    
    # V√©rifier les permissions
    athlete_profile = db.query(sql_models.AthleteProfile).filter(
        sql_models.AthleteProfile.id == coach_memory.athlete_profile_id,
        sql_models.AthleteProfile.user_id == current_user.id
    ).first()
    
    if not athlete_profile:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Acc√®s non autoris√©"
        )
    
    context = json.loads(coach_memory.current_context) if coach_memory.current_context else {}
    flags = json.loads(coach_memory.memory_flags) if coach_memory.memory_flags else {}
    
    recommendations = []
    
    # G√©n√©rer des recommandations bas√©es sur le contexte et les flags
    readiness = context.get('readiness_score', 50)
    
    if readiness < 40:
        recommendations.append({
            "type": "recovery",
            "priority": "high",
            "action": "R√©duire le volume d'entra√Ænement de 30% cette semaine",
            "reason": f"Score de pr√©paration bas ({readiness}/100)"
        })
    
    if flags.get('needs_deload'):
        recommendations.append({
            "type": "deload",
            "priority": "high",
            "action": "Planifier une semaine de d√©charge",
            "reason": "Accumulation de fatigue d√©tect√©e"
        })
    
    if flags.get('adaptation_window_open') and readiness > 70:
        recommendations.append({
            "type": "progression",
            "priority": "medium",
            "action": "Augmenter l'intensit√© de 5-10%",
            "reason": "Fen√™tre d'adaptation optimale"
        })
    
    if not recommendations:
        recommendations.append({
            "type": "maintenance",
            "priority": "low",
            "action": "Continuer le programme actuel",
            "reason": "√âtat d'entra√Ænement optimal"
        })
    
    return {
        "recommendations": recommendations,
        "generated_at": datetime.now().isoformat(),
        "context_used": {
            "readiness_score": readiness,
            "fatigue_state": context.get('fatigue_state', 'unknown'),
            "macrocycle_phase": context.get('macrocycle_phase', 'base')
        }
    }
-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/routers/feed.py
================================================================================
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from typing import List
from app.core.database import get_db
from app.models import sql_models, schemas
from app.dependencies import get_current_user

router = APIRouter(
    prefix="/feed",
    tags=["Neural Feed"]
)

@router.get("/", response_model=List[schemas.FeedItemResponse])
async def get_my_feed(
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    R√©cup√®re le flux d'√©v√©nements de l'utilisateur.
    Filtre : Uniquement les items NON COMPL√âT√âS.
    Tri : Priorit√© (DESC) puis Date de cr√©ation (DESC).
    """
    items = db.query(sql_models.FeedItem)\
        .filter(sql_models.FeedItem.user_id == current_user.id)\
        .filter(sql_models.FeedItem.is_completed == False)\
        .order_by(sql_models.FeedItem.priority.desc(), sql_models.FeedItem.created_at.desc())\
        .all()
    return items

@router.patch("/{item_id}/read")
async def mark_as_read(
    item_id: str,
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """Marque un item comme LU (mais le laisse dans le flux tant que pas compl√©t√©)."""
    item = db.query(sql_models.FeedItem).filter(sql_models.FeedItem.id == item_id, sql_models.FeedItem.user_id == current_user.id).first()
    if not item:
        raise HTTPException(status_code=404, detail="Item introuvable")
    
    item.is_read = True
    db.commit()
    return {"status": "success"}

@router.patch("/{item_id}/complete")
async def mark_as_completed(
    item_id: str,
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """Marque un item comme COMPL√âT√â (Dispara√Æt du flux)."""
    item = db.query(sql_models.FeedItem).filter(sql_models.FeedItem.id == item_id, sql_models.FeedItem.user_id == current_user.id).first()
    if not item:
        raise HTTPException(status_code=404, detail="Item introuvable")
    
    item.is_completed = True
    db.commit()
    return {"status": "success"}-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/routers/performance.py
================================================================================
from fastapi import APIRouter, HTTPException
from app.models.schemas import OneRepMaxRequest, OneRepMaxResponse
from app.domain import calculations

router = APIRouter(
    prefix="/performance",
    tags=["Performance & Metrics"]
)

@router.post("/1rm", response_model=OneRepMaxResponse)
async def compute_one_rep_max(payload: OneRepMaxRequest):
    """
    Calcule le 1RM (One Rep Max) estim√© bas√© sur une performance.
    S√©lectionne automatiquement la meilleure formule (Epley, Brzycki, Wathan).
    """
    try:
        result = calculations.calculate_1rm(payload.weight, payload.reps)
        
        return {
            "estimated_1rm": result["1rm"],
            "method_used": result["method"],
            "input_weight": payload.weight,
            "input_reps": payload.reps
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/routers/profiles.py
================================================================================
from fastapi import APIRouter, Depends, HTTPException, status
from sqlalchemy.orm import Session
from app.core.database import get_db
from app.models import sql_models, schemas
from app.dependencies import get_current_user
from app.services.coach_logic import CoachLogic

router = APIRouter(
    prefix="/api/v1",
    tags=["Athlete Profile & Memory"]
)

@router.get("/profiles/me", response_model=schemas.AthleteProfileResponse)
async def get_my_profile(
    current_user: sql_models.User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    if not current_user.athlete_profile:
        profile = sql_models.AthleteProfile(user_id=current_user.id)
        db.add(profile)
        db.commit()
        db.refresh(profile)
        return profile
    return current_user.athlete_profile

@router.post("/profiles/complete", response_model=schemas.AthleteProfileResponse)
async def complete_profile(
    profile_data: schemas.AthleteProfileCreate,
    current_user: sql_models.User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    sport = profile_data.sport_context.sport
    pos = profile_data.sport_context.position
    if not CoachLogic.validate_sport_position(sport, pos):
        raise HTTPException(status_code=400, detail=f"Position {pos} invalide pour le sport {sport}")

    db_profile = current_user.athlete_profile
    if not db_profile:
        db_profile = sql_models.AthleteProfile(user_id=current_user.id)
        db.add(db_profile)
    
    db_profile.basic_info = profile_data.basic_info.dict()
    db_profile.physical_metrics = profile_data.physical_metrics.dict()
    db_profile.sport_context = profile_data.sport_context.dict()
    db_profile.training_preferences = profile_data.training_preferences.dict()
    db_profile.goals = profile_data.goals
    db_profile.constraints = profile_data.constraints
    
    if not db_profile.coach_memory:
        memory = CoachLogic.initialize_memory(db_profile)
        db.add(memory)
    
    db.commit()
    db.refresh(db_profile)
    return db_profile

@router.get("/coach-memories/me", response_model=schemas.CoachMemoryResponse)
async def get_my_coach_memory(
    current_user: sql_models.User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    if not current_user.athlete_profile or not current_user.athlete_profile.coach_memory:
        raise HTTPException(status_code=404, detail="Profil ou M√©moire introuvable. Compl√©tez votre profil.")
    return current_user.athlete_profile.coach_memory

@router.post("/coach-memories/recalculate")
async def force_recalculate(
    current_user: sql_models.User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    profile = current_user.athlete_profile
    if not profile or not profile.coach_memory:
        raise HTTPException(status_code=404, detail="Introuvable")
        
    CoachLogic.update_daily(profile.coach_memory, profile)
    db.commit()
    return {"status": "updated", "new_readiness": profile.coach_memory.current_context.get('readiness_score')}
-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/routers/safety.py
================================================================================
from fastapi import APIRouter, HTTPException
from app.models.schemas import ACWRRequest, ACWRResponse
from app.domain import safety

router = APIRouter(
    prefix="/safety",
    tags=["Safety & Prevention"]
)

@router.post("/acwr", response_model=ACWRResponse)
async def compute_acwr_metrics(payload: ACWRRequest):
    """
    Calcule le Ratio Aigu/Chronique (ACWR) pour pr√©venir les blessures.
    Envoie l'historique des s√©ances (Date, Dur√©e, RPE).
    Retourne le statut de risque (Optimal, Surcharge, Danger).
    """
    try:
        # Conversion des mod√®les Pydantic en liste de dicts pour Pandas
        history_dicts = [log.dict() for log in payload.history]
        
        result = safety.calculate_acwr(history_dicts)
        
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/routers/user.py
================================================================================
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from app.core.database import get_db
from app.models import sql_models, schemas
from app.dependencies import get_current_user
import json

from fastapi import APIRouter, Depends, HTTPException, status
router = APIRouter(
    prefix="/user",
    tags=["User Profile"]
)

@router.get("/profile", response_model=schemas.UserProfileUpdate)
async def get_profile(
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    R√©cup√®re le profil complet de l'utilisateur (JSON).
    """
    if not current_user.profile_data:
        return {"profile_data": {}}
    
    try:
        # On convertit la string stock√©e en BDD en dictionnaire
        data = json.loads(current_user.profile_data)
        return {"profile_data": data}
    except:
        return {"profile_data": {}}

@router.put("/profile")


@router.get("/profile/complete", response_model=schemas.AthleteProfileResponse)
async def get_complete_profile(
    current_user: sql_models.User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """
    R√©cup√®re le profil athl√®te complet.
    Compatibilit√© avec l'ancien syst√®me (profile_data) et le nouveau (AthleteProfile).
    """
    # V√©rifier d'abord si l'utilisateur a un profil athl√®te v2
    if current_user.athlete_profile:
        return current_user.athlete_profile
    
    # Fallback : retourner les donn√©es du profil legacy
    if current_user.profile_data:
        try:
            profile_data = json.loads(current_user.profile_data)
            return {
                "id": current_user.id,
                "user_id": current_user.id,
                "created_at": current_user.created_at if hasattr(current_user, 'created_at') else None,
                "basic_info": {
                    "pseudo": current_user.username,
                    "email": current_user.email,
                    **profile_data.get('basic_info', {})
                },
                "physical_metrics": profile_data.get('physical_metrics', {}),
                "sport_context": profile_data.get('sport_context', {}),
                "training_preferences": profile_data.get('training_preferences', {}),
                "goals": profile_data.get('goals', {}),
                "constraints": profile_data.get('constraints', {}),
                "injury_prevention": profile_data.get('injury_prevention', {}),
                "performance_baseline": profile_data.get('performance_baseline', {})
            }
        except Exception as e:
            raise HTTPException(
                status_code=500,
                detail=f"Erreur lecture profil: {str(e)}"
            )
    
    raise HTTPException(
        status_code=404,
        detail="Profil non trouv√©. Compl√©tez votre profil d'abord."
    )

@router.post("/profile/complete", response_model=schemas.AthleteProfileResponse)
async def create_complete_profile(
    profile_data: dict,
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    Cr√©e ou met √† jour un profil athl√®te complet.
    Supporte √† la fois l'ancien format (profile_data) et le nouveau (sections).
    """
    try:
        # V√©rifier si l'utilisateur a d√©j√† un profil athl√®te v2
        if current_user.athlete_profile:
            # Mettre √† jour le profil existant
            profile = current_user.athlete_profile
            for section, data in profile_data.items():
                if hasattr(profile, section):
                    setattr(profile, section, json.dumps(data))
        else:
            # Cr√©er un nouveau profil athl√®te
            profile = sql_models.AthleteProfile(
                user_id=current_user.id,
                basic_info=json.dumps(profile_data.get('basic_info', {})),
                physical_metrics=json.dumps(profile_data.get('physical_metrics', {})),
                sport_context=json.dumps(profile_data.get('sport_context', {})),
                training_preferences=json.dumps(profile_data.get('training_preferences', {})),
                goals=json.dumps(profile_data.get('goals', {})),
                constraints=json.dumps(profile_data.get('constraints', {})),
                injury_prevention=json.dumps(profile_data.get('injury_prevention', {})),
                performance_baseline=json.dumps(profile_data.get('performance_baseline', {}))
            )
            db.add(profile)
        
        # Mettre √† jour aussi le profil legacy pour compatibilit√©
        current_user.profile_data = json.dumps(profile_data)
        
        db.commit()
        db.refresh(profile)
        
        return profile
        
    except Exception as e:
        db.rollback()
        raise HTTPException(
            status_code=500,
            detail=f"Erreur cr√©ation profil: {str(e)}"
        )

@router.post("/profile/sections/{section}")
async def update_profile_section(
    section: str,
    section_data: dict,
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    Met √† jour une section sp√©cifique du profil.
    Section peut √™tre: basic_info, physical_metrics, sport_context, etc.
    """
    # Liste des sections valides
    valid_sections = [
        'basic_info', 'physical_metrics', 'sport_context',
        'training_preferences', 'goals', 'constraints',
        'injury_prevention', 'performance_baseline'
    ]
    
    if section not in valid_sections:
        raise HTTPException(
            status_code=400,
            detail=f"Section invalide. Options: {', '.join(valid_sections)}"
        )
    
    try:
        # Mettre √† jour le profil athl√®te v2 si existant
        if current_user.athlete_profile:
            profile = current_user.athlete_profile
            setattr(profile, section, json.dumps(section_data))
        else:
            # Si pas de profil athl√®te, cr√©er un profil minimal
            profile = sql_models.AthleteProfile(user_id=current_user.id)
            setattr(profile, section, json.dumps(section_data))
            db.add(profile)
        
        # Mettre √† jour aussi le profil legacy
        legacy_data = {}
        if current_user.profile_data:
            try:
                legacy_data = json.loads(current_user.profile_data)
            except:
                pass
        
        legacy_data[section] = section_data
        current_user.profile_data = json.dumps(legacy_data)
        
        db.commit()
        
        return {
            "status": "success",
            "message": f"Section '{section}' mise √† jour",
            "section": section,
            "data": section_data
        }
        
    except Exception as e:
        db.rollback()
        raise HTTPException(
            status_code=500,
            detail=f"Erreur mise √† jour section: {str(e)}"
        )



@router.get("/profile/complete", response_model=schemas.AthleteProfileResponse)
async def get_complete_profile(
    current_user: sql_models.User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """
    R√©cup√®re le profil athl√®te complet.
    Compatibilit√© avec l'ancien syst√®me (profile_data) et le nouveau (AthleteProfile).
    """
    # V√©rifier d'abord si l'utilisateur a un profil athl√®te v2
    if current_user.athlete_profile:
        return current_user.athlete_profile
    
    # Fallback : retourner les donn√©es du profil legacy
    if current_user.profile_data:
        try:
            profile_data = json.loads(current_user.profile_data)
            return {
                "id": current_user.id,
                "user_id": current_user.id,
                "created_at": current_user.created_at if hasattr(current_user, 'created_at') else None,
                "basic_info": {
                    "pseudo": current_user.username,
                    "email": current_user.email,
                    **profile_data.get('basic_info', {})
                },
                "physical_metrics": profile_data.get('physical_metrics', {}),
                "sport_context": profile_data.get('sport_context', {}),
                "training_preferences": profile_data.get('training_preferences', {}),
                "goals": profile_data.get('goals', {}),
                "constraints": profile_data.get('constraints', {}),
                "injury_prevention": profile_data.get('injury_prevention', {}),
                "performance_baseline": profile_data.get('performance_baseline', {})
            }
        except Exception as e:
            raise HTTPException(
                status_code=500,
                detail=f"Erreur lecture profil: {str(e)}"
            )
    
    raise HTTPException(
        status_code=404,
        detail="Profil non trouv√©. Compl√©tez votre profil d'abord."
    )

@router.post("/profile/complete", response_model=schemas.AthleteProfileResponse)
async def create_complete_profile(
    profile_data: dict,
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    Cr√©e ou met √† jour un profil athl√®te complet.
    Supporte √† la fois l'ancien format (profile_data) et le nouveau (sections).
    """
    try:
        # V√©rifier si l'utilisateur a d√©j√† un profil athl√®te v2
        if current_user.athlete_profile:
            # Mettre √† jour le profil existant
            profile = current_user.athlete_profile
            for section, data in profile_data.items():
                if hasattr(profile, section):
                    setattr(profile, section, json.dumps(data))
        else:
            # Cr√©er un nouveau profil athl√®te
            profile = sql_models.AthleteProfile(
                user_id=current_user.id,
                basic_info=json.dumps(profile_data.get('basic_info', {})),
                physical_metrics=json.dumps(profile_data.get('physical_metrics', {})),
                sport_context=json.dumps(profile_data.get('sport_context', {})),
                training_preferences=json.dumps(profile_data.get('training_preferences', {})),
                goals=json.dumps(profile_data.get('goals', {})),
                constraints=json.dumps(profile_data.get('constraints', {})),
                injury_prevention=json.dumps(profile_data.get('injury_prevention', {})),
                performance_baseline=json.dumps(profile_data.get('performance_baseline', {}))
            )
            db.add(profile)
        
        # Mettre √† jour aussi le profil legacy pour compatibilit√©
        current_user.profile_data = json.dumps(profile_data)
        
        db.commit()
        db.refresh(profile)
        
        return profile
        
    except Exception as e:
        db.rollback()
        raise HTTPException(
            status_code=500,
            detail=f"Erreur cr√©ation profil: {str(e)}"
        )

@router.post("/profile/sections/{section}")
async def update_profile_section(
    section: str,
    section_data: dict,
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    Met √† jour une section sp√©cifique du profil.
    Section peut √™tre: basic_info, physical_metrics, sport_context, etc.
    """
    # Liste des sections valides
    valid_sections = [
        'basic_info', 'physical_metrics', 'sport_context',
        'training_preferences', 'goals', 'constraints',
        'injury_prevention', 'performance_baseline'
    ]
    
    if section not in valid_sections:
        raise HTTPException(
            status_code=400,
            detail=f"Section invalide. Options: {', '.join(valid_sections)}"
        )
    
    try:
        # Mettre √† jour le profil athl√®te v2 si existant
        if current_user.athlete_profile:
            profile = current_user.athlete_profile
            setattr(profile, section, json.dumps(section_data))
        else:
            # Si pas de profil athl√®te, cr√©er un profil minimal
            profile = sql_models.AthleteProfile(user_id=current_user.id)
            setattr(profile, section, json.dumps(section_data))
            db.add(profile)
        
        # Mettre √† jour aussi le profil legacy
        legacy_data = {}
        if current_user.profile_data:
            try:
                legacy_data = json.loads(current_user.profile_data)
            except:
                pass
        
        legacy_data[section] = section_data
        current_user.profile_data = json.dumps(legacy_data)
        
        db.commit()
        
        return {
            "status": "success",
            "message": f"Section '{section}' mise √† jour",
            "section": section,
            "data": section_data
        }
        
    except Exception as e:
        db.rollback()
        raise HTTPException(
            status_code=500,
            detail=f"Erreur mise √† jour section: {str(e)}"
        )

async def update_profile(
    profile: schemas.UserProfileUpdate,
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    Sauvegarde le profil complet (Age, Benchmarks, Planning...).
    """
    try:
        # On convertit le dictionnaire re√ßu en string pour le stockage SQL
        json_str = json.dumps(profile.profile_data)
        current_user.profile_data = json_str
        
        db.commit()
        db.refresh(current_user)
        return {"message": "Profil mis √† jour avec succ√®s"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur sauvegarde: {str(e)}")-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/routers/workouts.py
================================================================================
from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks
from sqlalchemy.orm import Session
from typing import List
from app.core.database import get_db
from app.models import sql_models, schemas
from app.dependencies import get_current_user
import json

# [DEV-CARD #03] Imports du Moteur de Feed
from app.services.feed.engine import TriggerEngine
from app.services.feed.triggers.workout_analysis import WorkoutAnalysisTrigger

router = APIRouter(
    prefix="/workouts",
    tags=["Workouts"]
)

@router.post("/", response_model=schemas.WorkoutSessionResponse)
async def create_workout(
    workout: schemas.WorkoutSessionCreate, 
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    # --- VALIDATION PHYSIOLOGIQUE ---
    def validate_physiological_limits(workout: schemas.WorkoutSessionCreate):
        """Valide les limites physiologiques humaines."""
        
        # Dur√©e r√©aliste (10min √† 4h)
        if workout.duration < 10 or workout.duration > 240:
            raise HTTPException(
                status_code=400, 
                detail=f"Dur√©e invalide ({workout.duration} min). Doit √™tre entre 10 et 240 minutes."
            )
        
        # RPE 1-10
        if workout.rpe < 1 or workout.rpe > 10:
            raise HTTPException(
                status_code=400,
                detail=f"RPE invalide ({workout.rpe}). Doit √™tre entre 1 et 10."
            )
        
        # √ânergie 1-10
        if workout.energy_level < 1 or workout.energy_level > 10:
            raise HTTPException(
                status_code=400,
                detail=f"Niveau d'√©nergie invalide ({workout.energy_level}). Doit √™tre entre 1 et 10."
            )
        
        # Validation des sets
        for s in workout.sets:
            # Watts max (record du monde ~2500W)
            if s.metric_type == 'POWER_TIME' and s.weight > 2000:
                raise HTTPException(
                    status_code=400,
                    detail=f"Puissance impossible ({s.weight}W). Record du monde ~2500W."
                )
            
            # Charge max (record +500kg)
            if s.metric_type == 'LOAD_REPS' and s.weight > 500:
                raise HTTPException(
                    status_code=400,
                    detail=f"Charge impossible ({s.weight}kg). Record du monde ~500kg."
                )
            
            # RPE s√©rie
            if s.rpe and (s.rpe < 1 or s.rpe > 10):
                raise HTTPException(
                    status_code=400,
                    detail=f"RPE s√©rie invalide ({s.rpe}). Doit √™tre entre 1 et 10."
                )
        
        return True

    # Appliquer la validation
    validate_physiological_limits(workout)

    """
    Enregistre une s√©ance compl√®te avec gestion du Polymorphisme (Metric Type).
    V√©rifie la coh√©rence des donn√©es (ex: Watts max, RPE bounds).
    [DEV-CARD #05] Supprime le brouillon associ√© une fois la s√©ance valid√©e.
    [DEV-CARD #03] Active le Neural Feed pour l'analyse post-s√©ance.
    """
    # 1. Validation de haut niveau avant insertion
    for s in workout.sets:
        # Validation RPE
        if s.rpe is not None and (s.rpe < 0 or s.rpe > 10):
            # On cap plut√¥t que de crasher
            s.rpe = max(0, min(10, s.rpe))
            
        # Validation Physiologique selon le mode
        if s.metric_type == 'POWER_TIME':
            # Check Watts (weight)
            if s.weight > 2000:
                raise HTTPException(status_code=400, detail=f"Valeur impossible : {s.weight} Watts sur l'exercice {s.exercise_name}. V√©rifiez la saisie.")
        
        elif s.metric_type == 'PACE_DISTANCE':
            # Dans ce mode : weight = Vitesse/Pace (souvent 0 si calcul√©e) ou Distance, reps = Distance ou Temps
            # Standard TitanFlow : Reps = Distance (m), Weight = 0 (ou vitesse m/s)
            if s.reps > 100000: # 100km max par s√©rie pour √™tre s√ªr
                 raise HTTPException(status_code=400, detail=f"Distance suspecte : {s.reps} m√®tres.")

    # 2. Cr√©ation de la Session
    db_workout = sql_models.WorkoutSession(
        date=workout.date,
        duration=workout.duration,
        rpe=workout.rpe,
        energy_level=workout.energy_level,
        notes=workout.notes,
        user_id=current_user.id
    )
    db.add(db_workout)
    db.commit()
    db.refresh(db_workout)
    
    # 3. Ajout des S√©ries (Sets)
    if workout.sets:
        for s in workout.sets:
            # Conversion explicite Pydantic -> SQL Model
            db_set = sql_models.WorkoutSet(
                session_id=db_workout.id,
                exercise_name=s.exercise_name,
                set_order=s.set_order,
                weight=s.weight, # D√©j√† nettoy√© par Pydantic (float)
                reps=s.reps,     # D√©j√† nettoy√© par Pydantic (float, secondes inclues)
                rpe=s.rpe,
                rest_seconds=s.rest_seconds,
                metric_type=s.metric_type # Le fameux recording_mode
            )
            db.add(db_set)
        
        # [DEV-CARD #05] Nettoyage du brouillon apr√®s succ√®s
        current_user.draft_workout_data = None
        
        db.commit()
        db.refresh(db_workout)

    # 4. [DEV-CARD #03] TRIGGER NEURAL FEED (L'IA s'active ici)
    # On lance l'analyse imm√©diatement (await) pour que le feed soit √† jour 
    # quand l'utilisateur revient sur l'accueil.
    try:
        # [MODIF V2] On passe le profil complet dans le contexte via user_data
        # On tente de parser le JSON profile_data s'il existe
        profile_data = {}
        if current_user.profile_data:
            try:
                profile_data = json.loads(current_user.profile_data)
            except:
                pass

        engine = TriggerEngine()
        engine.register(WorkoutAnalysisTrigger())
        await engine.run_all(db, current_user.id, {
            "workout": db_workout,
            "profile": profile_data # <--- ICI, on injecte les donn√©es pour le Bio-Twin
        })
    except Exception as e:
        # On ne bloque pas la r√©ponse si l'IA √©choue, c'est du bonus
        print(f"‚ö†Ô∏è Feed Engine Error: {e}")
    
    return db_workout

@router.get("/", response_model=List[schemas.WorkoutSessionResponse])
async def read_workouts(
    skip: int = 0, 
    limit: int = 100, 
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    """
    R√©cup√®re l'historique complet.
    Les champs polymorphes (weight/reps) sont renvoy√©s tels quels,
    le Frontend utilisera 'metric_type' pour savoir si c'est des kg ou des watts.
    """
    workouts = db.query(sql_models.WorkoutSession)\
        .filter(sql_models.WorkoutSession.user_id == current_user.id)\
        .order_by(sql_models.WorkoutSession.date.desc())\
        .offset(skip)\
        .limit(limit)\
        .all()
    return workouts-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/services/coach_logic.py
================================================================================
from datetime import date
from typing import Dict, Any
from app.models import sql_models

VALID_SPORT_POSITIONS = {
    'Rugby': ['Pilier', 'Talonneur', '2√®me ligne', '3√®me ligne', 'Demi', 'Centre', 'Ailier', 'Arri√®re'],
    'Football': ['Gardien', 'D√©fenseur', 'Milieu', 'Attaquant'],
}

class CoachLogic:
    @staticmethod
    def validate_sport_position(sport: str, position: str) -> bool:
        if sport in VALID_SPORT_POSITIONS:
            if position and position not in VALID_SPORT_POSITIONS[sport]:
                return False
        return True

    @staticmethod
    def initialize_memory(profile: sql_models.AthleteProfile) -> sql_models.CoachMemory:
        sport = profile.sport_context.get('sport', 'Autre')
        insights = {
            "primary_sport": sport,
            "specificity_index": "High" if sport in ['Rugby', 'Football'] else "Medium",
            "focus_areas": ["Strength", "Hypertrophy"] 
        }
        context = {
            "macrocycle_phase": "Adaptation Anatomique",
            "fatigue_state": "Fresh",
            "readiness_score": 100,
            "season_week": 1
        }
        flags = {
            "needs_deload": False,
            "injury_risk": False,
            "adaptation_window_open": True
        }
        memory = sql_models.CoachMemory(
            athlete_profile_id=profile.id,
            sport_specific_insights=insights,
            current_context=context,
            memory_flags=flags,
            coach_notes={"initialization": f"Profil cr√©√© le {date.today()}"}
        )
        return memory

    @staticmethod
    def calculate_readiness(profile: sql_models.AthleteProfile) -> int:
        base_score = 80
        sleep = profile.physical_metrics.get('sleep_quality_avg', 5)
        if sleep >= 8: base_score += 10
        elif sleep <= 4: base_score -= 20
        stress = profile.constraints.get('work_stress_level', 5)
        if stress >= 8: base_score -= 15
        return max(0, min(100, base_score))

    @staticmethod
    def update_daily(memory: sql_models.CoachMemory, profile: sql_models.AthleteProfile):
        new_readiness = CoachLogic.calculate_readiness(profile)
        current_context = dict(memory.current_context or {})
        current_context['readiness_score'] = new_readiness
        
        if new_readiness < 40:
            current_context['fatigue_state'] = "High"
        elif new_readiness < 70:
            current_context['fatigue_state'] = "Moderate"
        else:
            current_context['fatigue_state'] = "Optimal"
            
        memory.current_context = current_context
        flags = dict(memory.memory_flags or {})
        flags['needs_deload'] = new_readiness < 30
        flags['adaptation_window_open'] = new_readiness > 70
        memory.memory_flags = flags
-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/services/coach_memory/service.py
================================================================================
"""
Service de gestion de la m√©moire du coach
"""
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, Any, Optional, List
from sqlalchemy.orm import Session

from app.models import sql_models
from app.domain.bioenergetics import BioenergeticService

logger = logging.getLogger(__name__)

class CoachMemoryService:
    """Service principal pour la m√©moire du coach"""
    
    @staticmethod
    def initialize_coach_memory(athlete_profile: sql_models.AthleteProfile, db: Session) -> sql_models.CoachMemory:
        """Initialise la m√©moire du coach √† partir du profil"""
        logger.info(f"Initialisation de la m√©moire du coach pour l'athl√®te {athlete_profile.user_id}")
        
        # Extraire les donn√©es du profil
        basic_info = json.loads(athlete_profile.basic_info) if athlete_profile.basic_info else {}
        sport_context = json.loads(athlete_profile.sport_context) if athlete_profile.sport_context else {}
        performance_baseline = json.loads(athlete_profile.performance_baseline) if athlete_profile.performance_baseline else {}
        
        # Calculer les insights initiaux
        sport_insights = CoachMemoryService._calculate_initial_sport_insights(sport_context, basic_info)
        performance_baselines = CoachMemoryService._extract_initial_baselines(performance_baseline)
        initial_phase = CoachMemoryService._determine_initial_phase(athlete_profile)
        
        # Cr√©er la m√©moire
        memory = sql_models.CoachMemory(
            athlete_profile_id=athlete_profile.id,
            metadata=json.dumps({
                "athlete_id": athlete_profile.user_id,
                "created_at": datetime.utcnow().isoformat(),
                "last_updated": datetime.utcnow().isoformat(),
                "total_interactions": 0,
                "trust_score": 50,
                "data_points": 0
            }),
            current_context=json.dumps({
                'season_week': 1,
                'macrocycle_phase': initial_phase,
                'mesocycle_focus': 'base_fitness',
                'training_priority': 'volume',
                'next_competition': None,
                'days_to_competition': None,
                'fatigue_state': 'fresh',
                'readiness_score': 80,
                'current_constraints': json.loads(athlete_profile.constraints) if athlete_profile.constraints else {},
                'environmental_factors': {},
                'last_session_type': None,
                'last_session_rpe': None
            }),
            response_patterns=json.dumps({
                "volume_response": "neutral",
                "optimal_volumes": {},
                "intensity_tolerance": "medium",
                "recovery_profile": "normal",
                "fatigue_indicators": []
            }),
            performance_baselines=json.dumps(performance_baselines),
            adaptation_signals=json.dumps({
                "positive_adaptations": [],
                "last_adaptation_phase": None,
                "current_adaptation_status": "initial",
                "stagnation_signals": [],
                "regression_signals": [],
                "adaptation_windows": [],
                "next_suggested_focus": "base_fitness"
            }),
            sport_specific_insights=json.dumps(sport_insights),
            training_history_summary=json.dumps({
                "total_volume_by_type": {},
                "average_rpe_by_type": {},
                "successful_strategies": [],
                "failed_strategies": [],
                "lessons_learned": [],
                "seasonal_patterns": {},
                "best_training_weeks": [],
                "peak_periods": []
            }),
            athlete_preferences=json.dumps(json.loads(athlete_profile.training_preferences) if athlete_profile.training_preferences else {}),
            coach_notes=json.dumps({}),
            memory_flags=json.dumps({
                "needs_deload": False,
                "approaching_overtraining": False,
                "detraining_risk": False,
                "technique_regression": False,
                "adaptation_window_open": True,
                "pr_potential": False,
                "skill_integration_ready": False,
                "external_stress_high": False,
                "recovery_impaired": False,
                "motivation_low": False
            })
        )
        
        db.add(memory)
        db.commit()
        logger.info(f"M√©moire du coach cr√©√©e avec ID: {memory.id}")
        
        return memory
    
    @staticmethod
    def process_workout_session(
        coach_memory: sql_models.CoachMemory,
        athlete_profile: sql_models.AthleteProfile,
        session_data: Dict[str, Any],
        db: Session
    ) -> None:
        """Traite une s√©ance d'entra√Ænement et met √† jour la m√©moire"""
        logger.info(f"Traitement de la s√©ance pour la m√©moire {coach_memory.id}")
        
        # Mettre √† jour les m√©tadonn√©es
        metadata = json.loads(coach_memory.metadata) if coach_memory.metadata else {}
        metadata['total_interactions'] = metadata.get('total_interactions', 0) + 1
        metadata['last_updated'] = datetime.utcnow().isoformat()
        
        # Mettre √† jour le contexte
        context = json.loads(coach_memory.current_context) if coach_memory.current_context else {}
        context['last_session_type'] = session_data.get('type', 'unknown')
        context['last_session_rpe'] = session_data.get('rpe', 0)
        context['last_session_date'] = datetime.now().isoformat()
        
        # Mettre √† jour l'historique d'entra√Ænement
        history = json.loads(coach_memory.training_history_summary) if coach_memory.training_history_summary else {}
        
        session_type = session_data.get('type', 'strength')
        volume = session_data.get('volume', 0)
        rpe = session_data.get('rpe', 5)
        
        if 'total_volume_by_type' not in history:
            history['total_volume_by_type'] = {}
        
        history['total_volume_by_type'][session_type] = history['total_volume_by_type'].get(session_type, 0) + volume
        
        if 'average_rpe_by_type' not in history:
            history['average_rpe_by_type'] = {}
        
        if session_type not in history['average_rpe_by_type']:
            history['average_rpe_by_type'][session_type] = {'total': 0, 'count': 0}
        
        history['average_rpe_by_type'][session_type]['total'] += rpe
        history['average_rpe_by_type'][session_type]['count'] += 1
        
        # Calculer les r√©ponses √† l'entra√Ænement
        response_patterns = json.loads(coach_memory.response_patterns) if coach_memory.response_patterns else {}
        
        # Mettre √† jour la m√©moire
        coach_memory.metadata = json.dumps(metadata)
        coach_memory.current_context = json.dumps(context)
        coach_memory.training_history_summary = json.dumps(history)
        coach_memory.response_patterns = json.dumps(response_patterns)
        
        db.commit()
        logger.info(f"S√©ance trait√©e pour la m√©moire {coach_memory.id}")
    
    @staticmethod
    def update_daily_context(
        coach_memory: sql_models.CoachMemory,
        checkin_data: Dict[str, Any],
        db: Session
    ) -> Dict[str, Any]:
        """Met √† jour le contexte quotidien"""
        logger.info(f"Mise √† jour du contexte quotidien pour la m√©moire {coach_memory.id}")
        
        context = json.loads(coach_memory.current_context) if coach_memory.current_context else {}
        
        # Calculer le score de pr√©paration
        readiness_score = CoachMemoryService._calculate_readiness_score(checkin_data, context)
        context['readiness_score'] = readiness_score
        
        # Mettre √† jour l'√©tat de fatigue
        context['fatigue_state'] = CoachMemoryService._determine_fatigue_state(readiness_score)
        
        # Mettre √† jour les flags de m√©moire
        memory_flags = json.loads(coach_memory.memory_flags) if coach_memory.memory_flags else {}
        memory_flags['needs_deload'] = readiness_score < 40
        memory_flags['adaptation_window_open'] = readiness_score > 70
        memory_flags['recovery_impaired'] = checkin_data.get('sleep_quality', 5) < 4
        
        # Mettre √† jour la m√©moire
        coach_memory.current_context = json.dumps(context)
        coach_memory.memory_flags = json.dumps(memory_flags)
        
        db.commit()
        
        logger.info(f"Contexte mis √† jour - Readiness: {readiness_score}, Fatigue: {context['fatigue_state']}")
        
        return context
    
    @staticmethod
    def generate_insights(
        coach_memory: sql_models.CoachMemory,
        athlete_profile: sql_models.AthleteProfile,
        db: Session
    ) -> Dict[str, Any]:
        """G√©n√®re des insights bas√©s sur la m√©moire"""
        context = json.loads(coach_memory.current_context) if coach_memory.current_context else {}
        performance_baselines = json.loads(coach_memory.performance_baselines) if coach_memory.performance_baselines else {}
        sport_insights = json.loads(coach_memory.sport_specific_insights) if coach_memory.sport_specific_insights else {}
        
        insights = {
            "readiness_insight": CoachMemoryService._generate_readiness_insight(context),
            "fatigue_management": CoachMemoryService._generate_fatigue_insight(context),
            "progression_opportunities": CoachMemoryService._generate_progression_insights(performance_baselines),
            "sport_specific_recommendations": CoachMemoryService._generate_sport_recommendations(sport_insights),
            "risk_assessment": CoachMemoryService._generate_risk_assessment(coach_memory)
        }
        
        return insights
    
    @staticmethod
    def recalculate_memory(
        coach_memory: sql_models.CoachMemory,
        athlete_profile: sql_models.AthleteProfile,
        db: Session
    ) -> None:
        """Recalcule compl√®tement la m√©moire"""
        logger.info(f"Recalcul complet de la m√©moire {coach_memory.id}")
        
        # Recalculer tous les composants
        metadata = json.loads(coach_memory.metadata) if coach_memory.metadata else {}
        metadata['last_recalculated'] = datetime.utcnow().isoformat()
        metadata['version'] = metadata.get('version', 1) + 1
        
        # Recalculer les performances de base
        performance_baseline = json.loads(athlete_profile.performance_baseline) if athlete_profile.performance_baseline else {}
        updated_baselines = CoachMemoryService._extract_initial_baselines(performance_baseline)
        
        # Mettre √† jour la m√©moire
        coach_memory.metadata = json.dumps(metadata)
        coach_memory.performance_baselines = json.dumps(updated_baselines)
        coach_memory.version = metadata['version']
        
        db.commit()
        logger.info(f"M√©moire {coach_memory.id} recalcul√©e - version {metadata['version']}")
    
    # M√©thodes priv√©es helper
    @staticmethod
    def _calculate_initial_sport_insights(sport_context: Dict[str, Any], basic_info: Dict[str, Any]) -> Dict[str, Any]:
        """Calcule les insights sportifs initiaux"""
        primary_sport = sport_context.get('primary_sport', 'Musculation')
        position = sport_context.get('playing_position')
        level = sport_context.get('competition_level', 'Amateur')
        
        insights = {
            "primary_sport": primary_sport,
            "sport_requirements": CoachMemoryService._get_sport_requirements(primary_sport, position),
            "transfer_efficiency": 0.0,
            "high_transfer_exercises": CoachMemoryService._get_high_transfer_exercises(primary_sport),
            "low_transfer_exercises": [],
            "position_demands": CoachMemoryService._get_position_demands(primary_sport, position),
            "sport_skills_to_maintain": CoachMemoryService._get_sport_skills(primary_sport),
            "specificity_index": 0.5,
            "training_age_factor": basic_info.get('training_age', 1) / 10.0
        }
        
        return insights
    
    @staticmethod
    def _extract_initial_baselines(performance_baseline: Dict[str, Any]) -> Dict[str, Any]:
        """Extrait les performances de base"""
        return {
            "current_prs": performance_baseline.get('current_prs', {}),
            "metric_trends": {},
            "progress_rate": 0.0,
            "strength_ratios": CoachMemoryService._calculate_strength_ratios(performance_baseline.get('current_prs', {})),
            "balance_scores": {},
            "last_assessment_date": datetime.now().strftime('%Y-%m-%d')
        }
    
    @staticmethod
    def _determine_initial_phase(athlete_profile: sql_models.AthleteProfile) -> str:
        """D√©termine la phase initiale du macrocycle"""
        goals = json.loads(athlete_profile.goals) if athlete_profile.goals else {}
        target_date = goals.get('target_date')
        
        if target_date:
            target = datetime.strptime(target_date, '%Y-%m-%d')
            days_until = (target - datetime.now()).days
            
            if days_until > 120:
                return "base_fitness"
            elif days_until > 60:
                return "build"
            elif days_until > 30:
                return "peak"
            else:
                return "competition"
        else:
            return "base_fitness"
    
    @staticmethod
    def _calculate_readiness_score(checkin_data: Dict[str, Any], context: Dict[str, Any]) -> float:
        """Calcule le score de pr√©paration quotidien"""
        sleep_quality = checkin_data.get('sleep_quality', 5) / 10.0 * 30  # 30%
        sleep_duration = min(checkin_data.get('sleep_duration', 7) / 9.0 * 20, 20)  # 20%
        stress = (10 - checkin_data.get('perceived_stress', 5)) / 10.0 * 20  # 20%
        soreness = (10 - checkin_data.get('muscle_soreness', 5)) / 10.0 * 15  # 15%
        energy = checkin_data.get('energy_level', 5) / 10.0 * 15  # 15%
        
        readiness = sleep_quality + sleep_duration + stress + soreness + energy
        
        # Ajuster bas√© sur l'historique r√©cent
        last_readiness = context.get('readiness_score', 70)
        adjusted_readiness = (readiness * 0.7) + (last_readiness * 0.3)
        
        return round(adjusted_readiness, 1)
    
    @staticmethod
    def _determine_fatigue_state(readiness_score: float) -> str:
        """D√©termine l'√©tat de fatigue bas√© sur le score de pr√©paration"""
        if readiness_score >= 80:
            return "fresh"
        elif readiness_score >= 60:
            return "normal"
        elif readiness_score >= 40:
            return "accumulated"
        else:
            return "exhausted"
    
    @staticmethod
    def _generate_readiness_insight(context: Dict[str, Any]) -> str:
        """G√©n√®re un insight bas√© sur le score de pr√©paration"""
        readiness = context.get('readiness_score', 70)
        
        if readiness >= 80:
            return "√âtat de r√©cup√©ration optimal - pr√™t pour des s√©ances exigeantes"
        elif readiness >= 60:
            return "√âtat normal - bon pour l'entra√Ænement planifi√©"
        elif readiness >= 40:
            return "Fatigue accumul√©e - envisager une r√©duction du volume"
        else:
            return "Fatigue importante - n√©cessite une r√©cup√©ration active ou repos"
    
    @staticmethod
    def _generate_fatigue_insight(context: Dict[str, Any]) -> str:
        """G√©n√®re un insight sur la gestion de la fatigue"""
        fatigue_state = context.get('fatigue_state', 'normal')
        last_session_rpe = context.get('last_session_rpe', 5)
        
        if fatigue_state == "fresh" and last_session_rpe > 7:
            return "Bonne adaptation √† l'intensit√© - peut progresser"
        elif fatigue_state == "accumulated":
            return "Fatigue en accumulation - surveiller les signes de surentra√Ænement"
        elif fatigue_state == "exhausted":
            return "√âtat d'√©puisement - prioriser la r√©cup√©ration"
        else:
            return "Gestion de fatigue √©quilibr√©e"
    
    @staticmethod
    def _generate_progression_insights(performance_baselines: Dict[str, Any]) -> List[str]:
        """G√©n√®re des insights sur la progression"""
        insights = []
        current_prs = performance_baselines.get('current_prs', {})
        
        if 'squat_1rm' in current_prs and 'deadlift_1rm' in current_prs:
            squat = current_prs['squat_1rm']
            deadlift = current_prs['deadlift_1rm']
            
            if deadlift > squat * 1.2:
                insights.append("Rapport squat/deadlift d√©s√©quilibr√© - travailler le squat")
            elif squat > deadlift * 0.9:
                insights.append("Bon √©quilibre de force entre squat et deadlift")
        
        if 'bench_1rm' in current_prs:
            bench = current_prs['bench_1rm']
            if 'bodyweight' in current_prs:
                bw = current_prs['bodyweight']
                if bench > bw * 1.5:
                    insights.append("Force au bench excellente")
                elif bench < bw:
                    insights.append("Potentiel d'am√©lioration au bench")
        
        return insights
    
    @staticmethod
    def _generate_sport_recommendations(sport_insights: Dict[str, Any]) -> List[Dict[str, Any]]:
        """G√©n√®re des recommandations sp√©cifiques au sport"""
        recommendations = []
        primary_sport = sport_insights.get('primary_sport', 'Musculation')
        
        if primary_sport == 'Rugby':
            recommendations.append({
                "focus": "Puissance",
                "exercices": ["Squat explosif", "Power clean", "Sprints"],
                "ratio": "70% puissance / 30% endurance"
            })
        elif primary_sport == 'Football':
            recommendations.append({
                "focus": "Endurance intermittente",
                "exercices": ["Sprints r√©p√©t√©s", "Pliom√©trie", "Circuits"],
                "ratio": "60% endurance / 40% puissance"
            })
        elif primary_sport == 'Natation':
            recommendations.append({
                "focus": "Endurance et technique",
                "exercices": ["Tirage √©lastique", "Gainage", "Mobilit√© √©paules"],
                "ratio": "50% natation / 30% PPG / 20% muscu"
            })
        
        return recommendations
    
    @staticmethod
    def _generate_risk_assessment(coach_memory: sql_models.CoachMemory) -> Dict[str, Any]:
        """√âvalue les risques bas√©s sur la m√©moire"""
        memory_flags = json.loads(coach_memory.memory_flags) if coach_memory.memory_flags else {}
        context = json.loads(coach_memory.current_context) if coach_memory.current_context else {}
        
        risks = {
            "overtraining_risk": "low",
            "injury_risk": "low",
            "detraining_risk": "low",
            "motivation_risk": "low"
        }
        
        if memory_flags.get('approaching_overtraining'):
            risks["overtraining_risk"] = "high"
        
        if memory_flags.get('needs_deload'):
            risks["injury_risk"] = "medium"
        
        if memory_flags.get('detraining_risk'):
            risks["detraining_risk"] = "high"
        
        if memory_flags.get('motivation_low'):
            risks["motivation_risk"] = "high"
        
        readiness = context.get('readiness_score', 70)
        if readiness < 40:
            risks["overtraining_risk"] = "high"
            risks["injury_risk"] = "high"
        
        return risks
    
    @staticmethod
    def _get_sport_requirements(sport: str, position: Optional[str] = None) -> Dict[str, Any]:
        """Retourne les exigences sp√©cifiques au sport"""
        requirements = {
            "Musculation": {
                "strength": "high",
                "power": "medium",
                "endurance": "low",
                "mobility": "medium",
                "recovery": "high"
            },
            "Rugby": {
                "strength": "very high",
                "power": "very high",
                "endurance": "high",
                "mobility": "medium",
                "recovery": "high"
            },
            "Football": {
                "strength": "medium",
                "power": "high",
                "endurance": "very high",
                "mobility": "high",
                "recovery": "medium"
            },
            "Natation": {
                "strength": "medium",
                "power": "medium",
                "endurance": "very high",
                "mobility": "very high",
                "recovery": "medium"
            }
        }
        
        return requirements.get(sport, requirements["Musculation"])
    
    @staticmethod
    def _get_high_transfer_exercises(sport: str) -> List[str]:
        """Retourne les exercices √† haut transfert pour le sport"""
        transfers = {
            "Rugby": ["Squat", "Deadlift", "Power clean", "Bench press", "Sprints"],
            "Football": ["Squat", "Lunges", "Box jumps", "Sprints", "Plyometrics"],
            "Natation": ["Pull-ups", "Lat pulldowns", "Shoulder press", "Core work", "Rotator cuff"],
            "Musculation": ["Squat", "Deadlift", "Bench press", "Overhead press", "Rows"]
        }
        
        return transfers.get(sport, transfers["Musculation"])
    
    @staticmethod
    def _get_position_demands(sport: str, position: Optional[str] = None) -> Dict[str, Any]:
        """Retourne les exigences sp√©cifiques √† la position"""
        if sport == "Rugby" and position:
            if position in ["Pilier", "Talonneur", "2√®me ligne"]:
                return {"strength": "very high", "power": "high", "endurance": "medium"}
            elif position in ["3√®me ligne", "Demi"]:
                return {"strength": "high", "power": "very high", "endurance": "high"}
            else:  # Arri√®res
                return {"strength": "medium", "power": "high", "endurance": "very high"}
        
        return {"strength": "medium", "power": "medium", "endurance": "medium"}
    
    @staticmethod
    def _get_sport_skills(sport: str) -> List[str]:
        """Retourne les comp√©tences techniques √† maintenir"""
        skills = {
            "Rugby": ["Passe", "Jeu au pied", "Plaquage", "Ruck", "Maul"],
            "Football": ["Dribble", "Passe", "Tir", "Contr√¥le", "Positionnement"],
            "Natation": ["Crawl", "Dos", "Brasse", "Papillon", "Virage"],
            "Musculation": ["Technique squat", "Technique deadlift", "Technique bench", "Stabilit√©", "Respiration"]
        }
        
        return skills.get(sport, [])
    
    @staticmethod
    def _calculate_strength_ratios(current_prs: Dict[str, Any]) -> Dict[str, float]:
        """Calcule les ratios de force"""
        ratios = {}
        
        if 'squat_1rm' in current_prs and 'bodyweight' in current_prs:
            ratios['squat_to_bw'] = current_prs['squat_1rm'] / current_prs['bodyweight']
        
        if 'bench_1rm' in current_prs and 'bodyweight' in current_prs:
            ratios['bench_to_bw'] = current_prs['bench_1rm'] / current_prs['bodyweight']
        
        if 'deadlift_1rm' in current_prs and 'bodyweight' in current_prs:
            ratios['deadlift_to_bw'] = current_prs['deadlift_1rm'] / current_prs['bodyweight']
        
        if 'squat_1rm' in current_prs and 'bench_1rm' in current_prs:
            ratios['squat_to_bench'] = current_prs['squat_1rm'] / current_prs['bench_1rm']
        
        return ratios

# Fonctions d'interface pour compatibilit√© avec les routeurs
def initialize_coach_memory(athlete_profile: sql_models.AthleteProfile, db: Session) -> sql_models.CoachMemory:
    return CoachMemoryService.initialize_coach_memory(athlete_profile, db)

def process_workout_session(coach_memory: sql_models.CoachMemory, athlete_profile: sql_models.AthleteProfile, 
                          session_data: Dict[str, Any], db: Session) -> None:
    return CoachMemoryService.process_workout_session(coach_memory, athlete_profile, session_data, db)

def update_daily_context(coach_memory: sql_models.CoachMemory, checkin_data: Dict[str, Any], 
                        db: Session) -> Dict[str, Any]:
    return CoachMemoryService.update_daily_context(coach_memory, checkin_data, db)

def generate_insights(coach_memory: sql_models.CoachMemory, athlete_profile: sql_models.AthleteProfile,
                     db: Session) -> Dict[str, Any]:
    return CoachMemoryService.generate_insights(coach_memory, athlete_profile, db)

def recalculate_memory(coach_memory: sql_models.CoachMemory, athlete_profile: sql_models.AthleteProfile,
                      db: Session) -> None:
    return CoachMemoryService.recalculate_memory(coach_memory, athlete_profile, db)
-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/services/feed/engine.py
================================================================================
import logging
import uuid
import json
from datetime import datetime, timedelta
from typing import List, Dict, Any
from sqlalchemy.orm import Session
from sqlalchemy import and_

from app.models import sql_models, schemas
from app.services.feed.triggers.base import BaseTrigger

# Configuration des logs pour ne pas perdre une miette du match
logger = logging.getLogger(__name__)

class TriggerEngine:
    """
    Le Moteur de Jeu.
    Il poss√®de un registre de Triggers et les ex√©cute tous pour un contexte donn√©.
    Il g√®re aussi la s√©curit√© (Anti-Crash) et la filtration (D√©duplication).
    """
    def __init__(self):
        self._registry: List[BaseTrigger] = []

    def register(self, trigger: BaseTrigger):
        """Enr√¥le un nouveau Trigger dans l'√©quipe."""
        self._registry.append(trigger)
        logger.info(f"‚úÖ Trigger enregistr√© : {trigger.__class__.__name__}")

    async def run_all(self, db: Session, user_id: int, context: Dict[str, Any]) -> List[sql_models.FeedItem]:
        """
        Lance tous les Triggers enregistr√©s.
        
        R√®gles du jeu :
        1. Isolation : Si un trigger plante, les autres continuent.
        2. D√©duplication : On √©vite de spammer le m√™me message (ex: 1x par 24h).
        3. Persistance : Sauvegarde imm√©diate en base.
        """
        generated_events = []

        for trigger in self._registry:
            try:
                # Le Trigger analyse le jeu...
                event_schema = await trigger.check(user_id, context)
                
                if event_schema:
                    # Arbitrage vid√©o (D√©duplication)
                    if not self._should_discard(db, user_id, event_schema):
                        
                        # Transformation Schema -> SQL Model
                        db_item = sql_models.FeedItem(
                            id=str(uuid.uuid4()),
                            user_id=user_id,
                            type=event_schema.type,
                            title=event_schema.title,
                            message=event_schema.message,
                            priority=event_schema.priority,
                            is_read=False,
                            is_completed=False,
                            # Gestion propre du JSON payload
                            action_payload=json.dumps(event_schema.action_payload) if event_schema.action_payload else None
                        )
                        
                        db.add(db_item)
                        generated_events.append(db_item)
                        logger.info(f"üì¢ Event g√©n√©r√© : {db_item.title} ({trigger.__class__.__name__})")
                    else:
                        logger.info(f"üîá Event ignor√© (Doublon) : {event_schema.title}")

            except Exception as e:
                # Carton jaune : Le trigger a plant√©, mais le match continue
                logger.error(f"‚ö†Ô∏è Erreur Trigger {trigger.__class__.__name__}: {str(e)}")
                continue

        # Coup de sifflet final : on valide les buts
        if generated_events:
            db.commit()
            for ev in generated_events:
                db.refresh(ev)
                
        return generated_events

    def _should_discard(self, db: Session, user_id: int, event: schemas.FeedItemCreate) -> bool:
        """
        V√©rifie si un √©v√©nement similaire existe d√©j√† r√©cemment.
        R√®gle actuelle : Pas de doublon (M√™me Titre + M√™me Type) non trait√©.
        Ou pas de doublon identique cr√©√© dans les derni√®res 24h.
        """
        # 1. Chercher si le m√™me event est d√©j√† en attente (Non compl√©t√©)
        existing_active = db.query(sql_models.FeedItem).filter(
            sql_models.FeedItem.user_id == user_id,
            sql_models.FeedItem.type == event.type,
            sql_models.FeedItem.title == event.title,
            sql_models.FeedItem.is_completed == False
        ).first()

        if existing_active:
            return True # On jette, l'utilisateur a d√©j√† √ßa dans son feed

        # 2. Chercher si le m√™me event a √©t√© cr√©√© il y a moins de 24h (Anti-Spam)
        one_day_ago = datetime.utcnow() - timedelta(hours=24)
        recent_duplicate = db.query(sql_models.FeedItem).filter(
            sql_models.FeedItem.user_id == user_id,
            sql_models.FeedItem.type == event.type,
            sql_models.FeedItem.title == event.title,
            sql_models.FeedItem.created_at >= one_day_ago
        ).first()

        if recent_duplicate:
            return True

        return False-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/services/feed/triggers/base.py
================================================================================
from abc import ABC, abstractmethod
from typing import Optional, Dict, Any
from app.models import schemas

class BaseTrigger(ABC):
    """
    Interface abstraite pour tous les d√©clencheurs d'√©v√©nements (Triggers).
    Chaque Trigger est un 'sp√©cialiste' (ex: Sp√©cialiste Analyse, Sp√©cialiste Sant√©).
    """

    @abstractmethod
    async def check(self, user_id: int, context: Dict[str, Any]) -> Optional[schemas.FeedItemCreate]:
        """
        Analyse le contexte et retourne un FeedItemCreate si la condition est remplie.
        Retourne None sinon.
        
        :param user_id: L'ID de l'athl√®te concern√©.
        :param context: Un dictionnaire riche contenant les donn√©es (ex: {'workout': ..., 'profile': ...})
        :return: Un objet FeedItemCreate pr√™t √† √™tre ins√©r√©, ou None.
        """
        pass-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/services/feed/triggers/workout_analysis.py
================================================================================
import os
import json
import re
import google.generativeai as genai
from typing import Dict, Any, Optional
from app.services.feed.triggers.base import BaseTrigger
from app.models import schemas, sql_models
from app.domain.bioenergetics import BioenergeticService

class WorkoutAnalysisTrigger(BaseTrigger):
    """
    Trigger : Analyse Post-S√©ance Avanc√©e (Bio-Twin + Gemini).
    Condition : Une s√©ance vient d'√™tre termin√©e.
    Action : 
        1. Calcule les m√©triques bio√©nerg√©tiques (Kcal, Macros).
        2. G√©n√®re un rapport JSON complet via IA.
        3. Sauvegarde le rapport dans la s√©ance (Persistance).
        4. Cr√©e une carte Feed pour notifier l'athl√®te.
    """
    
    def __init__(self):
        self.api_key = os.getenv("GEMINI_API_KEY")

    async def check(self, user_id: int, context: Dict[str, Any]) -> Optional[schemas.FeedItemCreate]:
        # 1. V√©rifie si le contexte contient les donn√©es requises
        workout: sql_models.WorkoutSession = context.get("workout")
        profile_data: Dict[str, Any] = context.get("profile", {})
        
        if not workout:
            return None

        # 2. Si pas de cl√© API, on sort silencieusement
        if not self.api_key:
            return None

        try:
            # 3. PHASE 1 : CALCULS BIO√âNERG√âTIQUES (Les Maths)
            bio_metrics = BioenergeticService.calculate_needs(
                profile_data, 
                workout.sets, 
                workout.duration, 
                workout.rpe
            )
            
            # 4. PHASE 2 : G√âN√âRATION IA (Le Cerveau)
            sets_summary = "\n".join([
                f"- {s.exercise_name}: {s.weight} (load/watts) x {s.reps} (reps/sec/m) [{s.metric_type}]"
                for s in workout.sets
            ])
            
            prompt = f"""
            R√îLE : Expert en Physiologie Sportive et Nutrition (TitanFlow).
            TACHE : Analyser la s√©ance et g√©n√©rer un rapport JSON strict.

            === DONN√âES ATHL√àTE ===
            - Profil : {json.dumps(profile_data, ensure_ascii=False)}
            
            === DONN√âES S√âANCE ===
            - Dur√©e : {workout.duration} min
            - RPE : {workout.rpe}/10 (Intensit√© Ressentie)
            - Contenu :
            {sets_summary}
            
            === DONN√âES BIO-TWIN (CALCUL√âES) ===
            - D√©pense : ~{bio_metrics['kcal_total']} kcal
            - Besoins Post-Effort (Estim√©s) : 
              * Prot√©ines : {bio_metrics['protein_g']}g
              * Glucides : {bio_metrics['carbs_g']}g
              * Eau : {bio_metrics['water_ml']}ml
            
            === STRUCTURE DE SORTIE (JSON UNIQUEMENT) ===
            {{
              "performance_analysis": "Analyse technique de la charge et du volume en 2 phrases max.",
              "nutrition_comment": "Conseil pr√©cis validant ou ajustant les macros calcul√©es ci-dessus.",
              "recovery_score": 8,
              "coach_questions": ["Question pertinente 1?", "Question pertinente 2?"],
              "food_suggestion": {{
                  "option_shake": "Ex: Whey + Banane",
                  "option_solid": "Ex: Poulet + Riz + L√©gumes"
              }},
              "feed_message": "Une phrase d'accroche tr√®s courte (max 12 mots) pour la notification."
            }}
            """

            genai.configure(api_key=self.api_key)
            model = genai.GenerativeModel('gemini-2.0-flash', generation_config={"response_mime_type": "application/json"})
            response = model.generate_content(prompt)
            
            # Nettoyage JSON
            json_str = self._clean_json(response.text)
            analysis_result = json.loads(json_str)

            # 5. PHASE 3 : PERSISTANCE (Sauvegarde en BDD)
            # On fusionne les m√©triques calcul√©es avec l'analyse IA
            full_report = {
                **analysis_result,
                "bio_metrics": bio_metrics
            }
            
            # On stocke le JSON stringifi√© dans la colonne ai_analysis de la s√©ance
            # Note: L'objet 'workout' est attach√© √† la session DB, donc le commit du TriggerEngine validera cette modif.
            workout.ai_analysis = json.dumps(full_report)

            # 6. PHASE 4 : NOTIFICATION (Le Feed)
            # On utilise le message court g√©n√©r√© par l'IA pour le feed
            feed_msg = analysis_result.get("feed_message", "Analyse de s√©ance disponible.")

            return schemas.FeedItemCreate(
                type=schemas.FeedItemType.ANALYSIS,
                title="Rapport de S√©ance",
                message=feed_msg,
                priority=5,
                action_payload={
                    "route": "/history", 
                    # On pourra passer des args pour ouvrir directement le d√©tail plus tard
                    "args": {"workout_id": workout.id}
                }
            )

        except Exception as e:
            print(f"‚ö†Ô∏è Erreur IA Analysis: {e}")
            # Fallback : Si l'IA plante, on ne cr√©e pas de FeedItem, 
            # ou on pourrait en cr√©er un g√©n√©rique. Ici on choisit la discr√©tion.
            return None

    def _clean_json(self, text: str) -> str:
        """Extrait le JSON si l'IA ajoute du markdown."""
        try:
            pattern = r"```(?:json)?\s*([\s\S]*?)\s*```"
            match = re.search(pattern, text)
            if match:
                return match.group(1).strip()
            return text.strip()
        except:
            return text-e 

-e 
================================================================================
üìÑ FICHIER : backend/app/validators/athlete_profile_validators.py
================================================================================
"""
Validateurs pour les profils athl√®tes
"""
import re
from datetime import datetime
from typing import Dict, Any, Optional

# Sport ‚Üî Position coh√©rence
VALID_SPORT_POSITIONS = {
    'Rugby': ['Pilier', 'Talonneur', '2√®me ligne', '3√®me ligne', 'Demi', 'Centre', 'Ailier', 'Arri√®re'],
    'Football': ['Gardien', 'D√©fenseur', 'Milieu', 'Attaquant'],
    'Basketball': ['Meneur', 'Arri√®re', 'Ailier', 'Ailier fort', 'Pivot'],
    'Natation': ['Nage libre', 'Dos', 'Brasse', 'Papillon', '4 nages'],
    'Athl√©tisme': ['Sprint', 'Demi-fond', 'Fond', 'Haies', 'Saut', 'Lancer'],
    'Musculation': ['Powerlifting', 'Weightlifting', 'Bodybuilding', 'CrossFit', 'G√©n√©ral'],
    'Cyclisme': ['Route', 'Piste', 'VTT', 'Cyclocross'],
    'Triathlon': ['Sprint', 'Olympique', 'Half-Ironman', 'Ironman'],
    'Escalade': ['Bloc', 'Difficult√©', 'Vitesse'],
    'Arts martiaux': ['Judo', 'BJJ', 'Boxe', 'Muay Thai', 'MMA']
}

VALID_COMPETITION_LEVELS = ['D√©butant', 'Amateur', 'Comp√©titeur', '√âlite', 'Professionnel']

def validate_athlete_profile(profile_data: Dict[str, Any]) -> bool:
    """
    Valide la coh√©rence globale d'un profil athl√®te
    """
    errors = []
    
    # Valider le contexte sportif
    if 'sport_context' in profile_data:
        errors.extend(validate_sport_context(profile_data['sport_context']))
    
    # Valider les m√©triques physiques
    if 'physical_metrics' in profile_data:
        errors.extend(validate_physical_metrics(profile_data['physical_metrics']))
    
    # Valider les objectifs
    if 'goals' in profile_data:
        errors.extend(validate_goals(profile_data['goals']))
    
    # Valider les informations de base
    if 'basic_info' in profile_data:
        errors.extend(validate_basic_info(profile_data['basic_info']))
    
    if errors:
        raise ValueError(" | ".join(errors))
    
    return True

def validate_sport_context(sport_context: Dict[str, Any]) -> list:
    """Valide le contexte sportif"""
    errors = []
    
    primary_sport = sport_context.get('primary_sport')
    if not primary_sport:
        errors.append("Le sport principal est requis")
    
    playing_position = sport_context.get('playing_position')
    if playing_position and primary_sport in VALID_SPORT_POSITIONS:
        if playing_position not in VALID_SPORT_POSITIONS[primary_sport]:
            errors.append(f"Position '{playing_position}' invalide pour le sport '{primary_sport}'")
    
    competition_level = sport_context.get('competition_level')
    if competition_level and competition_level not in VALID_COMPETITION_LEVELS:
        errors.append(f"Niveau de comp√©tition invalide: {competition_level}")
    
    training_history = sport_context.get('training_history_years')
    if training_history and (training_history < 0 or training_history > 50):
        errors.append(f"Ann√©es d'entra√Ænement invalides: {training_history}")
    
    return errors

def validate_physical_metrics(metrics: Dict[str, Any]) -> list:
    """Valide les m√©triques physiques"""
    errors = []
    
    # Validation BMI
    if metrics.get('weight') and metrics.get('height'):
        weight = float(metrics['weight'])
        height = float(metrics['height']) / 100  # Convertir en m√®tres
        
        if height <= 0:
            errors.append("La taille doit √™tre positive")
        elif weight <= 0:
            errors.append("Le poids doit √™tre positif")
        else:
            bmi = weight / (height ** 2)
            if not (16 <= bmi <= 40):
                errors.append(f"BMI {bmi:.1f} hors des limites plausibles (16-40)")
    
    # Validation fr√©quence cardiaque
    resting_hr = metrics.get('resting_heart_rate')
    if resting_hr:
        hr = float(resting_hr)
        if not (30 <= hr <= 120):
            errors.append(f"Fr√©quence cardiaque au repos {hr} hors limites (30-120 bpm)")
    
    # Validation pourcentage de graisse
    body_fat = metrics.get('body_fat_estimate')
    if body_fat:
        bf = float(body_fat)
        if not (5 <= bf <= 50):
            errors.append(f"Pourcentage de graisse {bf}% hors limites (5-50%)")
    
    # Validation qualit√© de sommeil
    sleep_quality = metrics.get('sleep_quality_average')
    if sleep_quality:
        sq = float(sleep_quality)
        if not (1 <= sq <= 10):
            errors.append(f"Qualit√© de sommeil {sq} hors √©chelle (1-10)")
    
    return errors

def validate_goals(goals: Dict[str, Any]) -> list:
    """Valide les objectifs"""
    errors = []
    
    primary_goal = goals.get('primary_goal')
    if not primary_goal:
        errors.append("L'objectif principal est requis")
    
    target_date = goals.get('target_date')
    if target_date:
        try:
            target = datetime.strptime(target_date, '%Y-%m-%d')
            if target < datetime.now():
                errors.append("La date cible ne peut pas √™tre dans le pass√©")
        except ValueError:
            errors.append(f"Format de date invalide: {target_date}")
    
    # Valider les m√©triques cibles
    target_metrics = goals.get('target_metrics', {})
    for metric, value in target_metrics.items():
        if isinstance(value, (int, float)) and value <= 0:
            errors.append(f"M√©trique cible '{metric}' doit √™tre positive")
    
    return errors

def validate_basic_info(basic_info: Dict[str, Any]) -> list:
    """Valide les informations de base"""
    errors = []
    
    # Validation email
    email = basic_info.get('email')
    if email and not re.match(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$', email):
        errors.append("Format d'email invalide")
    
    # Validation date de naissance
    birth_date = basic_info.get('birth_date')
    if birth_date:
        try:
            birth = datetime.strptime(birth_date, '%Y-%m-%d')
            if birth > datetime.now():
                errors.append("La date de naissance ne peut pas √™tre dans le futur")
            
            # Calculer l'√¢ge
            age = datetime.now().year - birth.year
            if not (10 <= age <= 100):
                errors.append(f"√Çge {age} hors limites plausibles (10-100 ans)")
        except ValueError:
            errors.append(f"Format de date de naissance invalide: {birth_date}")
    
    # Validation √¢ge biologique
    bio_age = basic_info.get('biological_age')
    if bio_age and (bio_age < 10 or bio_age > 100):
        errors.append(f"√Çge biologique {bio_age} hors limites (10-100)")
    
    # Validation √¢ge d'entra√Ænement
    training_age = basic_info.get('training_age')
    if training_age and (training_age < 0 or training_age > 50):
        errors.append(f"√Çge d'entra√Ænement {training_age} hors limites (0-50)")
    
    # Validation sexe biologique
    biological_sex = basic_info.get('biological_sex')
    if biological_sex and biological_sex not in ['Homme', 'Femme', 'Autre']:
        errors.append(f"Sexe biologique invalide: {biological_sex}")
    
    # Validation main dominante
    dominant_hand = basic_info.get('dominant_hand')
    if dominant_hand and dominant_hand not in ['Droitier', 'Gaucher', 'Ambidextre']:
        errors.append(f"Main dominante invalide: {dominant_hand}")
    
    return errors

def validate_sport_position(sport: str, position: Optional[str]) -> bool:
    """Valide la coh√©rence sport/position"""
    if not position:
        return True
    
    if sport in VALID_SPORT_POSITIONS:
        return position in VALID_SPORT_POSITIONS[sport]
    
    return True

def validate_competition_level(level: str) -> bool:
    """Valide le niveau de comp√©tition"""
    return level in VALID_COMPETITION_LEVELS if level else True

def validate_training_preferences(preferences: Dict[str, Any]) -> list:
    """Valide les pr√©f√©rences d'entra√Ænement"""
    errors = []
    
    max_duration = preferences.get('max_session_duration')
    if max_duration and (max_duration < 15 or max_duration > 240):
        errors.append(f"Dur√©e maximale de session {max_duration} hors limites (15-240 min)")
    
    feedback_style = preferences.get('feedback_style')
    if feedback_style and feedback_style not in ['Direct', 'Encourageant', 'Technique', 'Mixte']:
        errors.append(f"Style de feedback invalide: {feedback_style}")
    
    autonomy_preference = preferences.get('autonomy_preference')
    if autonomy_preference and autonomy_preference not in ['Faible', 'Moyenne', 'Forte']:
        errors.append(f"Pr√©f√©rence d'autonomie invalide: {autonomy_preference}")
    
    return errors

def validate_injury_prevention(injury_data: Dict[str, Any]) -> list:
    """Valide les donn√©es de pr√©vention des blessures"""
    errors = []
    
    medical_clearance = injury_data.get('medical_clearance')
    if medical_clearance is False:
        errors.append("Avis m√©dical requis pour l'entra√Ænement")
    
    return errors
-e 

-e 
================================================================================
üìÑ FICHIER : backend/backend/app/models/schemas.py
================================================================================
from pydantic import BaseModel, Field, field_validator
from typing import List, Optional, Dict, Any, Union
from datetime import date, datetime
from enum import Enum
import json

# --- ENUMS & TYPES ---

class SportType(str, Enum):
    RUGBY = "Rugby"
    FOOTBALL = "Football"
    CROSSFIT = "CrossFit"
    HYBRID = "Hybrid"
    RUNNING = "Running"
    OTHER = "Autre"

# --- SUB-SCHEMAS FOR PROFILE ---

class BasicInfo(BaseModel):
    pseudo: Optional[str] = None
    email: Optional[str] = None
    birth_date: Optional[str] = None
    training_age: Optional[int] = 0

class PhysicalMetrics(BaseModel):
    height: float = 0
    weight: float = 0
    body_fat: Optional[float] = None
    resting_hr: Optional[int] = None
    sleep_quality_avg: Optional[int] = 5

class SportContext(BaseModel):
    sport: SportType = SportType.OTHER
    position: Optional[str] = None
    level: str = "Interm√©diaire"
    equipment: List[str] = ["Standard"]

class TrainingPreferences(BaseModel):
    days_available: List[str] = []
    duration_min: int = 60
    preferred_split: str = "Upper/Lower"

# --- MAIN PROFILE SCHEMAS ---

class AthleteProfileBase(BaseModel):
    basic_info: BasicInfo = Field(default_factory=BasicInfo)
    physical_metrics: PhysicalMetrics = Field(default_factory=PhysicalMetrics)
    sport_context: SportContext = Field(default_factory=SportContext)
    training_preferences: TrainingPreferences = Field(default_factory=TrainingPreferences)
    goals: Dict[str, Any] = {}
    constraints: Dict[str, Any] = {}
    injury_prevention: Dict[str, Any] = {}
    performance_baseline: Dict[str, Any] = {}

class AthleteProfileCreate(AthleteProfileBase):
    pass

class AthleteProfileResponse(AthleteProfileBase):
    id: int
    user_id: int
    created_at: datetime
    class Config:
        from_attributes = True

# --- MEMORY SCHEMAS (FIXED) ---

class CoachMemoryResponse(BaseModel):
    id: int
    # [FIX] Removed invalid .get() call here
    readiness_score: int = Field(alias="current_context", default=50)
    current_phase: str = "G√©n√©ral"
    flags: Dict[str, bool] = {}
    insights: Dict[str, Any] = {}
    
    @field_validator('readiness_score', mode='before')
    def extract_readiness(cls, v):
        if isinstance(v, dict):
            return v.get('readiness_score', 50)
        return v

    class Config:
        from_attributes = True

# --- LEGACY SCHEMAS (KEPT FOR COMPATIBILITY) ---

class WorkoutSetBase(BaseModel):
    exercise_name: str
    set_order: int
    weight: Union[float, str] = 0.0
    reps: Union[float, str] = 0.0
    rpe: Optional[float] = 0.0
    rest_seconds: int = 0
    metric_type: str = "LOAD_REPS"

    @field_validator('weight', 'reps', mode='before')
    def parse_polymorphic_fields(cls, v):
        if isinstance(v, str):
            v = v.strip().replace(',', '.')
            if ':' in v:
                parts = v.split(':')
                try:
                    seconds = 0.0
                    if len(parts) == 2:
                        seconds = float(parts[0]) * 60 + float(parts[1])
                    elif len(parts) == 3:
                        seconds = float(parts[0]) * 3600 + float(parts[1]) * 60 + float(parts[2])
                    return seconds
                except ValueError:
                    return 0.0
            try:
                return float(v)
            except ValueError:
                return 0.0
        return v

class WorkoutSetCreate(WorkoutSetBase):
    pass

class WorkoutSessionCreate(BaseModel):
    date: date
    duration: float
    rpe: float
    energy_level: int = 5
    notes: Optional[str] = None
    sets: List[WorkoutSetCreate] = []

class WorkoutSetResponse(WorkoutSetBase):
    id: int
    weight: float
    reps: float
    class Config:
        from_attributes = True

class WorkoutSessionResponse(WorkoutSessionCreate):
    id: int
    ai_analysis: Optional[str] = None
    sets: List[WorkoutSetResponse] = []
    class Config:
        from_attributes = True

class GenerateWorkoutRequest(BaseModel):
    profile_data: Dict[str, Any]
    context: Dict[str, Any]

class AIExercise(BaseModel):
    name: str
    sets: int
    reps: Union[str, int]
    rest: int
    tips: str
    recording_mode: str = "LOAD_REPS"
    @field_validator('reps')
    def force_string_reps(cls, v):
        return str(v)

class AIWorkoutPlan(BaseModel):
    title: str
    coach_comment: str
    warmup: List[str]
    exercises: List[AIExercise]
    cooldown: List[str]

# --- USER & AUTH ---
class UserCreate(BaseModel):
    username: str
    email: Optional[str] = None
    password: str

class UserResponse(BaseModel):
    id: int
    username: str
    email: Optional[str] = None
    profile_data: Optional[str] = None 
    class Config:
        from_attributes = True

class Token(BaseModel):
    access_token: str
    token_type: str

class TokenData(BaseModel):
    username: Optional[str] = None

# --- FEED ---
class FeedItemType(str, Enum):
    INFO = "INFO"
    ANALYSIS = "ANALYSIS"
    ACTION = "ACTION"
    ALERT = "ALERT"

class FeedItemCreate(BaseModel):
    type: FeedItemType
    title: str
    message: str
    priority: int = 1
    action_payload: Optional[Dict[str, Any]] = None

class FeedItemResponse(FeedItemCreate):
    id: str
    is_read: bool
    is_completed: bool
    created_at: datetime
    
    @field_validator('action_payload', mode='before')
    def parse_payload(cls, v):
        if isinstance(v, str) and v.strip():
            try: return json.loads(v)
            except: return None
        return v
    class Config:
        from_attributes = True

# --- PERFORMANCE ---
class OneRepMaxRequest(BaseModel):
    weight: float
    reps: int
class OneRepMaxResponse(BaseModel):
    estimated_1rm: float
    method_used: str
class ACWRRequest(BaseModel):
    history: List[Dict[str, Any]]
class ACWRResponse(BaseModel):
    ratio: float
    status: str
    color: str
    message: str
class ProfileAuditRequest(BaseModel):
    profile_data: Dict[str, Any]
class ProfileAuditResponse(BaseModel):
    markdown_report: str
class StrategyResponse(BaseModel):
    periodization_title: str
    phases: List[Any]
class WeeklyPlanResponse(BaseModel):
    schedule: List[Any]
    reasoning: str
class UserProfileUpdate(BaseModel):
    profile_data: Dict[str, Any]


# --- MISSING SCHEMAS FOR UPDATES ---

class AthleteProfileUpdate(AthleteProfileBase):
    pass

class ProfileSectionUpdate(BaseModel):
    section_data: Dict[str, Any]

class DailyMetrics(BaseModel):
    date: str
    weight: Optional[float] = None
    sleep_quality: Optional[int] = None
    resting_heart_rate: Optional[int] = None
    hrv: Optional[int] = None
    energy_level: Optional[int] = None
    muscle_soreness: Optional[int] = None
    perceived_stress: Optional[int] = None
    sleep_duration: Optional[float] = None

class GoalProgressUpdate(BaseModel):
    progress_value: int
    progress_note: Optional[str] = None
    achieved: bool = False
-e 

-e 
================================================================================
üìÑ FICHIER : backend/backend/app/models/sql_models.py
================================================================================
from sqlalchemy import Column, Integer, String, Float, Date, ForeignKey, DateTime, Text, Boolean, JSON
from sqlalchemy.orm import relationship
from sqlalchemy.sql import func
from app.core.database import Base

class User(Base):
    __tablename__ = "users"

    id = Column(Integer, primary_key=True, index=True)
    username = Column(String, unique=True, index=True)
    email = Column(String, unique=True, index=True, nullable=True)
    hashed_password = Column(String)
    
    # Anciens champs (Legacy support)
    profile_data = Column(Text, nullable=True)
    strategy_data = Column(Text, nullable=True)
    weekly_plan_data = Column(Text, nullable=True)
    draft_workout_data = Column(Text, nullable=True)

    # Relations
    workouts = relationship("WorkoutSession", back_populates="owner")
    feed_items = relationship("FeedItem", back_populates="owner", cascade="all, delete-orphan")
    
    # [NOUVEAU] Relation vers le Profil Enrichi
    athlete_profile = relationship("AthleteProfile", back_populates="user", uselist=False, cascade="all, delete-orphan")

class AthleteProfile(Base):
    __tablename__ = "athlete_profiles"

    id = Column(Integer, primary_key=True, index=True)
    user_id = Column(Integer, ForeignKey("users.id"), unique=True)

    # Blocs de donn√©es JSONB (Stock√©s en JSON/Text compatible SQLite/PG)
    basic_info = Column(JSON, default={})           # pseudo, email, birth_date, sex...
    physical_metrics = Column(JSON, default={})     # height, weight, body_fat...
    sport_context = Column(JSON, default={})        # sport, position, level...
    performance_baseline = Column(JSON, default={}) # strength, endurance, PRs...
    injury_prevention = Column(JSON, default={})    # history, weak_links...
    training_preferences = Column(JSON, default={}) # split, duration, times...
    goals = Column(JSON, default={})                # primary, milestones...
    constraints = Column(JSON, default={})          # time, travel, stress...

    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), onupdate=func.now())

    user = relationship("User", back_populates="athlete_profile")
    coach_memory = relationship("CoachMemory", back_populates="athlete_profile", uselist=False, cascade="all, delete-orphan")

class CoachMemory(Base):
    __tablename__ = "coach_memories"

    id = Column(Integer, primary_key=True, index=True)
    athlete_profile_id = Column(Integer, ForeignKey("athlete_profiles.id"), unique=True)

    # M√©moire contextuelle IA
    metadata_info = Column(JSON, default={})          # total_interactions, trust_score
    current_context = Column(JSON, default={})        # fatigue, readiness, phase
    response_patterns = Column(JSON, default={})      # volume_tolerance, recovery_speed
    performance_baselines = Column(JSON, default={})  # tracked_prs, progression_rate
    adaptation_signals = Column(JSON, default={})     # stagnation, overtraining_risk
    sport_specific_insights = Column(JSON, default={})# transfer_exercises, key_skills
    training_history_summary = Column(JSON, default={})
    athlete_preferences = Column(JSON, default={})    # copy for quick access
    coach_notes = Column(JSON, default={})            # active_hypotheses
    memory_flags = Column(JSON, default={})           # needs_deload, injury_risk

    last_updated = Column(DateTime(timezone=True), server_default=func.now())

    athlete_profile = relationship("AthleteProfile", back_populates="coach_memory")

# --- MOD√àLES EXISTANTS (Conserv√©s pour compatibilit√©) ---

class WorkoutSession(Base):
    __tablename__ = "workout_sessions"
    id = Column(Integer, primary_key=True, index=True)
    user_id = Column(Integer, ForeignKey("users.id"))
    date = Column(Date, index=True)
    duration = Column(Float)
    rpe = Column(Float)
    energy_level = Column(Integer, default=5) 
    notes = Column(Text, nullable=True)      
    ai_analysis = Column(Text, nullable=True)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    owner = relationship("User", back_populates="workouts")
    sets = relationship("WorkoutSet", back_populates="session", cascade="all, delete-orphan")

class WorkoutSet(Base):
    __tablename__ = "workout_sets"
    id = Column(Integer, primary_key=True, index=True)
    session_id = Column(Integer, ForeignKey("workout_sessions.id"))
    exercise_name = Column(String, index=True)
    set_order = Column(Integer)
    weight = Column(Float, default=0.0)
    reps = Column(Float, default=0.0)
    rpe = Column(Float, default=0.0)
    rest_seconds = Column(Integer, default=0)
    metric_type = Column(String, nullable=False, default="LOAD_REPS") 
    session = relationship("WorkoutSession", back_populates="sets")

class FeedItem(Base):
    __tablename__ = "feed_items"
    id = Column(String, primary_key=True, index=True)
    user_id = Column(Integer, ForeignKey("users.id"))
    type = Column(String, index=True)
    title = Column(String)
    message = Column(String)
    action_payload = Column(Text, nullable=True)
    is_read = Column(Boolean, default=False)
    is_completed = Column(Boolean, default=False)
    priority = Column(Integer, default=1)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    owner = relationship("User", back_populates="feed_items")
-e 

-e 
================================================================================
üìÑ FICHIER : backend/backend/app/routers/profiles.py
================================================================================
from fastapi import APIRouter, Depends, HTTPException, status
from sqlalchemy.orm import Session
from app.core.database import get_db
from app.models import sql_models, schemas
from app.dependencies import get_current_user
from app.services.coach_logic import CoachLogic

router = APIRouter(
    prefix="/api/v1",
    tags=["Athlete Profile & Memory"]
)

# --- PROFILE ENDPOINTS ---

@router.get("/profiles/me", response_model=schemas.AthleteProfileResponse)
async def get_my_profile(
    current_user: sql_models.User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    if not current_user.athlete_profile:
        # Cr√©ation √† la vol√©e si inexistant (Migration Lazy)
        profile = sql_models.AthleteProfile(user_id=current_user.id)
        db.add(profile)
        db.commit()
        db.refresh(profile)
        return profile
    return current_user.athlete_profile

@router.post("/profiles/complete", response_model=schemas.AthleteProfileResponse)
async def complete_profile(
    profile_data: schemas.AthleteProfileCreate,
    current_user: sql_models.User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    # 1. Validation Logic
    sport = profile_data.sport_context.sport
    pos = profile_data.sport_context.position
    if not CoachLogic.validate_sport_position(sport, pos):
        raise HTTPException(status_code=400, detail=f"Position {pos} invalide pour le sport {sport}")

    # 2. Update or Create
    db_profile = current_user.athlete_profile
    if not db_profile:
        db_profile = sql_models.AthleteProfile(user_id=current_user.id)
        db.add(db_profile)
    
    # Map fields
    db_profile.basic_info = profile_data.basic_info.dict()
    db_profile.physical_metrics = profile_data.physical_metrics.dict()
    db_profile.sport_context = profile_data.sport_context.dict()
    db_profile.training_preferences = profile_data.training_preferences.dict()
    db_profile.goals = profile_data.goals
    db_profile.constraints = profile_data.constraints
    
    # 3. Initialize Coach Memory if needed
    if not db_profile.coach_memory:
        memory = CoachLogic.initialize_memory(db_profile)
        db.add(memory)
    
    db.commit()
    db.refresh(db_profile)
    return db_profile

# --- MEMORY ENDPOINTS ---

@router.get("/coach-memories/me", response_model=schemas.CoachMemoryResponse)
async def get_my_coach_memory(
    current_user: sql_models.User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    if not current_user.athlete_profile or not current_user.athlete_profile.coach_memory:
        raise HTTPException(status_code=404, detail="Profil ou M√©moire introuvable. Compl√©tez votre profil.")
    
    return current_user.athlete_profile.coach_memory

@router.post("/coach-memories/recalculate")
async def force_recalculate(
    current_user: sql_models.User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    profile = current_user.athlete_profile
    if not profile or not profile.coach_memory:
        raise HTTPException(status_code=404, detail="Introuvable")
        
    CoachLogic.update_daily(profile.coach_memory, profile)
    db.commit()
    return {"status": "updated", "new_readiness": profile.coach_memory.current_context.get('readiness_score')}
-e 

-e 
================================================================================
üìÑ FICHIER : backend/backend/app/services/coach_logic.py
================================================================================
from datetime import date
from typing import Dict, Any
from app.models import sql_models

# Constantes de validation
VALID_SPORT_POSITIONS = {
    'Rugby': ['Pilier', 'Talonneur', '2√®me ligne', '3√®me ligne', 'Demi', 'Centre', 'Ailier', 'Arri√®re'],
    'Football': ['Gardien', 'D√©fenseur', 'Milieu', 'Attaquant'],
}

class CoachLogic:
    @staticmethod
    def validate_sport_position(sport: str, position: str) -> bool:
        if sport in VALID_SPORT_POSITIONS:
            if position and position not in VALID_SPORT_POSITIONS[sport]:
                return False
        return True

    @staticmethod
    def initialize_memory(profile: sql_models.AthleteProfile) -> sql_models.CoachMemory:
        """Cr√©e la structure initiale de la m√©moire du coach based sur le profil"""
        sport = profile.sport_context.get('sport', 'Autre')
        
        # Insights initiaux
        insights = {
            "primary_sport": sport,
            "specificity_index": "High" if sport in ['Rugby', 'Football'] else "Medium",
            "focus_areas": ["Strength", "Hypertrophy"] # D√©faut
        }
        
        # Contexte initial
        context = {
            "macrocycle_phase": "Adaptation Anatomique",
            "fatigue_state": "Fresh",
            "readiness_score": 100,
            "season_week": 1
        }
        
        # Drapeaux
        flags = {
            "needs_deload": False,
            "injury_risk": False,
            "adaptation_window_open": True
        }

        memory = sql_models.CoachMemory(
            athlete_profile_id=profile.id,
            sport_specific_insights=insights,
            current_context=context,
            memory_flags=flags,
            coach_notes={"initialization": f"Profil cr√©√© le {date.today()}"}
        )
        return memory

    @staticmethod
    def calculate_readiness(profile: sql_models.AthleteProfile) -> int:
        """Algorithme simple de readiness bas√© sur les m√©triques"""
        base_score = 80
        
        # Impact Sommeil
        sleep = profile.physical_metrics.get('sleep_quality_avg', 5)
        if sleep >= 8: base_score += 10
        elif sleep <= 4: base_score -= 20
        
        # Impact Stress
        stress = profile.constraints.get('work_stress_level', 5)
        if stress >= 8: base_score -= 15
        
        return max(0, min(100, base_score))

    @staticmethod
    def update_daily(memory: sql_models.CoachMemory, profile: sql_models.AthleteProfile):
        """Mise √† jour quotidienne (Batch Job simulation)"""
        # Recalcul Readiness
        new_readiness = CoachLogic.calculate_readiness(profile)
        
        # Update Context
        current_context = dict(memory.current_context or {})
        current_context['readiness_score'] = new_readiness
        
        if new_readiness < 40:
            current_context['fatigue_state'] = "High"
        elif new_readiness < 70:
            current_context['fatigue_state'] = "Moderate"
        else:
            current_context['fatigue_state'] = "Optimal"
            
        memory.current_context = current_context
        
        # Update Flags
        flags = dict(memory.memory_flags or {})
        flags['needs_deload'] = new_readiness < 30
        flags['adaptation_window_open'] = new_readiness > 70
        memory.memory_flags = flags
-e 

-e 
================================================================================
üìÑ FICHIER : backend/check_jwt_config.py
================================================================================
import os
import jwt
from datetime import datetime, timedelta
from dotenv import load_dotenv

load_dotenv()

SECRET_KEY = os.getenv("SECRET_KEY")
ALGORITHM = os.getenv("ALGORITHM", "HS256")
ACCESS_TOKEN_EXPIRE_MINUTES = int(os.getenv("ACCESS_TOKEN_EXPIRE_MINUTES", 30))

print("üîß Configuration JWT Actuelle:")
print(f"   SECRET_KEY: {'SET' if SECRET_KEY else 'NOT SET'}")
print(f"   ALGORITHM: {ALGORITHM}")
print(f"   ACCESS_TOKEN_EXPIRE_MINUTES: {ACCESS_TOKEN_EXPIRE_MINUTES} min")

# V√©rifier si on peut g√©n√©rer un token
if SECRET_KEY:
    print("\nüß™ Test de g√©n√©ration de token...")
    
    data = {"sub": "testuser", "exp": datetime.utcnow() + timedelta(minutes=30)}
    token = jwt.encode(data, SECRET_KEY, algorithm=ALGORITHM)
    
    print(f"   Token g√©n√©r√©: {token[:50]}...")
    
    # V√©rifier qu'on peut le d√©coder
    try:
        decoded = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        print(f"   ‚úÖ Token d√©cod√©: {decoded}")
    except Exception as e:
        print(f"   ‚ùå Erreur d√©codage: {e}")
else:
    print("\n‚ùå SECRET_KEY non d√©finie!")
    print("   D√©finissez-la dans .env:")
    print("   SECRET_KEY=votre_clef_secrete_tres_longue_ici")
-e 

-e 
================================================================================
üìÑ FICHIER : backend/debug_tokens.py
================================================================================
#!/usr/bin/env python3
"""
Debug JWT tokens - V√©rifie pourquoi les tokens sont rejet√©s
"""

import jwt
import os
from datetime import datetime
import sys

# Charger les variables d'environnement
from dotenv import load_dotenv
load_dotenv()

SECRET_KEY = os.getenv("SECRET_KEY", "fallback_secret_key_if_env_missing")
ALGORITHM = "HS256"

def decode_and_verify(token: str):
    """D√©code et v√©rifie un token JWT"""
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        
        print("‚úÖ TOKEN VALIDE")
        print(f"üìã Payload: {payload}")
        
        # V√©rifier l'expiration
        exp_timestamp = payload.get('exp')
        if exp_timestamp:
            exp_date = datetime.fromtimestamp(exp_timestamp)
            now = datetime.now()
            time_left = exp_date - now
            
            print(f"‚è∞ Expiration: {exp_date}")
            print(f"‚è≥ Temps restant: {time_left}")
            
            if time_left.total_seconds() < 0:
                print("‚ùå TOKEN EXPIRE !")
            else:
                print("‚úÖ Token encore valide")
        
        return payload
    except jwt.ExpiredSignatureError:
        print("‚ùå ERREUR: Token expir√©")
        return None
    except jwt.InvalidTokenError as e:
        print(f"‚ùå ERREUR: Token invalide - {e}")
        return None

def main():
    if len(sys.argv) < 2:
        print("Usage: python debug_tokens.py <token>")
        print("Ou: python debug_tokens.py --check-all")
        return
    
    if sys.argv[1] == "--check-all":
        # V√©rifier les tokens stock√©s dans un √©chantillon
        sample_tokens = []
        # Vous pouvez ajouter des tokens de test ici
        for token in sample_tokens:
            decode_and_verify(token)
    else:
        token = sys.argv[1]
        decode_and_verify(token)

if __name__ == "__main__":
    main()-e 

-e 
================================================================================
üìÑ FICHIER : backend/disable_coach_cache.py
================================================================================
import re

with open('app/routers/coach.py', 'r') as f:
    content = f.read()

# Supprimer TOUTES les r√©f√©rences √† cache dans ce fichier
content = re.sub(r'from app\.core\.cache import[^\n]*\n', '', content)
content = re.sub(r'@cached_response[^\n]*\n', '', content)

with open('app/routers/coach.py', 'w') as f:
    f.write(new_content)

print("‚úÖ Cache compl√®tement d√©sactiv√© pour coach.py")
-e 

-e 
================================================================================
üìÑ FICHIER : backend/exe.py
================================================================================
#!/usr/bin/env python3
"""
SCRIPT DE CORRECTION BACKEND TITANFLOW
Ajoute les 3 endpoints manquants pour le frontend Flutter
"""

import os
import re
from pathlib import Path

# Configuration des chemins
BASE_DIR = Path(__file__).parent
USER_ROUTER_FILE = BASE_DIR / "app" / "routers" / "user.py"
SCHEMAS_FILE = BASE_DIR / "app" / "models" / "schemas.py"

# Nouveau contenu pour les endpoints
NEW_ENDPOINTS = """
@router.get("/profile/complete", response_model=schemas.AthleteProfileResponse)
async def get_complete_profile(
    current_user: sql_models.User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    \"\"\"
    R√©cup√®re le profil athl√®te complet.
    Compatibilit√© avec l'ancien syst√®me (profile_data) et le nouveau (AthleteProfile).
    \"\"\"
    # V√©rifier d'abord si l'utilisateur a un profil athl√®te v2
    if current_user.athlete_profile:
        return current_user.athlete_profile
    
    # Fallback : retourner les donn√©es du profil legacy
    if current_user.profile_data:
        try:
            profile_data = json.loads(current_user.profile_data)
            return {
                "id": current_user.id,
                "user_id": current_user.id,
                "created_at": current_user.created_at if hasattr(current_user, 'created_at') else None,
                "basic_info": {
                    "pseudo": current_user.username,
                    "email": current_user.email,
                    **profile_data.get('basic_info', {})
                },
                "physical_metrics": profile_data.get('physical_metrics', {}),
                "sport_context": profile_data.get('sport_context', {}),
                "training_preferences": profile_data.get('training_preferences', {}),
                "goals": profile_data.get('goals', {}),
                "constraints": profile_data.get('constraints', {}),
                "injury_prevention": profile_data.get('injury_prevention', {}),
                "performance_baseline": profile_data.get('performance_baseline', {})
            }
        except Exception as e:
            raise HTTPException(
                status_code=500,
                detail=f"Erreur lecture profil: {str(e)}"
            )
    
    raise HTTPException(
        status_code=404,
        detail="Profil non trouv√©. Compl√©tez votre profil d'abord."
    )

@router.post("/profile/complete", response_model=schemas.AthleteProfileResponse)
async def create_complete_profile(
    profile_data: dict,
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    \"\"\"
    Cr√©e ou met √† jour un profil athl√®te complet.
    Supporte √† la fois l'ancien format (profile_data) et le nouveau (sections).
    \"\"\"
    try:
        # V√©rifier si l'utilisateur a d√©j√† un profil athl√®te v2
        if current_user.athlete_profile:
            # Mettre √† jour le profil existant
            profile = current_user.athlete_profile
            for section, data in profile_data.items():
                if hasattr(profile, section):
                    setattr(profile, section, json.dumps(data))
        else:
            # Cr√©er un nouveau profil athl√®te
            profile = sql_models.AthleteProfile(
                user_id=current_user.id,
                basic_info=json.dumps(profile_data.get('basic_info', {})),
                physical_metrics=json.dumps(profile_data.get('physical_metrics', {})),
                sport_context=json.dumps(profile_data.get('sport_context', {})),
                training_preferences=json.dumps(profile_data.get('training_preferences', {})),
                goals=json.dumps(profile_data.get('goals', {})),
                constraints=json.dumps(profile_data.get('constraints', {})),
                injury_prevention=json.dumps(profile_data.get('injury_prevention', {})),
                performance_baseline=json.dumps(profile_data.get('performance_baseline', {}))
            )
            db.add(profile)
        
        # Mettre √† jour aussi le profil legacy pour compatibilit√©
        current_user.profile_data = json.dumps(profile_data)
        
        db.commit()
        db.refresh(profile)
        
        return profile
        
    except Exception as e:
        db.rollback()
        raise HTTPException(
            status_code=500,
            detail=f"Erreur cr√©ation profil: {str(e)}"
        )

@router.post("/profile/sections/{section}")
async def update_profile_section(
    section: str,
    section_data: dict,
    db: Session = Depends(get_db),
    current_user: sql_models.User = Depends(get_current_user)
):
    \"\"\"
    Met √† jour une section sp√©cifique du profil.
    Section peut √™tre: basic_info, physical_metrics, sport_context, etc.
    \"\"\"
    # Liste des sections valides
    valid_sections = [
        'basic_info', 'physical_metrics', 'sport_context',
        'training_preferences', 'goals', 'constraints',
        'injury_prevention', 'performance_baseline'
    ]
    
    if section not in valid_sections:
        raise HTTPException(
            status_code=400,
            detail=f"Section invalide. Options: {', '.join(valid_sections)}"
        )
    
    try:
        # Mettre √† jour le profil athl√®te v2 si existant
        if current_user.athlete_profile:
            profile = current_user.athlete_profile
            setattr(profile, section, json.dumps(section_data))
        else:
            # Si pas de profil athl√®te, cr√©er un profil minimal
            profile = sql_models.AthleteProfile(user_id=current_user.id)
            setattr(profile, section, json.dumps(section_data))
            db.add(profile)
        
        # Mettre √† jour aussi le profil legacy
        legacy_data = {}
        if current_user.profile_data:
            try:
                legacy_data = json.loads(current_user.profile_data)
            except:
                pass
        
        legacy_data[section] = section_data
        current_user.profile_data = json.dumps(legacy_data)
        
        db.commit()
        
        return {
            "status": "success",
            "message": f"Section '{section}' mise √† jour",
            "section": section,
            "data": section_data
        }
        
    except Exception as e:
        db.rollback()
        raise HTTPException(
            status_code=500,
            detail=f"Erreur mise √† jour section: {str(e)}"
        )
"""

def backup_file(file_path):
    """Cr√©e une sauvegarde du fichier"""
    backup_path = file_path.with_suffix(f"{file_path.suffix}.backup")
    if file_path.exists():
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        with open(backup_path, 'w', encoding='utf-8') as f:
            f.write(content)
        print(f"‚úÖ Backup cr√©√©: {backup_path}")
    return backup_path

def update_user_router():
    """Ajoute les endpoints manquants au routeur user"""
    print(f"üîß Mise √† jour du fichier: {USER_ROUTER_FILE}")
    
    if not USER_ROUTER_FILE.exists():
        print(f"‚ùå Fichier introuvable: {USER_ROUTER_FILE}")
        return False
    
    backup_file(USER_ROUTER_FILE)
    
    with open(USER_ROUTER_FILE, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # V√©rifier si les endpoints existent d√©j√†
    if "@router.get(\"/profile/complete\")" in content:
        print("‚úÖ Endpoint GET /profile/complete existe d√©j√†")
        return True
    
    # Trouver la fin du fichier avant les imports optionnels
    lines = content.split('\n')
    new_lines = []
    endpoint_added = False
    
    for i, line in enumerate(lines):
        new_lines.append(line)
        
        # Ajouter apr√®s le dernier endpoint existant (avant la fin du fichier)
        if line.strip() == "@router.put(\"/profile\")" and not endpoint_added:
            # V√©rifier que nous avons bien un bloc de fonction complet
            j = i + 1
            while j < len(lines) and not (lines[j].strip().startswith("@") or lines[j].strip().startswith("async def")):
                j += 1
            
            # Trouver la fin de la fonction
            k = j
            while k < len(lines) and (lines[k].strip() or not lines[k].strip().startswith("async def")):
                k += 1
            
            # Ins√©rer nos nouveaux endpoints
            new_lines.append("\n" + NEW_ENDPOINTS)
            endpoint_added = True
    
    # Si nous n'avons pas trouv√© d'endpoint existant, ajouter √† la fin
    if not endpoint_added:
        # Trouver la derni√®re ligne avec du code
        last_code_line = len(lines) - 1
        while last_code_line > 0 and not lines[last_code_line].strip():
            last_code_line -= 1
        
        # Ins√©rer avant la derni√®re ligne (g√©n√©ralement vide)
        new_lines.insert(last_code_line + 1, "\n" + NEW_ENDPOINTS)
    
    # √âcrire le nouveau contenu
    new_content = '\n'.join(new_lines)
    
    with open(USER_ROUTER_FILE, 'w', encoding='utf-8') as f:
        f.write(new_content)
    
    print("‚úÖ Endpoints ajout√©s avec succ√®s!")
    return True

def verify_imports():
    """V√©rifie que les imports n√©cessaires sont pr√©sents"""
    print("üîç V√©rification des imports...")
    
    with open(USER_ROUTER_FILE, 'r', encoding='utf-8') as f:
        content = f.read()
    
    required_imports = [
        "from fastapi import APIRouter, Depends, HTTPException, status",
        "from sqlalchemy.orm import Session",
        "from app.core.database import get_db",
        "from app.models import sql_models, schemas",
        "from app.dependencies import get_current_user",
        "import json"
    ]
    
    missing_imports = []
    for imp in required_imports:
        if imp not in content:
            missing_imports.append(imp)
    
    if missing_imports:
        print("‚ö†Ô∏è  Imports manquants:")
        for imp in missing_imports:
            print(f"   - {imp}")
        
        # Ajouter les imports manquants
        with open(USER_ROUTER_FILE, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        # Trouver o√π ins√©rer les imports
        insert_line = 0
        for i, line in enumerate(lines):
            if line.startswith("router = APIRouter"):
                insert_line = i
                break
        
        # Ajouter les imports avant le routeur
        for imp in missing_imports:
            lines.insert(insert_line, imp + "\n")
        
        with open(USER_ROUTER_FILE, 'w', encoding='utf-8') as f:
            f.writelines(lines)
        
        print("‚úÖ Imports ajout√©s automatiquement")
    
    print("‚úÖ Tous les imports sont pr√©sents")

def check_schemas_existence():
    """V√©rifie que les sch√©mas n√©cessaires existent"""
    print("üîç V√©rification des sch√©mas Pydantic...")
    
    if not SCHEMAS_FILE.exists():
        print(f"‚ùå Fichier de sch√©mas introuvable: {SCHEMAS_FILE}")
        return False
    
    with open(SCHEMAS_FILE, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # V√©rifier que AthleteProfileResponse existe
    if "class AthleteProfileResponse" not in content:
        print("‚ö†Ô∏è  Le sch√©ma AthleteProfileResponse n'existe pas")
        print("   Mais les endpoints utilisent une r√©ponse dict directement, donc √ßa devrait fonctionner")
    
    print("‚úÖ Sch√©mas v√©rifi√©s")
    return True

def create_test_script():
    """Cr√©e un script de test pour v√©rifier les nouveaux endpoints"""
    test_script = BASE_DIR / "test_new_endpoints.py"
    
    test_content = '''#!/usr/bin/env python3
"""
Script de test pour les nouveaux endpoints
"""

import requests
import json
import sys

# Configuration
BASE_URL = "http://localhost:8000"
TOKEN = None  # Remplacer par votre token JWT

def test_endpoint(method, endpoint, data=None):
    """Test un endpoint et affiche le r√©sultat"""
    url = f"{BASE_URL}{endpoint}"
    headers = {"Authorization": f"Bearer {TOKEN}"} if TOKEN else {}
    
    try:
        if method == "GET":
            response = requests.get(url, headers=headers)
        elif method == "POST":
            headers["Content-Type"] = "application/json"
            response = requests.post(url, headers=headers, json=data)
        elif method == "PUT":
            headers["Content-Type"] = "application/json"
            response = requests.put(url, headers=headers, json=data)
        else:
            print(f"‚ùå M√©thode non support√©e: {method}")
            return
        
        print(f"\nüîç Test {method} {endpoint}")
        print(f"   Status Code: {response.status_code}")
        
        if response.status_code == 200:
            print(f"   ‚úÖ Succ√®s!")
            try:
                result = response.json()
                print(f"   R√©ponse: {json.dumps(result, indent=2, ensure_ascii=False)[:200]}...")
            except:
                print(f"   R√©ponse: {response.text[:200]}...")
        else:
            print(f"   ‚ùå Erreur!")
            print(f"   Message: {response.text}")
            
    except Exception as e:
        print(f"   ‚ùå Exception: {e}")

if __name__ == "__main__":
    if len(sys.argv) > 1:
        TOKEN = sys.argv[1]
    
    if not TOKEN:
        print("‚ö†Ô∏è  Aucun token fourni. Seuls les endpoints publics seront test√©s.")
        print("   Usage: python test_new_endpoints.py <votre_token_jwt>")
    
    # Test des nouveaux endpoints
    print("üß™ TEST DES NOUVEAUX ENDPOINTS")
    print("=" * 50)
    
    # 1. Test GET /user/profile/complete
    test_endpoint("GET", "/user/profile/complete")
    
    # 2. Test POST /user/profile/complete (avec des donn√©es d'exemple)
    sample_profile = {
        "basic_info": {
            "pseudo": "test_athlete",
            "email": "test@example.com",
            "training_age": 3
        },
        "physical_metrics": {
            "weight": 75.5,
            "height": 180,
            "body_fat": 15.0
        },
        "sport_context": {
            "sport": "Rugby",
            "level": "Interm√©diaire",
            "position": "Demi"
        }
    }
    test_endpoint("POST", "/user/profile/complete", sample_profile)
    
    # 3. Test POST /user/profile/sections/basic_info
    basic_info_update = {
        "pseudo": "athlete_updated",
        "email": "updated@example.com",
        "birth_date": "1990-01-01"
    }
    test_endpoint("POST", "/user/profile/sections/basic_info", basic_info_update)
    
    # 4. Test POST /user/profile/sections/physical_metrics
    physical_update = {
        "weight": 76.0,
        "height": 180,
        "body_fat": 14.5
    }
    test_endpoint("POST", "/user/profile/sections/physical_metrics", physical_update)
    
    print("\n" + "=" * 50)
    print("‚úÖ Tests termin√©s!")
'''
    
    with open(test_script, 'w', encoding='utf-8') as f:
        f.write(test_content)
    
    # Rendre le script ex√©cutable
    test_script.chmod(0o755)
    
    print(f"‚úÖ Script de test cr√©√©: {test_script}")
    print(f"   Usage: python {test_script} <votre_token_jwt>")

def main():
    """Fonction principale"""
    print("""
    ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
    ‚ïë     SCRIPT DE CORRECTION BACKEND TITANFLOW       ‚ïë
    ‚ïë            üîß FIX ENDPOINTS MANQUANTS            ‚ïë
    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """)
    
    print("üìã Endpoints √† ajouter:")
    print("   1. GET  /user/profile/complete")
    print("   2. POST /user/profile/complete")
    print("   3. POST /user/profile/sections/{section}")
    print()
    
    # 1. V√©rifier les pr√©requis
    print("üîç V√©rification des pr√©requis...")
    if not USER_ROUTER_FILE.exists():
        print(f"‚ùå Fichier routeur introuvable: {USER_ROUTER_FILE}")
        print("   Assurez-vous d'ex√©cuter ce script depuis le dossier backend/")
        return
    
    # 2. V√©rifier les sch√©mas
    check_schemas_existence()
    
    # 3. V√©rifier les imports
    verify_imports()
    
    # 4. Mettre √† jour le routeur
    print("\nüîß Application des corrections...")
    success = update_user_router()
    
    if success:
        print("\n‚úÖ CORRECTIONS APPLIQU√âES AVEC SUCC√àS!")
        print()
        print("üìã R√âSUM√â DES CHANGEMENTS:")
        print("   - ‚úÖ GET  /user/profile/complete ‚Üí R√©cup√®re le profil complet")
        print("   - ‚úÖ POST /user/profile/complete ‚Üí Cr√©e/m√†j profil complet")
        print("   - ‚úÖ POST /user/profile/sections/{section} ‚Üí M√†j section sp√©cifique")
        print()
        print("üöÄ POUR TESTER:")
        print("   1. Red√©marrez le serveur backend:")
        print("      python -m uvicorn app.main:app --reload")
        print()
        print("   2. Testez avec le script fourni:")
        print("      python test_new_endpoints.py <votre_token>")
        print()
        print("   3. Le frontend Flutter peut maintenant appeler ces endpoints:")
        print("      - POST /user/profile/sections/basic_info")
        print("      - POST /user/profile/complete")
        print("      - GET  /user/profile/complete")
        print()
        
        # 5. Cr√©er le script de test
        create_test_script()
        
        print("üí° REMARQUES:")
        print("   - Les donn√©es sont sauvegard√©es dans les 2 syst√®mes (ancien et nouveau)")
        print("   - Compatibilit√© totale avec le frontend Flutter existant")
        print("   - Les tokens JWT existants continuent de fonctionner")
        
    else:
        print("\n‚ùå √âCHEC DE LA MISE √Ä JOUR")
        print("   V√©rifiez les logs ci-dessus et contactez le support")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\n‚èπÔ∏è  Op√©ration annul√©e par l'utilisateur")
    except Exception as e:
        print(f"\n‚ùå ERREUR INATTENDUE: {e}")
        print("   Contactez le support technique")-e 

-e 
================================================================================
üìÑ FICHIER : backend/fix_coach_cache.py
================================================================================
#!/usr/bin/env python3
"""
Retire le d√©corateur @cached_response de la fonction generate_workout
qui cause l'erreur de s√©rialisation JSON
"""

import re

with open('app/routers/coach.py', 'r') as f:
    content = f.read()

# Trouver la fonction generate_workout
pattern = r'@cached_response\(ttl_hours=6\)\s*\nasync def generate_workout'
match = re.search(pattern, content)

if match:
    print("üîß Retrait du d√©corateur @cached_response probl√©matique...")
    
    # Retirer la ligne du d√©corateur
    new_content = content.replace(match.group(0), 'async def generate_workout')
    
    with open('app/routers/coach.py', 'w') as f:
        f.write(new_content)
    
    print("‚úÖ D√©corateur retir√© avec succ√®s")
else:
    print("‚úÖ Le d√©corateur n'est pas pr√©sent ou a d√©j√† √©t√© retir√©")
-e 

-e 
================================================================================
üìÑ FICHIER : backend/fix_table.py
================================================================================
from sqlalchemy import text
from app.core.database import engine

print("‚ò¢Ô∏è  D√©marrage de l'option nucl√©aire...")

with engine.connect() as connection:
    trans = connection.begin()
    try:
        # On supprime d'abord les s√©ances (qui d√©pendent des users)
        connection.execute(text("DROP TABLE IF EXISTS workout_sessions CASCADE;"))
        print("üí• Table workout_sessions pulv√©ris√©e.")
        
        # On supprime ensuite les users (pour recr√©er la table avec l'email)
        connection.execute(text("DROP TABLE IF EXISTS users CASCADE;"))
        print("üí• Table users pulv√©ris√©e.")
        
        trans.commit()
    except Exception as e:
        trans.rollback()
        print(f"‚ùå Erreur : {e}")

print("‚úÖ Termin√©. Red√©marre le serveur pour recr√©er les tables propres.")-e 

-e 
================================================================================
üìÑ FICHIER : backend/frontend/app.py
================================================================================
import streamlit as st
import requests
import pandas as pd
import os  # <--- Ajout de l'import os

# Configuration de la page
st.set_page_config(page_title="TitanFlow Pro", page_icon="‚ö°", layout="wide")

# L'URL de ton API (Backend)
# En PROD (Render) : Il utilisera la variable d'environnement BACKEND_URL
# En LOCAL (Ton PC) : Il utilisera http://127.0.0.1:8000 par d√©faut
API_URL = os.getenv("BACKEND_URL", "http://127.0.0.1:8000")

# --- GESTION DE LA SESSION (Token) ---
if "token" not in st.session_state:
    st.session_state.token = None

def login():
    st.sidebar.header("üîê Connexion")
    username = st.sidebar.text_input("Pseudo")
    password = st.sidebar.text_input("Mot de passe", type="password")
    
    if st.sidebar.button("Se connecter"):
        try:
            # Appel √† l'API pour r√©cup√©rer le token
            response = requests.post(
                f"{API_URL}/auth/token",
                data={"username": username, "password": password}
            )
            if response.status_code == 200:
                st.session_state.token = response.json()["access_token"]
                st.sidebar.success("Connect√© !")
                st.rerun()
            else:
                st.sidebar.error("Erreur de connexion")
        except Exception as e:
            st.sidebar.error(f"API introuvable : {e}")

def logout():
    if st.sidebar.button("Se d√©connecter"):
        st.session_state.token = None
        st.rerun()

# --- INTERFACE PRINCIPALE ---
st.title("‚ö° TitanFlow : Monitoring Athl√©tique")

# V√©rification de l'√©tat de l'API
try:
    health = requests.get(f"{API_URL}/health").json()
    st.success(f"Backend connect√© v{health['version']}")
except:
    st.error("üö® Le Backend semble √©teint. V√©rifie que l'URL est correcte.")

# Gestion Login/Logout
if not st.session_state.token:
    st.info("Veuillez vous connecter dans la barre lat√©rale pour acc√©der aux donn√©es.")
    login()
else:
    logout()
    st.write("---")
    
    # Onglets de l'application
    tab1, tab2 = st.tabs(["üèãÔ∏è‚Äç‚ôÇÔ∏è Historique", "‚ûï Nouvelle S√©ance"])
    
    # --- ONGLET 1 : HISTORIQUE ---
    with tab1:
        st.subheader("Vos s√©ances enregistr√©es")
        headers = {"Authorization": f"Bearer {st.session_state.token}"}
        
        try:
            res = requests.get(f"{API_URL}/workouts/", headers=headers)
            if res.status_code == 200:
                workouts = res.json()
                if workouts:
                    df = pd.DataFrame(workouts)
                    st.dataframe(df, use_container_width=True)
                else:
                    st.info("Aucune s√©ance trouv√©e.")
            else:
                st.error("Erreur chargement donn√©es")
        except Exception as e:
            st.error(f"Erreur : {e}")

    # --- ONGLET 2 : AJOUTER S√âANCE ---
    with tab2:
        st.subheader("Enregistrer un entra√Ænement")
        with st.form("new_workout"):
            col1, col2 = st.columns(2)
            date = col1.date_input("Date")
            duration = col2.number_input("Dur√©e (min)", min_value=0, value=60)
            rpe = st.slider("Intensit√© (RPE)", 0, 10, 5)
            
            submitted = st.form_submit_button("Sauvegarder")
            
            if submitted:
                payload = {
                    "date": str(date),
                    "duration": duration,
                    "rpe": rpe
                }
                res = requests.post(f"{API_URL}/workouts/", json=payload, headers=headers)
                
                if res.status_code == 200:
                    st.success("S√©ance enregistr√©e ! üéâ")
                    st.rerun()
                else:
                    st.error(f"Erreur : {res.text}")-e 

-e 
================================================================================
üìÑ FICHIER : backend/generate_map.py
================================================================================
import sys
import os
import json
import inspect
import re

# --- CONFIGURATION DES CHEMINS ---
# On se place dans backend/tools/, on veut remonter √† la racine du projet
BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '../../'))
BACKEND_DIR = os.path.join(BASE_DIR, 'backend')
PUBSPEC_PATH = os.path.join(BASE_DIR, 'pubspec.yaml') # Hypoth√®se: pubspec √† la racine ou dans frontend/
REQUIREMENTS_PATH = os.path.join(BACKEND_DIR, 'requirements.txt')
MAP_FILE = os.path.join(BASE_DIR, 'TITANFLOW_MAP.json')

# Ajout du backend au path pour les imports SQLAlchemy
sys.path.append(BACKEND_DIR)

try:
    from app.models import sql_models
    from app.core.database import Base
except ImportError as e:
    print(f"‚ùå Erreur d'import Backend : {e}")
    print("Assurez-vous d'√™tre dans l'environnement virtuel (venv).")
    sys.exit(1)

def get_flutter_environment():
    """Lit le pubspec.yaml pour extraire les versions."""
    env = {"sdk": "Unknown", "packages": {}}
    
    if not os.path.exists(PUBSPEC_PATH):
        print(f"‚ö†Ô∏è  pubspec.yaml introuvable ici : {PUBSPEC_PATH}")
        return env

    with open(PUBSPEC_PATH, 'r', encoding='utf-8') as f:
        lines = f.readlines()
        
    in_dependencies = False
    for line in lines:
        line = line.strip()
        # SDK Version
        if line.startswith('sdk:'):
            env['sdk'] = line.split(':')[1].strip()
        
        # D√©tection bloc dependencies
        if line == 'dependencies:':
            in_dependencies = True
            continue
        if line == 'dev_dependencies:':
            in_dependencies = False
            continue
            
        # Extraction packages cl√©s (http, provider, etc.)
        if in_dependencies and ':' in line:
            parts = line.split(':')
            pkg_name = parts[0].strip()
            version = parts[1].strip()
            # On garde seulement les packages int√©ressants pour l'IA
            interesting_libs = ['http', 'shared_preferences', 'intl', 'flutter', 'provider', 'riverpod', 'bloc', 'go_router']
            if pkg_name in interesting_libs:
                env['packages'][pkg_name] = version
                
    return env

def get_backend_environment():
    """Lit le requirements.txt pour les versions Python."""
    env = {"python": sys.version.split()[0], "libs": {}}
    
    if not os.path.exists(REQUIREMENTS_PATH):
        return env

    with open(REQUIREMENTS_PATH, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if '==' in line:
                parts = line.split('==')
                env['libs'][parts[0]] = parts[1]
            elif '>=' in line:
                parts = line.split('>=')
                env['libs'][parts[0]] = f">={parts[1]}"
                
    return env

def generate_db_schema():
    """Scan les mod√®les SQLAlchemy."""
    schema = {}
    for name, obj in inspect.getmembers(sql_models):
        if inspect.isclass(obj) and issubclass(obj, Base) and obj != Base:
            table_name = obj.__tablename__
            columns = []
            
            for col in obj.__table__.columns:
                col_type = str(col.type)
                info = f"{col.name} ({col_type})"
                if col.primary_key: info += " [PK]"
                if col.foreign_keys: info += " [FK]"
                columns.append(info)
            
            schema[table_name] = {
                "description": f"Table SQL {table_name}",
                "columns": columns
            }
    return schema

def main():
    print("üöÄ D√©marrage de la cartographie TitanFlow...")
    
    # 1. Chargement existant
    data = {}
    if os.path.exists(MAP_FILE):
        try:
            with open(MAP_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
        except json.JSONDecodeError:
            print("‚ö†Ô∏è  Fichier JSON corrompu, cr√©ation d'un nouveau.")

    # 2. Mise √† jour Environment (Versions)
    print("üì¶ Analyse des d√©pendances (Flutter & Python)...")
    data["environment"] = {
        "frontend": get_flutter_environment(),
        "backend": get_backend_environment()
    }

    # 3. Mise √† jour DB Schema
    print("üóÑÔ∏è  Introspection de la Base de Donn√©es...")
    data["database_schema"] = generate_db_schema()

    # 4. Sauvegarde
    with open(MAP_FILE, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    
    print(f"‚úÖ Succ√®s ! Fichier mis √† jour : {MAP_FILE}")
    print("   -> L'IA aura maintenant une vision exacte de tes versions et de ta BDD.")

if __name__ == "__main__":
    main()-e 

-e 
================================================================================
üìÑ FICHIER : backend/implement.py
================================================================================
import os
import sys
import logging
from sqlalchemy import create_engine, text, inspect
from dotenv import load_dotenv
import json

# Configuration
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger("TitanUpgrader")

# --- CHEMIN DYNAMIQUE ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# 1. NOUVEAUX MOD√àLES SQL
SQL_MODELS_CONTENT = """from sqlalchemy import Column, Integer, String, Float, Date, ForeignKey, DateTime, Text, Boolean, JSON
from sqlalchemy.orm import relationship
from sqlalchemy.sql import func
from app.core.database import Base

class User(Base):
    __tablename__ = "users"

    id = Column(Integer, primary_key=True, index=True)
    username = Column(String, unique=True, index=True)
    email = Column(String, unique=True, index=True, nullable=True)
    hashed_password = Column(String)
    
    # Anciens champs (Legacy support)
    profile_data = Column(Text, nullable=True)
    strategy_data = Column(Text, nullable=True)
    weekly_plan_data = Column(Text, nullable=True)
    draft_workout_data = Column(Text, nullable=True)

    # Relations
    workouts = relationship("WorkoutSession", back_populates="owner")
    feed_items = relationship("FeedItem", back_populates="owner", cascade="all, delete-orphan")
    
    # [NOUVEAU] Relation vers le Profil Enrichi
    athlete_profile = relationship("AthleteProfile", back_populates="user", uselist=False, cascade="all, delete-orphan")

class AthleteProfile(Base):
    __tablename__ = "athlete_profiles"

    id = Column(Integer, primary_key=True, index=True)
    user_id = Column(Integer, ForeignKey("users.id"), unique=True)

    # Blocs de donn√©es JSONB
    basic_info = Column(JSON, default={})
    physical_metrics = Column(JSON, default={})
    sport_context = Column(JSON, default={})
    performance_baseline = Column(JSON, default={})
    injury_prevention = Column(JSON, default={})
    training_preferences = Column(JSON, default={})
    goals = Column(JSON, default={})
    constraints = Column(JSON, default={})

    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), onupdate=func.now())

    user = relationship("User", back_populates="athlete_profile")
    coach_memory = relationship("CoachMemory", back_populates="athlete_profile", uselist=False, cascade="all, delete-orphan")

class CoachMemory(Base):
    __tablename__ = "coach_memories"

    id = Column(Integer, primary_key=True, index=True)
    athlete_profile_id = Column(Integer, ForeignKey("athlete_profiles.id"), unique=True)

    # M√©moire contextuelle IA
    metadata_info = Column(JSON, default={})
    current_context = Column(JSON, default={})
    response_patterns = Column(JSON, default={})
    performance_baselines = Column(JSON, default={})
    adaptation_signals = Column(JSON, default={})
    sport_specific_insights = Column(JSON, default={})
    training_history_summary = Column(JSON, default={})
    athlete_preferences = Column(JSON, default={})
    coach_notes = Column(JSON, default={})
    memory_flags = Column(JSON, default={})

    last_updated = Column(DateTime(timezone=True), server_default=func.now())

    athlete_profile = relationship("AthleteProfile", back_populates="coach_memory")

# --- MOD√àLES EXISTANTS ---
class WorkoutSession(Base):
    __tablename__ = "workout_sessions"
    id = Column(Integer, primary_key=True, index=True)
    user_id = Column(Integer, ForeignKey("users.id"))
    date = Column(Date, index=True)
    duration = Column(Float)
    rpe = Column(Float)
    energy_level = Column(Integer, default=5) 
    notes = Column(Text, nullable=True)      
    ai_analysis = Column(Text, nullable=True)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    owner = relationship("User", back_populates="workouts")
    sets = relationship("WorkoutSet", back_populates="session", cascade="all, delete-orphan")

class WorkoutSet(Base):
    __tablename__ = "workout_sets"
    id = Column(Integer, primary_key=True, index=True)
    session_id = Column(Integer, ForeignKey("workout_sessions.id"))
    exercise_name = Column(String, index=True)
    set_order = Column(Integer)
    weight = Column(Float, default=0.0)
    reps = Column(Float, default=0.0)
    rpe = Column(Float, default=0.0)
    rest_seconds = Column(Integer, default=0)
    metric_type = Column(String, nullable=False, default="LOAD_REPS") 
    session = relationship("WorkoutSession", back_populates="sets")

class FeedItem(Base):
    __tablename__ = "feed_items"
    id = Column(String, primary_key=True, index=True)
    user_id = Column(Integer, ForeignKey("users.id"))
    type = Column(String, index=True)
    title = Column(String)
    message = Column(String)
    action_payload = Column(Text, nullable=True)
    is_read = Column(Boolean, default=False)
    is_completed = Column(Boolean, default=False)
    priority = Column(Integer, default=1)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    owner = relationship("User", back_populates="feed_items")
"""

# 2. SCHEMAS PYDANTIC
SCHEMAS_CONTENT = """from pydantic import BaseModel, Field, field_validator
from typing import List, Optional, Dict, Any, Union
from datetime import date, datetime
from enum import Enum
import json

# --- ENUMS & TYPES ---
class SportType(str, Enum):
    RUGBY = "Rugby"
    FOOTBALL = "Football"
    CROSSFIT = "CrossFit"
    HYBRID = "Hybrid"
    RUNNING = "Running"
    OTHER = "Autre"

# --- SUB-SCHEMAS FOR PROFILE ---
class BasicInfo(BaseModel):
    pseudo: Optional[str] = None
    email: Optional[str] = None
    birth_date: Optional[str] = None
    training_age: Optional[int] = 0

class PhysicalMetrics(BaseModel):
    height: float = 0
    weight: float = 0
    body_fat: Optional[float] = None
    resting_hr: Optional[int] = None
    sleep_quality_avg: Optional[int] = 5

class SportContext(BaseModel):
    sport: SportType = SportType.OTHER
    position: Optional[str] = None
    level: str = "Interm√©diaire"
    equipment: List[str] = ["Standard"]

class TrainingPreferences(BaseModel):
    days_available: List[str] = []
    duration_min: int = 60
    preferred_split: str = "Upper/Lower"

# --- MAIN PROFILE SCHEMAS ---
class AthleteProfileBase(BaseModel):
    basic_info: BasicInfo = Field(default_factory=BasicInfo)
    physical_metrics: PhysicalMetrics = Field(default_factory=PhysicalMetrics)
    sport_context: SportContext = Field(default_factory=SportContext)
    training_preferences: TrainingPreferences = Field(default_factory=TrainingPreferences)
    goals: Dict[str, Any] = {}
    constraints: Dict[str, Any] = {}
    injury_prevention: Dict[str, Any] = {}
    performance_baseline: Dict[str, Any] = {}

class AthleteProfileCreate(AthleteProfileBase):
    pass

class AthleteProfileResponse(AthleteProfileBase):
    id: int
    user_id: int
    created_at: datetime
    class Config:
        from_attributes = True

# --- MEMORY SCHEMAS ---
class CoachMemoryResponse(BaseModel):
    id: int
    readiness_score: int = Field(alias="current_context", default={}).get("readiness_score", 0)
    current_phase: str = "G√©n√©ral"
    flags: Dict[str, bool] = {}
    insights: Dict[str, Any] = {}
    
    @field_validator('readiness_score', mode='before')
    def extract_readiness(cls, v):
        if isinstance(v, dict): return v.get('readiness_score', 50)
        return v

    class Config:
        from_attributes = True

# --- LEGACY SCHEMAS ---
class WorkoutSetBase(BaseModel):
    exercise_name: str
    set_order: int
    weight: Union[float, str] = 0.0
    reps: Union[float, str] = 0.0
    rpe: Optional[float] = 0.0
    rest_seconds: int = 0
    metric_type: str = "LOAD_REPS"

    @field_validator('weight', 'reps', mode='before')
    def parse_polymorphic_fields(cls, v):
        if isinstance(v, str):
            v = v.strip().replace(',', '.')
            if ':' in v:
                parts = v.split(':')
                try:
                    seconds = 0.0
                    if len(parts) == 2:
                        seconds = float(parts[0]) * 60 + float(parts[1])
                    elif len(parts) == 3:
                        seconds = float(parts[0]) * 3600 + float(parts[1]) * 60 + float(parts[2])
                    return seconds
                except ValueError:
                    return 0.0
            try:
                return float(v)
            except ValueError:
                return 0.0
        return v

class WorkoutSetCreate(WorkoutSetBase):
    pass

class WorkoutSessionCreate(BaseModel):
    date: date
    duration: float
    rpe: float
    energy_level: int = 5
    notes: Optional[str] = None
    sets: List[WorkoutSetCreate] = []

class WorkoutSetResponse(WorkoutSetBase):
    id: int
    weight: float
    reps: float
    class Config:
        from_attributes = True

class WorkoutSessionResponse(WorkoutSessionCreate):
    id: int
    ai_analysis: Optional[str] = None
    sets: List[WorkoutSetResponse] = []
    class Config:
        from_attributes = True

class GenerateWorkoutRequest(BaseModel):
    profile_data: Dict[str, Any]
    context: Dict[str, Any]

class AIExercise(BaseModel):
    name: str
    sets: int
    reps: Union[str, int]
    rest: int
    tips: str
    recording_mode: str = "LOAD_REPS"
    @field_validator('reps')
    def force_string_reps(cls, v):
        return str(v)

class AIWorkoutPlan(BaseModel):
    title: str
    coach_comment: str
    warmup: List[str]
    exercises: List[AIExercise]
    cooldown: List[str]

class UserCreate(BaseModel):
    username: str
    email: Optional[str] = None
    password: str

class UserResponse(BaseModel):
    id: int
    username: str
    email: Optional[str] = None
    profile_data: Optional[str] = None 
    class Config:
        from_attributes = True

class Token(BaseModel):
    access_token: str
    token_type: str

class TokenData(BaseModel):
    username: Optional[str] = None

class FeedItemType(str, Enum):
    INFO = "INFO"
    ANALYSIS = "ANALYSIS"
    ACTION = "ACTION"
    ALERT = "ALERT"

class FeedItemCreate(BaseModel):
    type: FeedItemType
    title: str
    message: str
    priority: int = 1
    action_payload: Optional[Dict[str, Any]] = None

class FeedItemResponse(FeedItemCreate):
    id: str
    is_read: bool
    is_completed: bool
    created_at: datetime
    
    @field_validator('action_payload', mode='before')
    def parse_payload(cls, v):
        if isinstance(v, str) and v.strip():
            try: return json.loads(v)
            except: return None
        return v
    class Config:
        from_attributes = True

# --- PERFORMANCE ---
class OneRepMaxRequest(BaseModel):
    weight: float
    reps: int
class OneRepMaxResponse(BaseModel):
    estimated_1rm: float
    method_used: str
class ACWRRequest(BaseModel):
    history: List[Dict[str, Any]]
class ACWRResponse(BaseModel):
    ratio: float
    status: str
    color: str
    message: str
class ProfileAuditRequest(BaseModel):
    profile_data: Dict[str, Any]
class ProfileAuditResponse(BaseModel):
    markdown_report: str
class StrategyResponse(BaseModel):
    periodization_title: str
    phases: List[Any]
class WeeklyPlanResponse(BaseModel):
    schedule: List[Any]
    reasoning: str
class UserProfileUpdate(BaseModel):
    profile_data: Dict[str, Any]
"""

# 3. LOGIQUE M√âTIER
LOGIC_CONTENT = """from datetime import date
from typing import Dict, Any
from app.models import sql_models

VALID_SPORT_POSITIONS = {
    'Rugby': ['Pilier', 'Talonneur', '2√®me ligne', '3√®me ligne', 'Demi', 'Centre', 'Ailier', 'Arri√®re'],
    'Football': ['Gardien', 'D√©fenseur', 'Milieu', 'Attaquant'],
}

class CoachLogic:
    @staticmethod
    def validate_sport_position(sport: str, position: str) -> bool:
        if sport in VALID_SPORT_POSITIONS:
            if position and position not in VALID_SPORT_POSITIONS[sport]:
                return False
        return True

    @staticmethod
    def initialize_memory(profile: sql_models.AthleteProfile) -> sql_models.CoachMemory:
        sport = profile.sport_context.get('sport', 'Autre')
        insights = {
            "primary_sport": sport,
            "specificity_index": "High" if sport in ['Rugby', 'Football'] else "Medium",
            "focus_areas": ["Strength", "Hypertrophy"] 
        }
        context = {
            "macrocycle_phase": "Adaptation Anatomique",
            "fatigue_state": "Fresh",
            "readiness_score": 100,
            "season_week": 1
        }
        flags = {
            "needs_deload": False,
            "injury_risk": False,
            "adaptation_window_open": True
        }
        memory = sql_models.CoachMemory(
            athlete_profile_id=profile.id,
            sport_specific_insights=insights,
            current_context=context,
            memory_flags=flags,
            coach_notes={"initialization": f"Profil cr√©√© le {date.today()}"}
        )
        return memory

    @staticmethod
    def calculate_readiness(profile: sql_models.AthleteProfile) -> int:
        base_score = 80
        sleep = profile.physical_metrics.get('sleep_quality_avg', 5)
        if sleep >= 8: base_score += 10
        elif sleep <= 4: base_score -= 20
        stress = profile.constraints.get('work_stress_level', 5)
        if stress >= 8: base_score -= 15
        return max(0, min(100, base_score))

    @staticmethod
    def update_daily(memory: sql_models.CoachMemory, profile: sql_models.AthleteProfile):
        new_readiness = CoachLogic.calculate_readiness(profile)
        current_context = dict(memory.current_context or {})
        current_context['readiness_score'] = new_readiness
        
        if new_readiness < 40:
            current_context['fatigue_state'] = "High"
        elif new_readiness < 70:
            current_context['fatigue_state'] = "Moderate"
        else:
            current_context['fatigue_state'] = "Optimal"
            
        memory.current_context = current_context
        flags = dict(memory.memory_flags or {})
        flags['needs_deload'] = new_readiness < 30
        flags['adaptation_window_open'] = new_readiness > 70
        memory.memory_flags = flags
"""

# 4. NOUVEAU ROUTEUR
ROUTER_CONTENT = """from fastapi import APIRouter, Depends, HTTPException, status
from sqlalchemy.orm import Session
from app.core.database import get_db
from app.models import sql_models, schemas
from app.dependencies import get_current_user
from app.services.coach_logic import CoachLogic

router = APIRouter(
    prefix="/api/v1",
    tags=["Athlete Profile & Memory"]
)

@router.get("/profiles/me", response_model=schemas.AthleteProfileResponse)
async def get_my_profile(
    current_user: sql_models.User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    if not current_user.athlete_profile:
        profile = sql_models.AthleteProfile(user_id=current_user.id)
        db.add(profile)
        db.commit()
        db.refresh(profile)
        return profile
    return current_user.athlete_profile

@router.post("/profiles/complete", response_model=schemas.AthleteProfileResponse)
async def complete_profile(
    profile_data: schemas.AthleteProfileCreate,
    current_user: sql_models.User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    sport = profile_data.sport_context.sport
    pos = profile_data.sport_context.position
    if not CoachLogic.validate_sport_position(sport, pos):
        raise HTTPException(status_code=400, detail=f"Position {pos} invalide pour le sport {sport}")

    db_profile = current_user.athlete_profile
    if not db_profile:
        db_profile = sql_models.AthleteProfile(user_id=current_user.id)
        db.add(db_profile)
    
    db_profile.basic_info = profile_data.basic_info.dict()
    db_profile.physical_metrics = profile_data.physical_metrics.dict()
    db_profile.sport_context = profile_data.sport_context.dict()
    db_profile.training_preferences = profile_data.training_preferences.dict()
    db_profile.goals = profile_data.goals
    db_profile.constraints = profile_data.constraints
    
    if not db_profile.coach_memory:
        memory = CoachLogic.initialize_memory(db_profile)
        db.add(memory)
    
    db.commit()
    db.refresh(db_profile)
    return db_profile

@router.get("/coach-memories/me", response_model=schemas.CoachMemoryResponse)
async def get_my_coach_memory(
    current_user: sql_models.User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    if not current_user.athlete_profile or not current_user.athlete_profile.coach_memory:
        raise HTTPException(status_code=404, detail="Profil ou M√©moire introuvable. Compl√©tez votre profil.")
    return current_user.athlete_profile.coach_memory

@router.post("/coach-memories/recalculate")
async def force_recalculate(
    current_user: sql_models.User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    profile = current_user.athlete_profile
    if not profile or not profile.coach_memory:
        raise HTTPException(status_code=404, detail="Introuvable")
        
    CoachLogic.update_daily(profile.coach_memory, profile)
    db.commit()
    return {"status": "updated", "new_readiness": profile.coach_memory.current_context.get('readiness_score')}
"""

# --- FONCTIONS UTILITAIRES ---
def write_file(path, content):
    full_path = os.path.join(BASE_DIR, path)
    os.makedirs(os.path.dirname(full_path), exist_ok=True)
    with open(full_path, "w", encoding="utf-8") as f:
        f.write(content)
    logger.info(f"‚úÖ Fichier √©crit : {path}")

def update_main_py():
    main_path = os.path.join(BASE_DIR, "app/main.py")
    if not os.path.exists(main_path):
        logger.error(f"‚ùå Impossible de trouver {main_path}. V√©rifiez l'emplacement du script.")
        return

    with open(main_path, "r", encoding="utf-8") as f:
        content = f.read()
    
    if "app.routers import profiles" in content:
        logger.info("‚ÑπÔ∏è main.py d√©j√† √† jour.")
        return

    content = content.replace(
        "from app.routers import performance, safety, auth, workouts, coach, user, feed",
        "from app.routers import performance, safety, auth, workouts, coach, user, feed, profiles"
    )
    
    if "app.include_router(feed.router)" in content:
        content = content.replace(
            "app.include_router(feed.router)",
            "app.include_router(feed.router)\napp.include_router(profiles.router)"
        )
    
    with open(main_path, "w", encoding="utf-8") as f:
        f.write(content)
    logger.info("‚úÖ main.py mis √† jour avec le routeur profiles.")

def migrate_database():
    """Migration SQL + Donn√©es"""
    logger.info("üîÑ D√©marrage de la migration base de donn√©es...")
    load_dotenv(os.path.join(BASE_DIR, ".env"))
    
    db_url = os.getenv("DATABASE_URL", "sqlite:///./sql_app.db")
    if db_url.startswith("postgres://"):
        db_url = db_url.replace("postgres://", "postgresql://", 1)
        
    engine = create_engine(db_url)
    
    with engine.connect() as conn:
        trans = conn.begin()
        try:
            # 1. CR√âATION DES TABLES V2
            logger.info("üÜï Cr√©ation des tables V2 (si absentes)...")
            conn.execute(text("""
                CREATE TABLE IF NOT EXISTS athlete_profiles (
                    id SERIAL PRIMARY KEY,
                    user_id INTEGER UNIQUE REFERENCES users(id) ON DELETE CASCADE,
                    basic_info JSON DEFAULT '{}',
                    physical_metrics JSON DEFAULT '{}',
                    sport_context JSON DEFAULT '{}',
                    performance_baseline JSON DEFAULT '{}',
                    injury_prevention JSON DEFAULT '{}',
                    training_preferences JSON DEFAULT '{}',
                    goals JSON DEFAULT '{}',
                    constraints JSON DEFAULT '{}',
                    created_at TIMESTAMPTZ DEFAULT NOW(),
                    updated_at TIMESTAMPTZ DEFAULT NOW()
                );
            """))
            conn.execute(text("""
                CREATE TABLE IF NOT EXISTS coach_memories (
                    id SERIAL PRIMARY KEY,
                    athlete_profile_id INTEGER UNIQUE REFERENCES athlete_profiles(id) ON DELETE CASCADE,
                    metadata_info JSON DEFAULT '{}',
                    current_context JSON DEFAULT '{}',
                    response_patterns JSON DEFAULT '{}',
                    performance_baselines JSON DEFAULT '{}',
                    adaptation_signals JSON DEFAULT '{}',
                    sport_specific_insights JSON DEFAULT '{}',
                    training_history_summary JSON DEFAULT '{}',
                    athlete_preferences JSON DEFAULT '{}',
                    coach_notes JSON DEFAULT '{}',
                    memory_flags JSON DEFAULT '{}',
                    last_updated TIMESTAMPTZ DEFAULT NOW()
                );
            """))

            # 1.5. PATCH DES TABLES EXISTANTES (Correction de l'erreur email)
            # Cette √©tape ajoute les colonnes AVANT d'essayer de les lire pour la migration
            logger.info("üîß V√©rification et r√©paration du sch√©ma 'users'...")
            try:
                # Tentative d'ajout des colonnes si elles manquent
                conn.execute(text("ALTER TABLE users ADD COLUMN IF NOT EXISTS email VARCHAR UNIQUE;"))
                conn.execute(text("ALTER TABLE users ADD COLUMN IF NOT EXISTS profile_data TEXT;"))
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Note sur le patch de sch√©ma: {e}")
                # En cas d'erreur ici, on continue, l'√©tape de migration de donn√©es √©chouera plus tard si critique

            # 2. MIGRATION DES DONN√âES
            logger.info("üîÑ Migration des donn√©es utilisateurs existants...")
            
            # Maintenant que la colonne email existe forc√©ment, cette requ√™te ne plantera plus
            result = conn.execute(text("SELECT id, username, email, profile_data FROM users"))
            users = result.fetchall()
            
            migrated_count = 0
            for u in users:
                # V√©rifier si profil existe d√©j√†
                exists = conn.execute(text(f"SELECT 1 FROM athlete_profiles WHERE user_id = {u.id}")).scalar()
                if not exists:
                    # Parsing old data
                    old_data = {}
                    if u.profile_data:
                        try:
                            old_data = json.loads(u.profile_data)
                        except:
                            pass
                    
                    # Construction new data structure
                    basic_info = json.dumps({"pseudo": u.username, "email": u.email})
                    physical_metrics = json.dumps({"weight": old_data.get("weight", 0), "height": old_data.get("height", 0)})
                    sport_context = json.dumps({"sport": old_data.get("sport", "Autre"), "level": old_data.get("level", "Interm√©diaire")})
                    
                    # Insert AthleteProfile
                    conn.execute(text("""
                        INSERT INTO athlete_profiles (user_id, basic_info, physical_metrics, sport_context)
                        VALUES (:uid, :bi, :pm, :sc)
                    """), {"uid": u.id, "bi": basic_info, "pm": physical_metrics, "sc": sport_context})
                    
                    # Get Profile ID
                    pid_result = conn.execute(text(f"SELECT id FROM athlete_profiles WHERE user_id = {u.id}"))
                    pid = pid_result.scalar()
                    
                    # Insert Default CoachMemory
                    if pid:
                        conn.execute(text("""
                            INSERT INTO coach_memories (athlete_profile_id, current_context, memory_flags)
                            VALUES (:pid, '{"readiness_score": 80, "phase": "Integration"}', '{"migrated": true}')
                        """), {"pid": pid})
                    
                    migrated_count += 1
            
            trans.commit()
            logger.info(f"‚úÖ Migration termin√©e : {migrated_count} utilisateurs migr√©s vers v2.")
            
        except Exception as e:
            trans.rollback()
            logger.error(f"‚ùå Erreur migration DB: {e}")
            raise e

# --- EXECUTION PRINCIPALE ---

if __name__ == "__main__":
    print("üöÄ D√©marrage de la mise √† jour TitanFlow v2 (Fix Email)...")
    
    # 1. √âcriture des fichiers
    write_file("app/models/sql_models.py", SQL_MODELS_CONTENT)
    write_file("app/models/schemas.py", SCHEMAS_CONTENT)
    write_file("app/services/coach_logic.py", LOGIC_CONTENT)
    write_file("app/routers/profiles.py", ROUTER_CONTENT)
    
    # 2. Mise √† jour main.py
    update_main_py()
    
    # 3. Migration DB
    try:
        migrate_database()
    except Exception as e:
        logger.error(f"‚ùå √âchec critique: {e}")

    print("\n‚ú® Installation termin√©e ! Relancez le serveur Uvicorn.")-e 

-e 
================================================================================
üìÑ FICHIER : backend/init_db.py
================================================================================
from app.core.database import engine, Base
from app.models import sql_models

print("üèóÔ∏è  Force-Cr√©ation des tables en cours...")
try:
    Base.metadata.create_all(bind=engine)
    print("‚úÖ  Succ√®s ! Toutes les tables (users + workout_sessions) sont pr√™tes.")
except Exception as e:
    print(f"‚ùå  Erreur : {e}")-e 

-e 
================================================================================
üìÑ FICHIER : backend/migrate_db.py
================================================================================
import os
import json
import sqlalchemy
from sqlalchemy import text, inspect
from dotenv import load_dotenv
import logging
import sys

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Charge les variables locales
load_dotenv()

DATABASE_URL = os.getenv("DATABASE_URL")

# Correction pour Render qui utilise parfois postgres:// au lieu de postgresql://
if DATABASE_URL and DATABASE_URL.startswith("postgres://"):
    DATABASE_URL = DATABASE_URL.replace("postgres://", "postgresql://", 1)

if not DATABASE_URL:
    logger.error("‚ùå Erreur : Pas de DATABASE_URL trouv√©e.")
    sys.exit(1)

logger.info(f"üîå Connexion √† la BDD...")
engine = sqlalchemy.create_engine(DATABASE_URL)

def table_exists(connection, table_name):
    """V√©rifie si une table existe"""
    inspector = inspect(engine)
    return table_name in inspector.get_table_names()

def column_exists(connection, table_name, column_name):
    """V√©rifie si une colonne existe dans une table"""
    inspector = inspect(engine)
    columns = [col['name'] for col in inspector.get_columns(table_name)]
    return column_name in columns

def migrate_v1_to_v2_data(connection):
    """
    Migre les donn√©es de l'ancien format (User.profile_data) 
    vers le nouveau format (AthleteProfile + CoachMemory).
    """
    logger.info("üîÑ V√©rification de la migration des donn√©es V1 -> V2...")
    
    # R√©cup√©rer les utilisateurs qui n'ont pas encore de profil V2
    # On v√©rifie s'il existe une entr√©e dans athlete_profiles pour chaque user
    sql_check = text("""
        SELECT u.id, u.username, u.email, u.profile_data 
        FROM users u 
        LEFT JOIN athlete_profiles ap ON u.id = ap.user_id 
        WHERE ap.id IS NULL
    """)
    
    result = connection.execute(sql_check)
    users_to_migrate = result.fetchall()
    
    if not users_to_migrate:
        logger.info("‚úÖ Toutes les donn√©es utilisateurs sont d√©j√† migr√©es.")
        return

    logger.info(f"üöÄ Migration de {len(users_to_migrate)} profils utilisateurs...")
    
    migrated_count = 0
    for u in users_to_migrate:
        try:
            # 1. Parsing de l'ancien JSON (s'il existe)
            old_data = {}
            if u.profile_data:
                try:
                    old_data = json.loads(u.profile_data)
                except:
                    logger.warning(f"‚ö†Ô∏è JSON invalide pour user {u.username}, utilisation par d√©faut.")
            
            # 2. Construction des nouvelles structures JSON
            # On mappe les anciens champs vers la nouvelle architecture
            basic_info = json.dumps({
                "pseudo": u.username, 
                "email": u.email,
                "training_age": old_data.get("experience_years", 0)
            })
            
            physical_metrics = json.dumps({
                "weight": float(old_data.get("weight", 0) or 0), 
                "height": float(old_data.get("height", 0) or 0),
                "body_fat": None
            })
            
            sport_context = json.dumps({
                "sport": old_data.get("sport", "Autre"), 
                "level": old_data.get("level", "Interm√©diaire"),
                "position": old_data.get("position", None)
            })
            
            goals = json.dumps({
                "primary_goal": old_data.get("goal", "Forme g√©n√©rale")
            })

            # 3. Insertion dans athlete_profiles
            # Note: On utilise des requ√™tes param√©tr√©es pour la s√©curit√©
            insert_profile_sql = text("""
                INSERT INTO athlete_profiles (user_id, basic_info, physical_metrics, sport_context, goals)
                VALUES (:uid, :bi, :pm, :sc, :gl)
                RETURNING id
            """)
            
            res = connection.execute(insert_profile_sql, {
                "uid": u.id, 
                "bi": basic_info, 
                "pm": physical_metrics, 
                "sc": sport_context,
                "gl": goals
            })
            
            # R√©cup√©ration de l'ID g√©n√©r√© pour lier la m√©moire
            profile_id = res.scalar()
            
            # 4. Initialisation de CoachMemory (Valeurs par d√©faut)
            insert_memory_sql = text("""
                INSERT INTO coach_memories (athlete_profile_id, current_context, memory_flags, metadata_info)
                VALUES (:pid, :ctx, :flags, :meta)
            """)
            
            default_context = json.dumps({"readiness_score": 80, "phase": "Integration", "fatigue_state": "Fresh"})
            default_flags = json.dumps({"migrated_from_v1": True})
            default_meta = json.dumps({"created_via": "migration_script"})
            
            connection.execute(insert_memory_sql, {
                "pid": profile_id,
                "ctx": default_context,
                "flags": default_flags,
                "meta": default_meta
            })
            
            migrated_count += 1
            
        except Exception as e:
            logger.error(f"‚ùå Erreur migration user {u.username} (ID: {u.id}): {e}")

    logger.info(f"‚ú® Succ√®s : {migrated_count} profils migr√©s vers l'architecture V2.")


def apply_migration():
    """Applique toutes les migrations n√©cessaires (Structure + Donn√©es)"""
    with engine.connect() as connection:
        trans = connection.begin()
        try:
            logger.info("üõ†Ô∏è D√©but des migrations de structure...")
            
            # --- 1. TABLES CORE (EXISTANTES) ---
            
            # Users
            if not table_exists(connection, "users"):
                logger.info("Cr√©ation table 'users'...")
                connection.execute(text("""
                    CREATE TABLE users (
                        id SERIAL PRIMARY KEY,
                        username VARCHAR UNIQUE NOT NULL,
                        email VARCHAR UNIQUE,
                        hashed_password VARCHAR NOT NULL,
                        profile_data TEXT,
                        strategy_data TEXT,
                        weekly_plan_data TEXT,
                        draft_workout_data TEXT
                    );
                """))
            
            # Workout Sessions
            if not table_exists(connection, "workout_sessions"):
                logger.info("Cr√©ation table 'workout_sessions'...")
                connection.execute(text("""
                    CREATE TABLE workout_sessions (
                        id SERIAL PRIMARY KEY,
                        user_id INTEGER REFERENCES users(id) ON DELETE CASCADE,
                        date DATE NOT NULL,
                        duration FLOAT NOT NULL,
                        rpe FLOAT NOT NULL,
                        energy_level INTEGER DEFAULT 5,
                        notes TEXT,
                        ai_analysis TEXT,
                        created_at TIMESTAMPTZ DEFAULT NOW()
                    );
                """))
            
            # Workout Sets
            if not table_exists(connection, "workout_sets"):
                logger.info("Cr√©ation table 'workout_sets'...")
                connection.execute(text("""
                    CREATE TABLE workout_sets (
                        id SERIAL PRIMARY KEY,
                        session_id INTEGER REFERENCES workout_sessions(id) ON DELETE CASCADE,
                        exercise_name VARCHAR NOT NULL,
                        set_order INTEGER NOT NULL,
                        weight FLOAT DEFAULT 0.0,
                        reps FLOAT DEFAULT 0.0,
                        rpe FLOAT DEFAULT 0.0,
                        rest_seconds INTEGER DEFAULT 0,
                        metric_type VARCHAR DEFAULT 'LOAD_REPS'
                    );
                """))

            # Feed Items
            if not table_exists(connection, "feed_items"):
                logger.info("Cr√©ation table 'feed_items'...")
                connection.execute(text("""
                    CREATE TABLE feed_items (
                        id VARCHAR PRIMARY KEY,
                        user_id INTEGER REFERENCES users(id) ON DELETE CASCADE,
                        type VARCHAR NOT NULL,
                        title VARCHAR NOT NULL,
                        message VARCHAR NOT NULL,
                        action_payload TEXT,
                        is_read BOOLEAN DEFAULT FALSE,
                        is_completed BOOLEAN DEFAULT FALSE,
                        priority INTEGER DEFAULT 1,
                        created_at TIMESTAMPTZ DEFAULT NOW()
                    );
                """))

            # --- 2. NOUVELLES TABLES V2 (TITANFLOW PRO) ---

            # Athlete Profiles
            if not table_exists(connection, "athlete_profiles"):
                logger.info("üÜï Cr√©ation table 'athlete_profiles' (V2)...")
                # Utilisation de JSON (compatible Postgres/SQLite via SQLAlchemy abstraction usually, 
                # mais en raw SQL on utilise JSON ou TEXT selon le moteur. Ici JSON est standard PG)
                connection.execute(text("""
                    CREATE TABLE athlete_profiles (
                        id SERIAL PRIMARY KEY,
                        user_id INTEGER UNIQUE REFERENCES users(id) ON DELETE CASCADE,
                        basic_info JSON DEFAULT '{}',
                        physical_metrics JSON DEFAULT '{}',
                        sport_context JSON DEFAULT '{}',
                        performance_baseline JSON DEFAULT '{}',
                        injury_prevention JSON DEFAULT '{}',
                        training_preferences JSON DEFAULT '{}',
                        goals JSON DEFAULT '{}',
                        constraints JSON DEFAULT '{}',
                        created_at TIMESTAMPTZ DEFAULT NOW(),
                        updated_at TIMESTAMPTZ DEFAULT NOW()
                    );
                """))
                logger.info("‚úÖ Table 'athlete_profiles' cr√©√©e.")

            # Coach Memories
            if not table_exists(connection, "coach_memories"):
                logger.info("üÜï Cr√©ation table 'coach_memories' (V2)...")
                connection.execute(text("""
                    CREATE TABLE coach_memories (
                        id SERIAL PRIMARY KEY,
                        athlete_profile_id INTEGER UNIQUE REFERENCES athlete_profiles(id) ON DELETE CASCADE,
                        metadata_info JSON DEFAULT '{}',
                        current_context JSON DEFAULT '{}',
                        response_patterns JSON DEFAULT '{}',
                        performance_baselines JSON DEFAULT '{}',
                        adaptation_signals JSON DEFAULT '{}',
                        sport_specific_insights JSON DEFAULT '{}',
                        training_history_summary JSON DEFAULT '{}',
                        athlete_preferences JSON DEFAULT '{}',
                        coach_notes JSON DEFAULT '{}',
                        memory_flags JSON DEFAULT '{}',
                        last_updated TIMESTAMPTZ DEFAULT NOW()
                    );
                """))
                logger.info("‚úÖ Table 'coach_memories' cr√©√©e.")

            # --- 3. MIGRATION DES COLONNES MANQUANTES (LEGACY) ---
            
            if table_exists(connection, "users"):
                migrations_users = [
                                        ("email", "ALTER TABLE users ADD COLUMN IF NOT EXISTS email VARCHAR UNIQUE;"),
                    ("profile_data", "ALTER TABLE users ADD COLUMN IF NOT EXISTS profile_data TEXT;"),
                    ("strategy_data", "ALTER TABLE users ADD COLUMN IF NOT EXISTS strategy_data TEXT;"),
                    ("weekly_plan_data", "ALTER TABLE users ADD COLUMN IF NOT EXISTS weekly_plan_data TEXT;"),
                    ("draft_workout_data", "ALTER TABLE users ADD COLUMN IF NOT EXISTS draft_workout_data TEXT;"),
                ]
                for col_name, sql in migrations_users:
                    if not column_exists(connection, "users", col_name):
                        logger.info(f"Ajout colonne '{col_name}' -> 'users'")
                        connection.execute(text(sql))

            if table_exists(connection, "workout_sessions"):
                migrations_sessions = [
                    ("energy_level", "ALTER TABLE workout_sessions ADD COLUMN IF NOT EXISTS energy_level INTEGER DEFAULT 5;"),
                    ("notes", "ALTER TABLE workout_sessions ADD COLUMN IF NOT EXISTS notes TEXT;"),
                    ("created_at", "ALTER TABLE workout_sessions ADD COLUMN IF NOT EXISTS created_at TIMESTAMPTZ DEFAULT NOW();"),
                    ("ai_analysis", "ALTER TABLE workout_sessions ADD COLUMN IF NOT EXISTS ai_analysis TEXT;"),
                ]
                for col_name, sql in migrations_sessions:
                    if not column_exists(connection, "workout_sessions", col_name):
                        logger.info(f"Ajout colonne '{col_name}' -> 'workout_sessions'")
                        connection.execute(text(sql))

            if table_exists(connection, "workout_sets"):
                migrations_sets = [
                    ("metric_type", "ALTER TABLE workout_sets ADD COLUMN IF NOT EXISTS metric_type VARCHAR DEFAULT 'LOAD_REPS';"),
                    ("rest_seconds", "ALTER TABLE workout_sets ADD COLUMN IF NOT EXISTS rest_seconds INTEGER DEFAULT 0;"),
                ]
                for col_name, sql in migrations_sets:
                    if not column_exists(connection, "workout_sets", col_name):
                        logger.info(f"Ajout colonne '{col_name}' -> 'workout_sets'")
                        connection.execute(text(sql))

            # --- 4. INDEXATION ---
            logger.info("Optimisation des index...")
            indexes = [
                "CREATE INDEX IF NOT EXISTS idx_workout_sessions_user_id ON workout_sessions(user_id);",
                "CREATE INDEX IF NOT EXISTS idx_workout_sessions_date ON workout_sessions(date);",
                "CREATE INDEX IF NOT EXISTS idx_feed_items_user_id ON feed_items(user_id);",
                # Index V2
                "CREATE INDEX IF NOT EXISTS idx_athlete_profiles_user_id ON athlete_profiles(user_id);",
                "CREATE INDEX IF NOT EXISTS idx_coach_memories_profile_id ON coach_memories(athlete_profile_id);"
            ]
            
            for idx_sql in indexes:
                try:
                    connection.execute(text(idx_sql))
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Index existant ou erreur mineure: {e}")
            
            # --- 5. MIGRATION DES DONN√âES ---
            migrate_v1_to_v2_data(connection)

            trans.commit()
            logger.info("üéâ Toutes les migrations (Structure V2 + Donn√©es) sont termin√©es !")
            return True
            
        except Exception as e:
            trans.rollback()
            logger.error(f"‚ùå CRASH MIGRATION : {str(e)}")
            return False

if __name__ == "__main__":
    logger.info("üöÄ Lancement TitanFlow DB Migrator...")
    success = apply_migration()
    if success:
        sys.exit(0)
    else:
        sys.exit(1)-e 

-e 
================================================================================
üìÑ FICHIER : backend/migrate_to_v2.py
================================================================================
#!/usr/bin/env python3
"""
Script de migration des donn√©es existantes vers les nouveaux mod√®les v2
"""
import sys
import json
import logging
from datetime import datetime
from sqlalchemy.orm import Session

# Ajouter le chemin du backend
sys.path.append('.')

from app.core.database import SessionLocal, engine
from app.models import sql_models
from app.services.coach_memory.service import CoachMemoryService

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def migrate_existing_users():
    """Migre les utilisateurs existants vers les nouveaux mod√®les"""
    logger.info("üöÄ D√©marrage de la migration vers v2...")
    
    db = SessionLocal()
    try:
        # V√©rifier si les nouvelles tables existent
        inspector = sql_models.inspect(engine)
        table_names = inspector.get_table_names()
        
        if 'athlete_profiles' not in table_names:
            logger.error("‚ùå Table 'athlete_profiles' non trouv√©e. Ex√©cutez d'abord les migrations SQL.")
            return False
        
        # R√©cup√©rer tous les utilisateurs
        users = db.query(sql_models.User).all()
        logger.info(f"üë• {len(users)} utilisateurs √† migrer")
        
        migrated_count = 0
        skipped_count = 0
        error_count = 0
        
        for user in users:
            try:
                # V√©rifier si l'utilisateur a d√©j√† un profil
                existing_profile = db.query(sql_models.AthleteProfile).filter(
                    sql_models.AthleteProfile.user_id == user.id
                ).first()
                
                if existing_profile:
                    logger.info(f"‚è≠Ô∏è  Utilisateur {user.username} d√©j√† migr√©")
                    skipped_count += 1
                    continue
                
                # Extraire les donn√©es existantes
                profile_data = json.loads(user.profile_data) if user.profile_data else {}
                
                # Construire le profil enrichi
                athlete_profile = sql_models.AthleteProfile(
                    user_id=user.id,
                    basic_info=json.dumps({
                        "pseudo": user.username,
                        "email": user.email,
                        "birth_date": None,
                        "biological_sex": profile_data.get('gender', 'Homme'),
                        "dominant_hand": None,
                        "biological_age": None,
                        "training_age": None
                    }),
                    physical_metrics=json.dumps({
                        "height": profile_data.get('height'),
                        "weight": profile_data.get('weight'),
                        "body_fat_estimate": None,
                        "resting_heart_rate": None,
                        "hrv_baseline": None,
                        "sleep_quality_average": None,
                        "last_updated": None
                    }),
                    sport_context=json.dumps({
                        "primary_sport": profile_data.get('sport', 'Musculation'),
                        "secondary_sports": [],
                        "playing_position": None,
                        "competition_level": profile_data.get('level', 'Amateur'),
                        "training_environment": None,
                        "available_equipment": profile_data.get('equipment', ['Standard']),
                        "training_history_years": None
                    }),
                    performance_baseline=json.dumps({
                        "strength_level": None,
                        "endurance_level": None,
                        "power_level": None,
                        "mobility_level": None,
                        "current_prs": {},
                        "last_test_dates": {}
                    }),
                    injury_prevention=json.dumps({
                        "chronic_issues": profile_data.get('injuries', []),
                        "injury_history": [],
                        "weak_links_identified": [],
                        "prehab_focus": [],
                        "medical_clearance": True
                    }),
                    training_preferences=json.dumps({
                        "preferred_training_split": None,
                        "max_session_duration": 60,
                        "ideal_training_times": [],
                        "disliked_exercises": [],
                        "enjoyed_exercises": [],
                        "motivation_drivers": [],
                        "feedback_style": "Direct",
                        "autonomy_preference": "Medium"
                    }),
                    goals=json.dumps({
                        "primary_goal": profile_data.get('goal', 'Performance G√©n√©rale'),
                        "secondary_goals": [],
                        "target_date": profile_data.get('target_date', '2025-12-31'),
                        "target_metrics": {},
                        "milestones": []
                    }),
                    constraints=json.dumps({
                        "time_availability": profile_data.get('availability', []),
                        "travel_schedule": [],
                        "work_stress_level": None,
                        "family_commitments": None,
                        "budget_constraints": None
                    })
                )
                
                db.add(athlete_profile)
                db.commit()
                db.refresh(athlete_profile)
                
                # Calculer le pourcentage de compl√©tion
                athlete_profile.completion_percentage = athlete_profile.calculate_completion()
                athlete_profile.is_complete = athlete_profile.completion_percentage >= 80
                
                # Initialiser la m√©moire du coach
                try:
                    CoachMemoryService.initialize_coach_memory(athlete_profile, db)
                    migrated_count += 1
                    logger.info(f"‚úÖ {user.username} migr√© (compl√©tion: {athlete_profile.completion_percentage}%)")
                except Exception as e:
                    logger.error(f"‚ùå Erreur initialisation m√©moire pour {user.username}: {str(e)}")
                    db.rollback()
                    error_count += 1
                
            except Exception as e:
                logger.error(f"‚ùå Erreur migration utilisateur {user.username}: {str(e)}")
                error_count += 1
                db.rollback()
                continue
        
        db.commit()
        
        logger.info(f"""
üéâ Migration termin√©e!
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üìä Statistiques:
   ‚Ä¢ Migr√©s: {migrated_count}
   ‚Ä¢ D√©j√† migr√©s: {skipped_count}
   ‚Ä¢ Erreurs: {error_count}
   ‚Ä¢ Total: {len(users)}
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
""")
        
        return True
        
    except Exception as e:
        logger.error(f"üí• Erreur critique de migration: {str(e)}")
        db.rollback()
        return False
    finally:
        db.close()

def create_tables_if_not_exist():
    """Cr√©e les tables si elles n'existent pas"""
    logger.info("üèóÔ∏è  V√©rification/Cr√©ation des tables...")
    
    try:
        sql_models.Base.metadata.create_all(bind=engine, tables=[
            sql_models.AthleteProfile.__table__,
            sql_models.CoachMemory.__table__
        ])
        logger.info("‚úÖ Tables v√©rifi√©es/cr√©√©es")
        return True
    except Exception as e:
        logger.error(f"‚ùå Erreur cr√©ation tables: {str(e)}")
        return False

def main():
    """Fonction principale"""
    print("""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë   MIGRATION TITANFLOW v1 ‚Üí v2            ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
""")
    
    # 1. Cr√©er les tables
    if not create_tables_if_not_exist():
        print("‚ùå √âchec cr√©ation tables. Arr√™t.")
        sys.exit(1)
    
    # 2. Migrer les donn√©es
    print("\nüîÅ Migration des donn√©es utilisateurs...")
    if not migrate_existing_users():
        print("‚ùå √âchec migration donn√©es. V√©rifiez les logs.")
        sys.exit(1)
    
    print("\n‚úÖ Migration termin√©e avec succ√®s!")
    print("   Red√©marrez le serveur pour activer les nouvelles fonctionnalit√©s.")

if __name__ == "__main__":
    main()
-e 

-e 
================================================================================
üìÑ FICHIER : backend/remove_cache_decorator.py
================================================================================
import re

with open('app/routers/coach.py', 'r') as f:
    content = f.read()

# Supprimer toutes les occurrences de @cached_response
new_content = re.sub(r'@cached_response\([^)]*\)\s*\n', '', content)

with open('app/routers/coach.py', 'w') as f:
    f.write(new_content)

print("‚úÖ Tous les d√©corateurs @cached_response retir√©s")
-e 

-e 
================================================================================
üìÑ FICHIER : backend/synchro.py
================================================================================
#!/usr/bin/env python3
"""
Script de v√©rification backend TitanFlow
Teste la compatibilit√© avec le frontend Flutter
"""

import os
import sys
import json
import time
import asyncio
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any
import aiohttp
import jwt
from sqlalchemy import create_engine, text
from dotenv import load_dotenv

# Configuration
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class BackendCompatibilityChecker:
    """V√©rifie la compatibilit√© compl√®te du backend TitanFlow"""
    
    def __init__(self):
        self.results = {
            'timestamp': datetime.now().isoformat(),
            'environment': {},
            'database': {},
            'api_endpoints': {},
            'data_models': {},
            'security': {},
            'performance': {},
            'issues': [],
            'recommendations': []
        }
        self.base_url = "http://localhost:8000"
        self.auth_token = None
        
    async def run_comprehensive_check(self):
        """Ex√©cute tous les tests de compatibilit√©"""
        print("üîß D√âMARRAGE DES TESTS BACKEND TITANFLOW")
        print("=" * 60)
        
        try:
            # 1. V√©rification de l'environnement
            await self._check_environment()
            
            # 2. V√©rification de la base de donn√©es
            await self._check_database()
            
            # 3. V√©rification des endpoints API
            await self._check_api_endpoints()
            
            # 4. V√©rification des mod√®les de donn√©es
            await self._check_data_models()
            
            # 5. V√©rification de la s√©curit√©
            await self._check_security()
            
            # 6. Tests de performance
            await self._check_performance()
            
            # 7. G√©n√©ration du rapport
            self._generate_report()
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors des tests: {e}")
            self.results['issues'].append(f"Erreur critique: {str(e)}")
            
        return self.results
    
    async def _check_environment(self):
        """V√©rifie l'environnement d'ex√©cution"""
        print("\nüîç 1/6: V√©rification de l'environnement")
        
        try:
            # Variables d'environnement
            load_dotenv()
            
            env_vars = {
                'DATABASE_URL': os.getenv('DATABASE_URL'),
                'SECRET_KEY': 'D√âFINIE' if os.getenv('SECRET_KEY') else 'MANQUANTE',
                'GEMINI_API_KEY': 'D√âFINIE' if os.getenv('GEMINI_API_KEY') else 'MANQUANTE',
                'ALGORITHM': os.getenv('ALGORITHM', 'HS256'),
                'ACCESS_TOKEN_EXPIRE_MINUTES': os.getenv('ACCESS_TOKEN_EXPIRE_MINUTES', '30'),
            }
            
            self.results['environment'] = env_vars
            
            # V√©rifications critiques
            issues = []
            if not env_vars['DATABASE_URL']:
                issues.append("‚ùå DATABASE_URL non d√©finie")
            if env_vars['SECRET_KEY'] == 'MANQUANTE':
                issues.append("‚ùå SECRET_KEY non d√©finie (JWT requis)")
            if env_vars['GEMINI_API_KEY'] == 'MANQUANTE':
                issues.append("‚ö†Ô∏è GEMINI_API_KEY non d√©finie (Coach IA d√©sactiv√©)")
            
            for issue in issues:
                self.results['issues'].append(issue)
                logger.info(f"   {issue}")
                
            logger.info("   ‚úÖ Environnement charg√©")
            
        except Exception as e:
            logger.error(f"   ‚ùå Erreur environnement: {e}")
            self.results['issues'].append(f"Erreur environnement: {str(e)}")
    
    async def _check_database(self):
        """V√©rifie la connexion et le sch√©ma de la base"""
        print("\nüóÑÔ∏è  2/6: V√©rification de la base de donn√©es")
        
        try:
            db_url = os.getenv('DATABASE_URL')
            if not db_url:
                logger.error("   ‚ùå URL de base non d√©finie")
                return
            
            # Correction PostgreSQL
            if db_url.startswith("postgres://"):
                db_url = db_url.replace("postgres://", "postgresql://", 1)
            
            engine = create_engine(db_url)
            
            with engine.connect() as conn:
                # Test de connexion
                start = time.time()
                conn.execute(text("SELECT 1"))
                latency = (time.time() - start) * 1000
                
                # V√©rifier les tables
                result = conn.execute(text("""
                    SELECT table_name 
                    FROM information_schema.tables 
                    WHERE table_schema = 'public'
                    ORDER BY table_name
                """))
                
                tables = [row[0] for row in result]
                
                # V√©rifier les tables critiques
                critical_tables = [
                    'users', 'athlete_profiles', 'coach_memories',
                    'workout_sessions', 'workout_sets', 'feed_items'
                ]
                
                missing_tables = [t for t in critical_tables if t not in tables]
                
                self.results['database'] = {
                    'connection': '‚úÖ OK',
                    'latency_ms': round(latency, 2),
                    'tables_found': len(tables),
                    'critical_tables_found': len(critical_tables) - len(missing_tables),
                    'missing_tables': missing_tables,
                    'all_tables': tables
                }
                
                logger.info(f"   ‚úÖ Connexion: {latency:.2f}ms")
                logger.info(f"   üìä Tables: {len(tables)} trouv√©es")
                
                if missing_tables:
                    logger.warning(f"   ‚ö†Ô∏è Tables manquantes: {missing_tables}")
                    self.results['issues'].extend(
                        [f"Table manquante: {t}" for t in missing_tables]
                    )
                
                # V√©rifier les contraintes
                constraints = conn.execute(text("""
                    SELECT 
                        tc.table_name, 
                        tc.constraint_type,
                        tc.constraint_name
                    FROM information_schema.table_constraints tc
                    WHERE tc.table_schema = 'public'
                    ORDER BY tc.table_name, tc.constraint_type
                """))
                
                constraints_list = [
                    f"{row[0]}.{row[2]} ({row[1]})" 
                    for row in constraints
                ]
                
                self.results['database']['constraints'] = constraints_list
                
        except Exception as e:
            logger.error(f"   ‚ùå Erreur base de donn√©es: {e}")
            self.results['database'] = {'error': str(e)}
            self.results['issues'].append(f"Erreur base de donn√©es: {str(e)}")
    
    async def _check_api_endpoints(self):
        """Teste tous les endpoints API"""
        print("\nüåê 3/6: Test des endpoints API")
        
        endpoints = [
            # Endpoints publics
            {'method': 'GET', 'path': '/health', 'auth': False},
            {'method': 'GET', 'path': '/docs', 'auth': False},
            {'method': 'GET', 'path': '/redoc', 'auth': False},
            
            # Authentification
            {'method': 'POST', 'path': '/auth/signup', 'auth': False},
            {'method': 'POST', 'path': '/auth/token', 'auth': False},
            
            # Endpoints prot√©g√©s (n√©cessitent auth)
            {'method': 'GET', 'path': '/user/profile', 'auth': True},
            {'method': 'GET', 'path': '/workouts/', 'auth': True},
            {'method': 'GET', 'path': '/feed/', 'auth': True},
            {'method': 'GET', 'path': '/api/v1/profiles/me', 'auth': True},
            {'method': 'GET', 'path': '/api/v1/coach-memories/me', 'auth': True},
            
            # Coach IA
            {'method': 'POST', 'path': '/coach/audit', 'auth': True},
            {'method': 'GET', 'path': '/coach/strategy', 'auth': True},
            {'method': 'GET', 'path': '/coach/week', 'auth': True},
            
            # Performance & Safety
            {'method': 'POST', 'path': '/performance/1rm', 'auth': True},
            {'method': 'POST', 'path': '/safety/acwr', 'auth': True},
            
            # R√©paration syst√®me
            {'method': 'GET', 'path': '/fix_db', 'auth': False},
        ]
        
        results = {}
        successful = 0
        failed = 0
        warnings = 0
        
        async with aiohttp.ClientSession() as session:
            for endpoint in endpoints:
                url = f"{self.base_url}{endpoint['path']}"
                method = endpoint['method']
                requires_auth = endpoint['auth']
                
                # Pr√©parer les headers
                headers = {'Content-Type': 'application/json'}
                if requires_auth and self.auth_token:
                    headers['Authorization'] = f'Bearer {self.auth_token}'
                
                # Pr√©parer le payload si n√©cessaire
                data = None
                if method == 'POST':
                    if 'auth/token' in endpoint['path']:
                        data = {'username': 'testuser', 'password': 'password123'}
                    elif 'auth/signup' in endpoint['path']:
                        data = {
                            'username': f'test_{int(time.time())}',
                            'email': f'test_{int(time.time())}@example.com',
                            'password': 'Test123!'
                        }
                    elif 'performance/1rm' in endpoint['path']:
                        data = {'weight': 100, 'reps': 5}
                    elif 'coach/audit' in endpoint['path']:
                        data = {'profile_data': {'sport': 'Musculation', 'level': 'Interm√©diaire'}}
                    else:
                        data = {}
                
                try:
                    start = time.time()
                    
                    if method == 'GET':
                        async with session.get(url, headers=headers) as response:
                            status = response.status
                            latency = (time.time() - start) * 1000
                    elif method == 'POST':
                        async with session.post(url, headers=headers, json=data) as response:
                            status = response.status
                            latency = (time.time() - start) * 1000
                            
                            # Sauvegarder le token si c'est une connexion r√©ussie
                            if 'auth/token' in endpoint['path'] and status == 200:
                                response_data = await response.json()
                                self.auth_token = response_data.get('access_token')
                    
                    # √âvaluer le r√©sultat
                    if status in [200, 201]:
                        result = '‚úÖ OK'
                        successful += 1
                    elif status == 404:
                        result = '‚ö†Ô∏è NON IMPL√âMENT√â'
                        warnings += 1
                    elif status == 401 and requires_auth:
                        result = 'üîí AUTH REQUISE'
                        warnings += 1
                    else:
                        result = f'‚ùå {status}'
                        failed += 1
                    
                    results[endpoint['path']] = {
                        'status': status,
                        'latency_ms': round(latency, 2),
                        'result': result
                    }
                    
                    logger.info(f"   {result} {method} {endpoint['path']} ({latency:.2f}ms)")
                    
                except Exception as e:
                    results[endpoint['path']] = {'error': str(e), 'result': '‚ùå ERREUR'}
                    failed += 1
                    logger.error(f"   ‚ùå ERREUR {method} {endpoint['path']}: {e}")
        
        self.results['api_endpoints'] = {
            'tested': len(endpoints),
            'successful': successful,
            'failed': failed,
            'warnings': warnings,
            'details': results
        }
        
        logger.info(f"   üìä R√©sum√©: {successful}‚úÖ {failed}‚ùå {warnings}‚ö†Ô∏è")
    
    async def _check_data_models(self):
        """V√©rifie la coh√©rence des mod√®les de donn√©es"""
        print("\nüìä 4/6: V√©rification des mod√®les de donn√©es")
        
        try:
            # Importer les mod√®les SQLAlchemy
            sys.path.append('.')
            from app.models import sql_models
            
            models_to_check = [
                ('User', sql_models.User),
                ('AthleteProfile', sql_models.AthleteProfile),
                ('CoachMemory', sql_models.CoachMemory),
                ('WorkoutSession', sql_models.WorkoutSession),
                ('WorkoutSet', sql_models.WorkoutSet),
                ('FeedItem', sql_models.FeedItem),
            ]
            
            results = {}
            
            for model_name, model_class in models_to_check:
                try:
                    # V√©rifier que le mod√®le peut √™tre instanci√©
                    instance = model_class()
                    
                    # V√©rifier les colonnes
                    columns = [col.name for col in model_class.__table__.columns]
                    
                    # V√©rifier les relations
                    relationships = []
                    if hasattr(model_class, '__mapper__'):
                        for rel in model_class.__mapper__.relationships:
                            relationships.append(rel.key)
                    
                    results[model_name] = {
                        'status': '‚úÖ VALIDE',
                        'columns': columns,
                        'relationships': relationships,
                        'table_name': model_class.__tablename__
                    }
                    
                    logger.info(f"   ‚úÖ {model_name}: {len(columns)} colonnes")
                    
                except Exception as e:
                    results[model_name] = {'status': f'‚ùå ERREUR: {e}'}
                    logger.error(f"   ‚ùå {model_name}: {e}")
                    self.results['issues'].append(f"Mod√®le {model_name}: {str(e)}")
            
            self.results['data_models'] = results
            
        except ImportError as e:
            logger.error(f"   ‚ùå Impossible d'importer les mod√®les: {e}")
            self.results['issues'].append(f"Import mod√®les: {str(e)}")
        except Exception as e:
            logger.error(f"   ‚ùå Erreur mod√®les: {e}")
            self.results['issues'].append(f"Erreur mod√®les: {str(e)}")
    
    async def _check_security(self):
        """V√©rifie les aspects de s√©curit√©"""
        print("\nüîí 5/6: V√©rification de s√©curit√©")
        
        security_checks = {
            'jwt_config': '‚ùå NON V√âRIFI√â',
            'password_hashing': '‚ùå NON V√âRIFI√â',
            'cors_headers': '‚ùå NON V√âRIFI√â',
            'rate_limiting': '‚ö†Ô∏è  NON D√âTECT√â',
            'input_validation': '‚úÖ TEST REQUIS'
        }
        
        try:
            # Tester JWT
            secret = os.getenv('SECRET_KEY')
            if secret and secret != 'your-super-secret-key-change-in-production':
                try:
                    # G√©n√©rer un token de test
                    payload = {'sub': 'test', 'exp': datetime.now().timestamp() + 3600}
                    token = jwt.encode(payload, secret, algorithm='HS256')
                    jwt.decode(token, secret, algorithms=['HS256'])
                    security_checks['jwt_config'] = '‚úÖ CONFIGUR√â'
                except:
                    security_checks['jwt_config'] = '‚ùå ERREUR JWT'
            else:
                security_checks['jwt_config'] = '‚ùå SECRET PAR D√âFAIT'
            
            # Tester CORS
            async with aiohttp.ClientSession() as session:
                async with session.options(f"{self.base_url}/health") as response:
                    if 'Access-Control-Allow-Origin' in response.headers:
                        security_checks['cors_headers'] = '‚úÖ ACTIV√â'
                    else:
                        security_checks['cors_headers'] = '‚ö†Ô∏è  NON D√âTECT√â'
            
            for check, status in security_checks.items():
                logger.info(f"   {status} {check.replace('_', ' ').title()}")
            
            self.results['security'] = security_checks
            
        except Exception as e:
            logger.error(f"   ‚ùå Erreur s√©curit√©: {e}")
            self.results['security'] = {'error': str(e)}
    
    async def _check_performance(self):
        """Effectue des tests de performance"""
        print("\n‚ö° 6/6: Tests de performance")
        
        try:
            async with aiohttp.ClientSession() as session:
                # Test de latence
                latencies = []
                for _ in range(5):
                    start = time.time()
                    async with session.get(f"{self.base_url}/health") as _:
                        latencies.append((time.time() - start) * 1000)
                    await asyncio.sleep(0.1)
                
                avg_latency = sum(latencies) / len(latencies)
                
                # Test de charge (simplifi√©)
                start = time.time()
                tasks = []
                for _ in range(10):
                    task = session.get(f"{self.base_url}/health")
                    tasks.append(task)
                
                responses = await asyncio.gather(*tasks, return_exceptions=True)
                load_time = (time.time() - start) * 1000
                
                self.results['performance'] = {
                    'avg_latency_ms': round(avg_latency, 2),
                    'load_test_10req_ms': round(load_time, 2),
                    'recommended_max_latency': '300ms',
                    'status': '‚úÖ OK' if avg_latency < 300 else '‚ö†Ô∏è  LENT'
                }
                
                logger.info(f"   üìà Latence moyenne: {avg_latency:.2f}ms")
                logger.info(f"   üìä Test de charge (10 req): {load_time:.2f}ms")
                
        except Exception as e:
            logger.error(f"   ‚ùå Erreur performance: {e}")
            self.results['performance'] = {'error': str(e)}
    
    def _generate_report(self):
        """G√©n√®re un rapport d√©taill√©"""
        print("\n" + "=" * 60)
        print("üìã RAPPORT DE COMPATIBILIT√â BACKEND")
        print("=" * 60)
        
        # R√©sum√©
        total_tests = (
            (1 if self.results['environment'] else 0) +
            (1 if self.results['database'] else 0) +
            (self.results['api_endpoints'].get('tested', 0)) +
            (len(self.results.get('data_models', {}))) +
            (len(self.results.get('security', {}))) +
            (1 if self.results.get('performance') else 0)
        )
        
        successful = (
            (1 if not self.results['issues'] else 0) +
            self.results['api_endpoints'].get('successful', 0)
        )
        
        print(f"\nüìä STATISTIQUES:")
        print(f"   ‚Ä¢ Tests ex√©cut√©s: {total_tests}")
        print(f"   ‚Ä¢ Endpoints test√©s: {self.results['api_endpoints'].get('tested', 0)}")
        print(f"   ‚Ä¢ Endpoints OK: {self.results['api_endpoints'].get('successful', 0)}")
        print(f"   ‚Ä¢ Mod√®les valid√©s: {len(self.results.get('data_models', {}))}")
        
        print(f"\nüîß ENVIRONNEMENT:")
        for key, value in self.results['environment'].items():
            print(f"   ‚Ä¢ {key}: {value}")
        
        print(f"\nüóÑÔ∏è  BASE DE DONN√âES:")
        db = self.results['database']
        if 'error' not in db:
            print(f"   ‚Ä¢ Connexion: {db.get('connection', 'N/A')}")
            print(f"   ‚Ä¢ Latence: {db.get('latency_ms', 0)}ms")
            print(f"   ‚Ä¢ Tables critiques: {db.get('critical_tables_found', 0)}/6")
            if db.get('missing_tables'):
                print(f"   ‚Ä¢ Tables manquantes: {', '.join(db['missing_tables'])}")
        
        print(f"\nüö® PROBL√àMES IDENTIFI√âS ({len(self.results['issues'])}):")
        for issue in self.results['issues']:
            print(f"   ‚Ä¢ {issue}")
        
        print(f"\nüí° RECOMMANDATIONS:")
        recommendations = [
            "‚úÖ Garder les cl√©s JWT en variables d'environnement",
            "‚úÖ Activer CORS pour le frontend Flutter",
            "‚úÖ Configurer les index de base de donn√©es",
            "‚úÖ Mettre en place le logging structur√©",
            "‚úÖ Tester avec des donn√©es r√©elles",
        ]
        
        for rec in recommendations:
            print(f"   {rec}")
        
        # Sauvegarder le rapport JSON
        report_file = f"backend_compatibility_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w') as f:
            json.dump(self.results, f, indent=2, default=str)
        
        print(f"\nüìÑ Rapport JSON sauvegard√©: {report_file}")
        print("=" * 60)

async def main():
    """Point d'entr√©e principal"""
    print("üöÄ D√âMARRAGE DES TESTS DE COMPATIBILIT√â BACKEND")
    print("=" * 60)
    
    checker = BackendCompatibilityChecker()
    results = await checker.run_comprehensive_check()
    
    # √âvaluation finale
    critical_issues = [
        issue for issue in results['issues'] 
        if any(keyword in issue.lower() for keyword in ['‚ùå', 'erreur', 'manquant'])
    ]
    
    if critical_issues:
        print("\n‚ö†Ô∏è  ATTENTION: Probl√®mes critiques d√©tect√©s!")
        print("   Le backend n√©cessite des corrections avant d√©ploiement.")
        return 1
    else:
        print("\n‚úÖ Backend pr√™t pour l'int√©gration avec Flutter!")
        print("   Tous les tests de compatibilit√© sont pass√©s.")
        return 0

if __name__ == "__main__":
    try:
        exit_code = asyncio.run(main())
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print("\n‚ùå Tests interrompus par l'utilisateur")
        sys.exit(1)-e 

-e 
================================================================================
üìÑ FICHIER : backend/test_new_endpoints.py
================================================================================
#!/usr/bin/env python3
"""
Script de test pour les nouveaux endpoints
"""

import requests
import json
import sys

# Configuration
BASE_URL = "http://localhost:8000"
TOKEN = None  # Remplacer par votre token JWT

def test_endpoint(method, endpoint, data=None):
    """Test un endpoint et affiche le r√©sultat"""
    url = f"{BASE_URL}{endpoint}"
    headers = {"Authorization": f"Bearer {TOKEN}"} if TOKEN else {}
    
    try:
        if method == "GET":
            response = requests.get(url, headers=headers)
        elif method == "POST":
            headers["Content-Type"] = "application/json"
            response = requests.post(url, headers=headers, json=data)
        elif method == "PUT":
            headers["Content-Type"] = "application/json"
            response = requests.put(url, headers=headers, json=data)
        else:
            print(f"‚ùå M√©thode non support√©e: {method}")
            return
        
        print(f"
üîç Test {method} {endpoint}")
        print(f"   Status Code: {response.status_code}")
        
        if response.status_code == 200:
            print(f"   ‚úÖ Succ√®s!")
            try:
                result = response.json()
                print(f"   R√©ponse: {json.dumps(result, indent=2, ensure_ascii=False)[:200]}...")
            except:
                print(f"   R√©ponse: {response.text[:200]}...")
        else:
            print(f"   ‚ùå Erreur!")
            print(f"   Message: {response.text}")
            
    except Exception as e:
        print(f"   ‚ùå Exception: {e}")

if __name__ == "__main__":
    if len(sys.argv) > 1:
        TOKEN = sys.argv[1]
    
    if not TOKEN:
        print("‚ö†Ô∏è  Aucun token fourni. Seuls les endpoints publics seront test√©s.")
        print("   Usage: python test_new_endpoints.py <votre_token_jwt>")
    
    # Test des nouveaux endpoints
    print("üß™ TEST DES NOUVEAUX ENDPOINTS")
    print("=" * 50)
    
    # 1. Test GET /user/profile/complete
    test_endpoint("GET", "/user/profile/complete")
    
    # 2. Test POST /user/profile/complete (avec des donn√©es d'exemple)
    sample_profile = {
        "basic_info": {
            "pseudo": "test_athlete",
            "email": "test@example.com",
            "training_age": 3
        },
        "physical_metrics": {
            "weight": 75.5,
            "height": 180,
            "body_fat": 15.0
        },
        "sport_context": {
            "sport": "Rugby",
            "level": "Interm√©diaire",
            "position": "Demi"
        }
    }
    test_endpoint("POST", "/user/profile/complete", sample_profile)
    
    # 3. Test POST /user/profile/sections/basic_info
    basic_info_update = {
        "pseudo": "athlete_updated",
        "email": "updated@example.com",
        "birth_date": "1990-01-01"
    }
    test_endpoint("POST", "/user/profile/sections/basic_info", basic_info_update)
    
    # 4. Test POST /user/profile/sections/physical_metrics
    physical_update = {
        "weight": 76.0,
        "height": 180,
        "body_fat": 14.5
    }
    test_endpoint("POST", "/user/profile/sections/physical_metrics", physical_update)
    
    print("
" + "=" * 50)
    print("‚úÖ Tests termin√©s!")
-e 

-e 
================================================================================
üìÑ FICHIER : frontend/app.py
================================================================================
import streamlit as st
import requests
import pandas as pd
from datetime import datetime

# Configuration de la page
st.set_page_config(page_title="TitanFlow Pro", page_icon="‚ö°", layout="wide")

# L'URL de ton API (Backend)
API_URL = "http://127.0.0.1:8000"

# --- GESTION DE LA SESSION (Token) ---
if "token" not in st.session_state:
    st.session_state.token = None

def login():
    st.sidebar.header("üîê Connexion")
    username = st.sidebar.text_input("Pseudo")
    password = st.sidebar.text_input("Mot de passe", type="password")
    
    if st.sidebar.button("Se connecter"):
        try:
            # Appel √† l'API pour r√©cup√©rer le token
            response = requests.post(
                f"{API_URL}/auth/token",
                data={"username": username, "password": password}
            )
            if response.status_code == 200:
                st.session_state.token = response.json()["access_token"]
                st.sidebar.success("Connect√© !")
                st.rerun()
            else:
                st.sidebar.error("Erreur de connexion")
        except Exception as e:
            st.sidebar.error(f"API introuvable : {e}")

def logout():
    if st.sidebar.button("Se d√©connecter"):
        st.session_state.token = None
        st.rerun()

# --- INTERFACE PRINCIPALE ---
st.title("‚ö° TitanFlow : Monitoring Athl√©tique")

# V√©rification de l'√©tat de l'API
try:
    health = requests.get(f"{API_URL}/health").json()
    st.success(f"Backend connect√© v{health['version']}")
except:
    st.error("üö® Le Backend semble √©teint. V√©rifie que le Terminal 1 tourne bien !")

# Gestion Login/Logout
if not st.session_state.token:
    st.info("Veuillez vous connecter dans la barre lat√©rale pour acc√©der aux donn√©es.")
    login()
else:
    logout()
    st.write("---")
    
    # Onglets de l'application
    tab1, tab2 = st.tabs(["üèãÔ∏è‚Äç‚ôÇÔ∏è Historique", "‚ûï Nouvelle S√©ance"])
    
    # --- ONGLET 1 : HISTORIQUE ---
    with tab1:
        st.subheader("Vos s√©ances enregistr√©es")
        headers = {"Authorization": f"Bearer {st.session_state.token}"}
        
        try:
            res = requests.get(f"{API_URL}/workouts/", headers=headers)
            if res.status_code == 200:
                workouts = res.json()
                if workouts:
                    df = pd.DataFrame(workouts)
                    st.dataframe(df, use_container_width=True)
                else:
                    st.info("Aucune s√©ance trouv√©e.")
            else:
                st.error("Erreur chargement donn√©es")
        except Exception as e:
            st.error(f"Erreur : {e}")

    # --- ONGLET 2 : AJOUTER S√âANCE ---
    with tab2:
        st.subheader("Enregistrer un entra√Ænement")
        with st.form("new_workout"):
            col1, col2 = st.columns(2)
            date = col1.date_input("Date")
            duration = col2.number_input("Dur√©e (min)", min_value=0, value=60)
            rpe = st.slider("Intensit√© (RPE)", 0, 10, 5)
            
            submitted = st.form_submit_button("Sauvegarder")
            
            if submitted:
                # Pr√©paration du JSON
                payload = {
                    "date": str(date),
                    "duration": duration,
                    "rpe": rpe
                }
                # Envoi √† l'API
                headers = {"Authorization": f"Bearer {st.session_state.token}"}
                res = requests.post(f"{API_URL}/workouts/", json=payload, headers=headers)
                
                if res.status_code == 200:
                    st.success("S√©ance enregistr√©e ! üéâ")
                    # Petit hack pour rafra√Æchir l'historique
                    st.rerun()
                else:
                    st.error(f"Erreur : {res.text}")
-e 

